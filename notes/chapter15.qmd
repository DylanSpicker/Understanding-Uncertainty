# The Basics of Null Hypothesis Significance Testing

## The Framework of Null Hypothesis Significance Testing
Over the past two chapters the focus has been on *estimation procedures* for parameters of interest. Point estimation gives us methods to make inferences regarding the specific value of a particular parameter, while interval estimation permits the discussion of the uncertainty of these estimates. Taken together these estimation techniques allow for a deep understanding of the value of parameters, based on observed samples. We know that every sample we take is likely to result in different estimates based on sampling variability. Correspondingly, the specific inferences that are made regarding the parameter values will depend on the specific sample that is observed. Using interval estimates we hope to capture how uncertain our knowledge is, however, it is an unavoidable truth that any conclusions we draw are subject to the whims of the sample. A fundamental question we can ask, in light of this truth, is "how are we able to draw specific conclusions or make decisions on the basis of our estimated values?"

The framework of **null hypothesis significance testing (NHST)**, often referred to as simply **hypothesis testing**, provides one mechanism for making decisions regarding the value(s) of parameter(s) using observed samples. In a hypothesis test the observed sample is used to weigh evidence against competing explanations for what is true in the population. We make educated guesses regarding the likely value of a population parameter and then determine how likely it would be to observe the sample that we have actually observed, if our guesses are actually true. If the sample is likely to be produced under our specified version of truth, then the sample provides no evidence against the guesses. If, on the other hand, the sample is unlikely given our specified version of the truth, this serves as evidence against our guesses. 

:::{#exm-the-weight-die}
## Sadie and the Weighted Die
Sadie has been playing a board game that was borrowed from Garth, and has been feeling particularly unlucky based on dice rolls. Uncertain if the die provided in the game is actually fair, Sadie decides to test it. Specifically, in the game it benefits the player to roll high numbers and is a detriment to rolling low numbers. Sadie decides to roll the die many times, recording whether each roll is high (greater than or equal to $3$) or low (less than or equal to $3$).

a. Suppose Sadie rolls the die $50$ times and observes $21$ high values and $29$ low values. If the die were fair, approximately how likely is it that $21$ or fewer high values would be rolled in $50$ rolls? 
b. Suppose Sadie continues rolling, and in $100$ rolls of the die, observes $35$ high values and $65$ low values. If the die were fair, approximately how likely is it that $35$ or fewer high values would be rolled in $100$ rolls?
c. In light of these probabilities, what is your conclusion about the die?

::::{.callout .solution collapse='true'}
## Solution
a. If the die were actually fair, then we know the number of high values should follow a $\text{Bin}(50, 0.5)$. Thus, we want $P(X \leq 21)$. We can compute this directly from the binomial (for instance, using `R`), which gives `r round(pbinom(21, size = 50, prob = 0.5), 3)`. Alternatively, we can use the normal approximation to the binomial. In this case, we would find that $X$ share an approximate distribution with $W \sim N(25, 12.5)$, and $P(X \leq 21) \approx P(W \leq 21.5)$. Then, $$P(W \leq 21.5) = P(Z \leq \frac{21.5-25}{\sqrt{12.5}}) = \Phi(-0.99) \approx \Phi(-1).$$ This can be approximated via the empirical rule as $$1 - (0.5 + \frac{0.68}{2}) = 0.16,$$ which is near the value calculated directly from the binomial. Thus, there is about a $16\%$ chance that the die would produce this few high numbers, if it were fair.
b. Using the exact same procedure as above, we get $X \sim \text{Bin}(100, 0.5)$ approximated by $W \sim N(50, 25)$. This gives either a direct binomial probability of approximately `r round(pbinom(35, size = 100, prob = 0.5),3)`. For the approximation, we get $$P(W \leq 35.5) = P(Z \leq \frac{35.5 - 50}{5}) = \Phi(-2.9) \approx \Phi(-3).$$ This is, from the empirical rule, approximately $$1 - (0.5 + \frac{0.997}{2}) = 0.0015.$$
c. Based on the $100$ rolls of the die, it seems very unlikely that the die is actually fair. Sadie could be getting very unlucky, however, if the die is fair there is about a $1$ in $1000$ chance that this few high rolls would be seen. Thus, it is more reasonable to assume that the die is not actually fair than it is to assume that Sadie has just been very unlucky. The evidence after $50$ rolls is far less strong, being approximately $1$ in $5$. 
::::
:::

:::{#def-hypothesis-testing}
## Hypothesis Testing
**Hypothesis testing** is a statistical framework through which data are assessed to determine whether they sufficiently support a particular hypothesis regarding a population parameter. Under an assumed hypothesis regarding the true population parameter, the likelihood of observing the present sample is computed. From this, conclusions are drawn regarding the statistical validity of the hypothesis. 
:::

At the core of the hypothesis testing framework is the idea of a statistical hypothesis. A statistical hypothesis is a specification of a plausible parameter value made on the basis of subject-matter expertise, rather than through the data themselves. Alongside the statistical hypothesis, subject-matter expertise is required to define the evidentiary requirements of the test. Specifically, we must consider how certain we want to be regarding conclusions that we draw, a decision that is ultimately based on the stakes of drawing an incorrect conclusion. With the specification of a hypothesis and a required level of significance, we turn to the data that have been observed and summarize them using a test statistic. A test statistic is a statistic that has a known sampling distribution under the assumption that our hypothesis is correct. Using this test statistic, and the knowledge of its sampling distribution, we can assess the likelihood of having observed this specific statistic if the hypothesis were true. Based on our assessment, and the level of significance we desire, we can then draw conclusions and interpret the results.

This high-level description summarizes the procedure of hypothesis testing as a five-step procedure:

1. Determine the **null** and **alternative hypotheses**.
2. Determine the **significance level** for the test.
3. Compute the **test statistic** using the observed data.
4. Find the **$p$-value** based on the null sampling distribution.
5. Make a comparison, draw conclusions, and interpret the results. 

Each of these steps is outlined in general in the following sections. 

### The Null and Alternative Hypothesis
At the core of the framework of hypothesis testing is the concept of statistical hypotheses. Specifically, hypothesis testing is fundamentally a procedure wherein evidence within a sample is assessed to determine whether it contradicts a particular hypothesis. 

:::{#def-statistical-hypothesis}
## Statistical Hypothesis
A **statistical hypothesis** is a statement regarding the value of an (unknown) population parameter, informed via outside knowledge rather than through estimation. For instance, we may state that $\theta = 0$ or $\theta \geq 10$.
:::

In the framework of null hypothesis significance testing, we focus on the specification of two distinct hypotheses. First, and most importantly, we specify the **null hypothesis**. The null hypothesis corresponds to our underlying belief about the world prior to conducting statistical tests. The null hypothesis will capture how we will act without strong evidence to the contrary. In contrast to the null hypothesis, we also specify an **alternative hypothesis**. The alternative gives an *alternative* explanation compared to the null. That is, if the null is not true, then the alternative should be. 

:::{#def-null-hypothesis}
## Null Hypothesis
The **null hypothesis** is a statistical hypothesis that is meant to capture the assumed state of the world that would be believed without contradictory evidence. The null hypothesis captures the value of a parameter that is assumed to hold by default. Often, the null hypothesis captures the idea of *no effect* or *no difference*, such as taking $\theta = 0$. The null hypothesis is typically denoted $H_0$, so we write, for instance, $H_0: \theta = 0$ or $H_0: \theta \geq 10$.
:::

:::{#def-alternative-hypothesis}
## Alternative Hypothesis
The **alternative hypothesis** is a statistical hypothesis that is defined in contrast to a stated null hypothesis. Generally, we take the alternative hypothesis to be *not* the null. If the null hypothesis states $\theta = 0$ then the alternative will be $\theta \neq 0$. We denote the alternative as either $H_1$ or $H_A$. If the null hypothesis is not true, the alternative hypothesis will be. 
:::

When framing the null and the alternative hypothesis it is important to keep in mind that we are seeking evidence *against* the null hypothesis. We will never seek to positively confirm that the null hypothesis actually holds. Instead, we want to see if the observed data allow us to conclude that it certainly does not hold. For this reason, the hypotheses we are considering should be determined prior to data collection and should be informed by a subject-matter understanding, rather than for statistical convenience. To determine your null hypothesis, you should ask what you would continue to believe without evidence to the contrary, and take that as the null. If you have no such prior beliefs, then it is best to take the most conservative statement as the null hypothesis.^[For instance, if you are considering a treatment, you should assume that the treatment has no effect. This way, without strong evidence suggesting that there is an effect, you will not use the treatment. The same ideas apply more broadly.]

:::{#exm-determine-hypotheses}
## Sadie and Charles Care for Trees: Identifying Hypotheses
Sadie and Charles have, after watching a new documentary, become quite interested in caring for trees. They have started a small tree nursery, and are interested in how statistics may be used to inform their progress through this adventure. For each of the following questions, they are looking to identify what null and alternative hypotheses may be reasonable for the following questions. 

a. They have found a pouch of aspen seeds, but are not sure where they came from. Aspens typically grow to be a total of about $50$ feet tall, with a diameter of $10.5$ inches. They want to know whether these aspens seeds are similar to common aspens, or not.
b. They have many different fertilizer options. They want to know whether a particular blend of their fertilizers influences the height to which the trees grow, or not. 
c. They understand that bark beetles are a problem that can often impact pine trees. Before they help reforest in a certain area, they want to know whether more than $10\%$ of pine trees are infested with bark beetles, or not, to understand whether new plants are likely to survive. 

::::{.callout .solution collapse='true'}
## Solution

a. Here, we could think of running two separate hypothesis tests. If we take $\theta_1$ to represent the average height of an aspen grown from the seed, we may test $H_0: \theta_1 = 50$ versus the alternative $H_A: \theta_1 \neq 50$. If $\theta_2$ represents the average diameter of the grown aspens, then we can test $H_1: \theta_2 = 10.5$ versus the alternative $H_A: \theta_2 \neq 10.5$. 
b. Here, we require a parameter, say $\theta_3$, to represent the effect of the fertilizer on tree growth. Specifically, we may take $\theta_3$ to represent the average change in height that a tree receiving the fertilizer has (compared to if the tree had not received the fertilizer). Then, it is most sensible to assume by default that there is no effect, and thus $H_0: \theta_3 = 0$ versus $H_A: \theta_3 \neq 0$.
c. Here, the parameter of interest is $\theta_4$, the proportion of trees in a particular area that are infected with bark beetles. We may consider $H_0: \theta = 0.1$ versus the alternative $H_A: \theta \neq 0.1$. However, in this case, it may be more prudent to actually use a **one-sided hypothesis**, taking $H_0: \theta \geq 0.1$ versus $H_A: \theta \leq 0.1$. The rationale would be that, since the decision to act would only be influenced by a small value of $\theta_4$, the true interest is in whether it is smaller than $0.1$, not simply whether it is different from $0.1$.
::::
:::

A hypothesis test is outlined by the collective statement of the null and alternative hypotheses. When deciding on the null and alternative, there is the possibility of considering either a **one-tailed** or a **two-tailed** hypothesis test. These refer to the number of tails of the distribution that would be considered evidence *against* the null hypothesis, or put differently, the number of tails captured by the alternative hypothesis. 

:::{#def-one-tailed-test}
## One-Tailed Hypothesis Test
In a **one-tailed hypothesis test**, the alternative hypothesis will take the form $H_A: \theta > \theta_0$, or $H_A: \theta < \theta_0$, for some constant value $\theta_0$. In this sense, only one tail (either the **upper tail** in terms of $\theta > \theta_0$ or the **lower tail** in terms of $\theta < \theta_0$) will be considered as evidence against the null hypothesis. In a one-tailed test, the null hypothesis is given as either $H_0: \theta \leq \theta_0$ or $H_0: \theta \geq \theta_0$.^[Note, many sources will list the null hypothesis as simply $H_0: \theta = \theta_0$ and control the tail behaviour exclusively via the alternative. This results in the same test mechanism but is, in my opinion, less clear as to the actual hypotheses that are being tested.]
:::

:::{#def-two-tailed-test}
## Two-Tailed Hypothesis Test
In a **two-tailed hypothesis test**, the alternative hypothesis will take the form $H_A: \theta \neq \theta_0$, for some constant value $\theta_0$. In this sense, both the upper and lower tails will be considered as evidence against the null hypothesis. In a two-tailed test, the null hypothesis is given as $H_0: \theta = \theta_0$. 
:::

As we continue to discuss the implementation of a hypothesis test and the practicalities in testing a particular hypothesis, we will discuss the implications in using one versus two tailed tests. The general guidance is that, unless there is a very strong, scientific rationale for considering a one-tailed test, two tailed tests ought to be preferred. If a one-tailed test is to be suggested, the null and alternative should still be specified prior to doing any analysis of the experiment, and which tail is tested should be informed through subject-matter expertise.^[The use of a one-tailed test should be reserved for cases where there is no real possibility of contradicting the null in both directions.] Once the hypotheses have been selected, we need to make a choice regarding the levels of significance. 

### Significance Levels 
In addition to specifying the hypotheses on the basis of subject-matter knowledge, to perform a hypothesis test we must also decide on the **level of significance** that we are considering. Generally, the level of significance refers to the amount of evidence that we would need *against* the null hypothesis in order to reject it (in favour of the alternative). This decision should be informed based on the stakes of the decisions that we are making. When the stakes are high, or it is very important to not reject the null hypothesis incorrectly, we should increase the amount of evidence we are looking for. 

:::{#def-level-of-significance}
## Level of Significance
The **level of significance**, denoted $\alpha$, is a measure of how much evidence is required to reject the null hypothesis in favour of the alternative. Formally, the level of significance specifies the probability of rejecting the null hypothesis, if it were actually true. In this sense, the lower the value of $\alpha$, the **stronger** the evidence required against the null hypothesis. Typically, $\alpha \leq 0.1$ is selected, with the specific value dependent on the context.
:::

A fundamental truth of hypothesis testing^[and, indeed, of any procedure that deals with randomness and uncertainty] is that we can never be certain in our conclusions. Owing to the inherent random nature of data, there is always the possibility that mistakes occur. The level of significance provides us with the capacity to control the likelihood of one class of these mistakes. Specifically, it controls how likely it is to conclude that the null hypothesis is not true when it is. In a perfect world, free of randomness and uncertainty, we would take $\alpha = 0$. This way, we would never conclude that the null hypothesis should be rejected when it is actually true. Unfortunately, the only way to ensure that we never make this mistake is to never reject the null hypothesis.^[Uncertainty ensures us that there is always a *chance* that the null hypothesis was incorrectly rejected.] Instead, we take $\alpha$ to be a small value greater than $0$. 

To decide what value specifically, we have to consider the question "What if we are wrong?". If you incorrectly reject the null hypothesis, what are the stakes of that error? Consider a scenario where we are testing a new treatment for a severe illness, and we want to test whether the treatment has an effect on survival rates. In this case, we would typically take $H_0: \theta = 0$ versus the alternative, $H_A: \theta \neq 0$; by default we are assuming that the treatment does not have any impact, and are looking for evidence to the contrary. Now, suppose that there already exists a treatment for the illness that is moderately effective. In this setting, it is far more costly to make a mistake stating that the new treatment is also effective, if it turns out that it is not. The reason being that if we conclude the treatment is effective it may be prescribed to individuals, and this may happen in place of the existing effective treatment. If it turns out that the new treatment does not have a positive impact, this would mean that real people will receive treatments that are ineffective, when they could receive treatments that actually help. Conversely, if there does not exist any treatment at all for the illness now, we likely are willing to take a slightly higher risk. In this case, if we are wrong and the treatment does not meaningfully help, there is no alternative for the patient. As a result, in the first case we should take a lower value of $\alpha$ compared to the second. 

:::{#exm-determine-significance-levels}
## Sadie and Charles Care for Trees: Identifying Hypotheses
With the statistical hypotheses identified for their tree planting hobby, Charles and Sadie are now trying to determine what level of significance they should pick in running these hypotheses. Specifically, they are looking to rank the following hypotheses in order of required significance levels, and justify these choices to one another. 

a. They have found a pouch of aspen seeds, but are not sure where they came from. Aspens typically grow to be a total of about $50$ feet tall, with a diameter of $10.5$ inches. They want to know whether these aspens seeds are similar to common aspens, or not. Importantly, they have decided that they will plant the aspens no matter what they find, and care for them all the same. Because they do not know where the seeds came from, they cannot change suppliers even if they wanted to. 
b. They have many different fertilizer options. They want to know whether a particular blend of their fertilizers influences the height to which the trees grow, or not. The mix of fertilizer that they are considering here is cheaper than their current fertilizer choice, and so they have a desire to switch if possible. However, they know that the current fertilizer they use is effective at increasing growth.
c. They understand that bark beetles are a problem that can often impact pine trees. Before they help reforest in a certain area, they want to know whether more than $10\%$ of pine trees are infested with bark beetles, or not, to understand whether new plants are likely to survive. If they plant trees into an infested area, these trees will likely all die relatively young, wasting resources and the ability to help revitalize the ecosystem. 


::::{.callout .solution collapse='true'}
## Solution
These hypotheses require evidence in the order that they are currently written. Namely, hypothesis (a) requires less than (b) which requires less than (c). 

- For (a), the hypothesis test is mostly for intrigue. It will not alter the way that Sadie and Charles act in any way, nor are there any stakes to the decision. As a result, if they are wrong, that is okay -- the only impact of this is that they may tell people they found aspen seeds that grow differently compared to regular, when they have not actually done so. 
- Next, is (b). The stakes here are higher since there currently exists a fertilizer that works well. If Sadie and Charles conclude that the new fertilizer also improves growth, but it does not, then they will end up with plants that are worse off than they otherwise would be had they not made the same conclusion. Fortunately, however, the fertilizer is less costly than the current one. As a result, the mistake, while impacting the growth of their trees, will not have cascading negative outcomes. 
- Finally, hypothesis (c) is the hypothesis with the largest stakes. If they draw the wrong conclusion in this case, they are risking resources and opportunity. Additionally, if they find that the infestation is bad, they have remedies for this, and as such, it is important to draw the correct conclusion here. 

::::
:::

:::{.callout-warning icon="false"}
## The Historical Case for $\alpha = 0.05$
There is a long tradition, one which persists across many fields today, of taking $\alpha = 0.05$, regardless of the situation. This tendency owes itself to early work by Ronald Fisher (a statistician largely responsible for our approach to statistical inference). In *Statistical Methods for Research Workers*, published in 1925, Fisher wrote "It is convenient to take this point as a limit in judging whether a deviation is to be considered significant or not. Deviations exceeding twice the standard deviation are thus formally regarded as significant." In 1926, Fisher further wrote "If one in twenty does not seem high enough odds, we may, if we prefer it, draw the line at one in fifty (the $2$ per cent point), or one in a hundred (the $1$ per cent point). Personally, the writer prefers to set a low standard of significance at the $5$ per cent point, and ignore entirely all results which fail to reach this level. A scientific fact should be regarded as experimentally established only if a properly designed experiment *rarely fails* to give this level of significance." 

Fisher largely worked in well-designed experiments applied to the study of agriculture. It is probably prudent to read Fisher's advice of the $5\%$ level of significance as existing in this context. Even as the originator of taking $\alpha = 0.05$, Fisher acknowledged the importance of selecting a line based on the needs of the particular context. It is a somewhat unfortunate historical artifact that his declarative statements of $\alpha = 0.05$ took far stronger hold on the scientific world at large, as compared to his more contextual statements. Still, with our present understanding of hypothesis testing, we should always keep in mind the context of the question we are asking, and determine an appropriate level of significance within this context.
:::

Once a level of significance has been decided upon, we need to determine a method for establishing how we can assess whether the evidence in the sample achieves the level of significance or not. To do so, we rely on test statistics.

### Test Statistics 
Test statistics are statistics^[Recall that statistics are any quantities that can be computed given a sample of data.] with a special property. Notably, if the null hypothesis is true, a test statistic must have a known distribution, with all parameter values known as well. That is, if the sampling distribution of a statistic is fully specified assuming that the null hypothesis holds, this statistic is a test statistic.

:::{#def-test-statistic}
## Test Statistic
A **test statistic** is any statistic, which is to say a quantity computable given a specific sample, used for hypothesis testing. Test statistics must have the property that, supposing the null hypothesis is true, the sampling distribution of the statistic is completely specified. Often, the exact sampling distribution will not be known, but a statistic can still be regarded as a test statistic if an approximate distribution is available.
:::

:::{#def-null-distribution}
## Null Distribution
The sampling distribution of a test statistic, assuming the null hypothesis holds, is referred to as **the null distribution**. We may also say that the statistic follows a particular distribution, *under the null*. Suppose that $\widehat{\theta}$ is a test statistic with a null distribution $F$, then we can write $\widehat{\theta} \stackrel{H_0}{\sim} F$, to mean that, under the null, the statistic follows the distribution given by $F$.
:::

Because the null distribution is fully specified, it is possible to calculate probabilities associated with test statistics assuming that the null hypothesis holds. These probabilities will ultimately be assessed as evidence in relation to the level of significance. Typically, a test statistic can be derived by first considering a statistic, and then introducing dependence on the unknown parameters that are being tested. While these parameter values are unknown, generally speaking, if the null hypothesis is assumed to be true this will specify concretely the value of the parameter. For instance, suppose that we have a sample from a normal distribution, with a known variance ($\sigma^2$) but an unknown mean ($\mu$). We know that the sample mean will have a normal sampling distribution, giving $\overline{X} \sim N(\mu,\sigma^2)$. If we wish to test the null hypothesis $H_0: \mu = \mu_0$, for some constant $\mu_0$, then the null distribution of $\overline{X}$ becomes $N(\mu_0, \sigma^2)$. This stands as a test statistic since we exactly know the distribution under the null. More commonly we may take $\overline{X} - \mu_0$, resulting in $N(0, \sigma^2)$, or even $$\frac{\overline{X} - \mu_0}{\sigma} \stackrel{H_0}{\sim} N(0,1).$$ 

In each case, the statistic has a known null distribution. As a result, we can solve for probabilities assuming that the null hypothesis were true. These probabilities are most commonly codified through the use of p-values. 

### $p$-values and Critical Values
To determine whether there is sufficient evidence to contradict the assumed null hypothesis, we must be able to translate statements regarding our test statistic into probability values. Specifically, we can "ask what values for our test statistic could we have observed that would have been at least as contradictory to the null hypothesis as the value that we *did* observe?". Then, using the null distribution, we can ask, "assuming the null hypothesis were true, how likely would it have been to observe an outcome of the test statistic at least as contradictory as the outcome we did observe?". The probability associated with this is called the $p$-value.

:::{#def-p-value}
## $p$-value
The **$p$-value**, (or probability value), is a measure of how likely the observed data is assuming that the null hypothesis is true. Specifically, the $p$-value measures the probability of observing an outcome at least as extreme^[Here, extreme means contradictory to the null hypothesis. Note that the extremity of an observation depends on whether we have a one-tailed or two-tailed alternative. With a two-tailed alternative, we need to consider extreme values in both tails, with a one-tailed alternative, we simply need to consider values that are further away from the null value.] as the outcome that was observed, based on the test statistic. This probability indicates how likely the data were to be observed by random chance, assuming the null hypothesis holds.
:::

Suppose, for sake of example, we observe a value of a test statistic, $t=-2$. Under the null hypothesis, we know that $t$ should equal $0$. Then, we ask "what could we have observed that would have been *more* contradictory than what we did observe?" Suppose that we are considering a two-tailed alternative. Note that if we had observed any value of $t < -2$, this would have been more extreme. On the other hand, had we observed any value of $t \geq 2$, this also would have been more extreme. Thus, any value outside the interval $(-2, 2)$ would have been more extreme. Thus, to work out the $p$-value in this case we would find $$P(\{T \leq -2\}\cup\{T \geq 2\}) = P(T \leq -2) + P(T \geq 2).$$ Had this been a one-tailed test, the $p$-value would have been only $P(T \leq -2)$. 

Intuitively, the smaller the $p$-value, the more evidence *against* the null hypothesis based on the sample. If the $p$-value were sufficiently low, then it would be the case that the observation we made would be sufficiently rare, assuming the null hypothesis were true. With a small enough probability, it is more reasonable to assume that the null hypothesis is not accurate, rather than assuming that we happened to make a particularly rare observation. To decide how small of a probability is small enough, we turn to our level of significance. If the $p$-value is below the selected level of significance, then we can conclude that the results are unlikely to have been observed due to chance, and instead, it is more likely to be observed as the alternative is a better explanation of the underlying reality. On the flip side, if the $p$-value is larger than the level of significance, then we did not see sufficient evidence in the sample to reject the null hypothesis. 

:::{#exm-finding-p-values}
## Sadie and Charles Care for Trees: Identifying $p$-Values
Charles and Sadie are feeling quite confident in their application of hypothesis testing for tree care. They have decided on hypotheses, significance levels, and have even worked out what test statistics they will use. Now, they are hoping to practice calculating $p$-values before the data collection actually begins, to ensure that they will be able to do it later on, when it matters. Determine the $p$-values based on the given information in the following. 

a. For the pouch of aspen seeds, they are planning to test $H_0: \theta_1 = 50$ versus $H_A: \theta_1 \neq 50$. This will be done using the test statistic $$T_1 = \frac{\overline{X} - 50}{10/\sqrt{n}} \stackrel{H_0}{\sim} N(0,1).$$ Suppose than in a sample of size $4$ they find $\overline{x} = 60$. For the diameters, testing $H_0: \theta_2 = 10.5$ versus $H_A: \theta_2 \neq 10.5$, they will take $$T_2 = \frac{\overline{X} - 10.5}{3/\sqrt{n}} \stackrel{H_0}{\sim} N(0,1).$$ Suppose in a sample of size $9$ they observe $\overline{x} = 8.5$. 
b. To understand the effects of their fertilizer types, they will test $H_0: \theta_3 = 0$ versus $H_A: \theta_3 \neq 0$. They will use the statistic $$T = \frac{\overline{X}}{s/\sqrt{n}} \stackrel{H_0}{\sim} t_{n-1}.$$ In a sample of $49$ trees, they observe $\overline{x} = 3$, with a sample standard deviation of $14$. 
c. To investigate the extent of the bark beetle infestation, they will test $H_0: \theta_4 \leq 0.1$ versus $H_A: \theta_4 > 0.1$. This can be done via $$T=\frac{\widehat{p} - 0.1}{\sqrt{0.1(0.9)/n}} \stackrel{H_0}{\sim} N(0,1).$$ In a sample of size $100$ they observe $14$ trees that are infected. 

::::{.callout .solution collapse='true'}
## Solution

a. Using $T_1$, we find an observed value, $t_1$ by plugging in $n=4$ and $\overline{x} = 60$. This gives $t_1 = 10/5 = 2$. As a result, the $p$-value is given by $$P(Z \geq 2) + P(Z \leq -2) = 1 - \Phi(2) + \Phi(-2) = 2(1 - \Phi(2)) \approx `r round(2 * pnorm(-2), 3)`.$$ Alternatively, we could have used the empirical rule to conclude that this will be approximately $0.05$. For $T_2$, plugging in $n=9$ and $\overline{x} = 8.5$ gives $t_2 = -2/1 = -2$. Thus, for the $p$-value we would get the exact same quantity, since we need $P(Z \leq -2) + P(Z > 2)$. Consider the following plot that shades the area needed to calculate the $p$-value, with $t_1$ and $t_2$ each labelled. 

```{r}
#| echo: false
#| fig-height: 3

library(ggplot2)
library(ggthemes)

# Define the value of t
t <- 2

# Create a sequence of x values for plotting the density curve
x <- seq(-4, 4, length.out = 5000)

# Calculate the normal density function
y <- dnorm(x)

# Create the ggplot
ggplot() +
  geom_area(data = data.frame(x = x[x <= -t], y = y[x <= -t], fill = as.factor(1)),
            aes(x = x, y = y), fill = "#F8766D") +
  geom_area(data = data.frame(x = x[x >= t], y = y[x >= t], fill = as.factor(1)),
            aes(x = x, y = y), fill = "#F8766D") +
  geom_line(aes(x = x, y = y), linewidth = 1.5, color = "#00BFC4") +
#   geom_vline(xintercept = -1.96, linetype = 2) + geom_vline(xintercept = 1.96, linetype = 2) + 
  geom_vline(xintercept = -t, color = "#F8766D") + 
  annotate("text", x = -t + 0.15, y = 0.3, label = "t[2]", parse = TRUE, size = 8) + 
  geom_vline(xintercept = t, color = "#F8766D") + 
  annotate("text", x = t + 0.15, y = 0.3, label = "t[1]", parse = TRUE, size = 8) + 
  labs(x = "T", y = "") +
  theme_clean() + 
  scale_y_continuous(expand = c(0, 0), limits = c(0, 0.41)) + 
  theme(plot.background = element_blank(), legend.position="none")
```

b. Here, taking $n=49$, $\overline{x} = 3$, and $s = 14$. Thus, $$t = \frac{3}{14/\sqrt{49}} = \frac{3}{2} = 1.5.$$ To calculate a $p$-value, we make reference to the $t_{48}$ distribution, taking $F$ to be its CDF, this gives $$P(T \leq -1.5) + P(T > 1.5) = F(-1.5) + (1 - F(1.5)) = 2F(-1.5) \approx `r round(2 * pt(-1.5, df = 48), 3)`.$$ The corresponding area is identified in the following figure. 
```{r}
#| echo: false
#| fig-height: 3

library(ggplot2)
library(ggthemes)

# Define the value of t
t <- 1.5

# Create a sequence of x values for plotting the density curve
x <- seq(-4, 4, length.out = 5000)

# Calculate the normal density function
y <- dt(x, df = 48)

# Create the ggplot
ggplot() +
  geom_area(data = data.frame(x = x[x <= -t], y = y[x <= -t], fill = as.factor(1)),
            aes(x = x, y = y), fill = "#F8766D") +
  geom_area(data = data.frame(x = x[x >= t], y = y[x >= t], fill = as.factor(1)),
            aes(x = x, y = y), fill = "#F8766D") +
  geom_line(aes(x = x, y = y), linewidth = 1.5, color = "#00BFC4") +
#   geom_vline(xintercept = -1.96, linetype = 2) + geom_vline(xintercept = 1.96, linetype = 2) + 
  geom_vline(xintercept = t, color = "#F8766D") + 
  annotate("text", x = t + 0.1, y = 0.3, label = "t", parse = TRUE, size = 8) + 
  labs(x = "T", y = "") +
  theme_clean() + 
  scale_y_continuous(expand = c(0, 0), limits = c(0, 0.41)) + 
  theme(plot.background = element_blank(), legend.position="none")
```

c. Here, the observation has been $\widehat{p} = 14/100 = 0.14$, with $n=100$. Thus, $t = 0.04/0.03 = 4/3$. The $p$-value here is given by $$P\left(Z \leq \frac{-4}{3}\right) + P\left(Z \geq \frac{4}{3}\right) = \Phi\left(\frac{-4}{3}\right) + \left(1 - \Phi\left(\frac{4}{3}\right)\right) = 2\Phi\left(\frac{-4}{3}\right) \approx `r round(2 * pnorm(-4/3), 3)`.$$ The following plot shades the relevant region. 
```{r}
#| echo: false
#| fig-height: 3

library(ggplot2)
library(ggthemes)

# Define the value of t
t <- 4/3

# Create a sequence of x values for plotting the density curve
x <- seq(-4, 4, length.out = 5000)

# Calculate the normal density function
y <- dnorm(x)

# Create the ggplot
ggplot() +
  geom_area(data = data.frame(x = x[x <= -t], y = y[x <= -t], fill = as.factor(1)),
            aes(x = x, y = y), fill = "#F8766D") +
  geom_area(data = data.frame(x = x[x >= t], y = y[x >= t], fill = as.factor(1)),
            aes(x = x, y = y), fill = "#F8766D") +
  geom_line(aes(x = x, y = y), linewidth = 1.5, color = "#00BFC4") +
#   geom_vline(xintercept = -1.96, linetype = 2) + geom_vline(xintercept = 1.96, linetype = 2) + 
  geom_vline(xintercept = t, color = "#F8766D") + 
  annotate("text", x = t + 0.1, y = 0.3, label = "t", parse = TRUE, size = 8) + 
  labs(x = "T", y = "") +
  theme_clean() + 
  scale_y_continuous(expand = c(0, 0), limits = c(0, 0.41)) + 
  theme(plot.background = element_blank(), legend.position="none")
```
::::
:::

The $p$-value can be interpreted as the level of significance at which the observed data contradict the null hypothesis. In this sense, the $p$-value serves as a measure of how strong the evidence against the null hypothesis will be. An alternative method for the same mathematical approach is to determine how large of a test statistic value would need to be observed in order to achieve a $p$-value equal to the level of significance. If we observe a test statistic that is larger than this value that is then equivalent to observing a $p$-value which is smaller (and vice versa). This value is known as a **critical value**.

:::{#def-critical-value}
## Critical Value
A **critical value** is the value, derived through a null distribution, at which the corresponding $p$-value coincides with the significance level. That is, the critical value is the value, $t^*$, such that $$P(T \leq -t^*) + P(T \geq t^*) = \alpha,$$ assuming a two-tailed hypothesis test, where $T$ is the statistic of interest, and the probability is taken with respect to the null distribution. 
:::

The critical values are related to the percentiles of a distribution. Specifically, if $T$ has a symmetric distribution^[Such as the normal, or the $t$ distributions, which are both quite common.] then the critical value for a significance level $\alpha$ exactly coincides with the $1-\alpha/2$ percentile of the distribution. We saw these critical values being used when discussing confidence intervals, if we substitute the confidence level $p=1-\alpha$. If a critical value of $t^*$ is found then if we observe $t$ such that $|t| > t^*$, this is equivalent to observing a $p$-value that is less than $\alpha$. The $p$-value provides more information than a comparison to a critical value, however, the critical value technique can be easier to leverage for quick calculations, particularly when you are away from a computer. 

:::{#exm-finding-critical-values}
## Sadie and Charles Care for Trees: Identifying Critical Values
While they had some success identifying the $p$-values, Charles and Sadie realize that the conclusions that the calculations for the $p$-values are highly dependent on what values they assumed they would see. However, if they approached the problems via critical values, this will be relevant as long as the null distributions are correct. For each scenario, they wish to identify the critical value associated with the given hypothesis test. 

a. For the pouch of aspen seeds, they are planning to test $H_0: \theta_1 = 50$ versus $H_A: \theta_1 \neq 50$. This will be done using a test statistic with a null distribution of $N(0,1)$, at $\alpha = 0.1$. For the diameters, testing $H_0: \theta_2 = 10.5$ versus $H_A: \theta_2 \neq 10.5$, they will use the same null distribution and significance level. 
b. To understand the effects of their fertilizer types, they will test $H_0: \theta_3 = 0$ versus $H_A: \theta_3 \neq 0$. They wish to take $\alpha = 0.05$, and use a test statistic with $t_{n-1}$ degrees of freedom. They suspect the sample size will be $49$. 
c. To investigate the extent of the bark beetle infestation, they will test $H_0: \theta_4 \leq 0.1$ versus $H_A: \theta_4 > 0.1$. Here, they require a significance level of $\alpha = 0.01$.

::::{.callout .solution collapse='true'}
## Solution

a. Since these tests use the same significance levels, they will have the same critical values. Notably, we require $\Phi(-t^*) + 1-\Phi(t^*) = \alpha$, which is equivalent to saying $$\Phi(-t^*) = \frac{\alpha}{2} \implies t^* = -Z_{0.1/2} \approx `r round(-1 * qnorm(0.05), 2)`.$$
b. For this test, taking $F(t)$ to represent the cumulative distribution function for a $t_{48}$ distribution, we require $F(-t^*) + 1 - F(t^*) = \alpha$, which is equivalent to saying $$F(-t^*) = \frac{\alpha}{2} \implies t^* = -F^{-1}(\frac{0.05}{2}) = -t_{n-1,0.025} \approx `r round(-1 * qt(0.025, df = 48), 2)`.$$
c. For this test, the critical value must satisfy $1 - \Phi(t^*) = \alpha$, which is equivalent to $$\Phi(-t^*) = \alpha \implies t^* = -Z_{0.01} \approx `r round(-1 * qnorm(0.01),2)`.$$

::::
:::

With either the $p$-value calculated or the critical value for comparison, it is possible to draw scientific conclusions from the hypothesis test. 

### Drawing Conclusions and Interpretation
By comparing the calculated $p$-value to the level of significance, or the calculated test statistic to the critical value, we are able to draw conclusions from the hypothesis test. Our conclusion will provide an answer to whether we reject or fail to reject the null hypothesis. Specifically, if the $p$-value is less than the level of significance, or the observed test statistic is less than the critical value^[Or, equivalently, if the test statistic is larger in magnitude than the critical value.] then we can reject the null hypothesis, in favour of the alternative. We call the region of observations where we reject the null hypothesis the **rejection region** (see @fig-rejection-region-plot). Practically this means that the level of evidence contradicting the null hypothesis in the observed data exceeds the level of evidence we needed to see, in the given context, in order to act as though the null hypothesis is not true. If the $p$-value is larger than the level of significance (or equivalently, if the critical value does not fall into the rejection region) then we fail to reject the null hypothesis. This means that our sample did not contain sufficient contradictory evidence to the null hypothesis, based on the context of the situation. 

```{r}
#| echo: false
#| label: fig-rejection-region-plot
#| fig-height: 3
#| fig-cap: The labelled components of a hypothesis test. The observed test statistic is either compared to the critical values to see whether it falls into the rejection region or not. In a one-sided hypothesis test, the rejection region is one-sided as well. Alternatively, the area under the null distribution outside of the observed test statistic (in both tails, as depicted here, for a two-sided hypothesis test; in a single tail for a one-sided hypothesis test) can be computed as the $p$-value. The $p$-value is then compared to the level of significance.

library(ggplot2)
library(ggthemes)

# Define the value of t
t <- 2.2

# Create a sequence of x values for plotting the density curve
x <- seq(-3, 3, length.out = 5000)

# Calculate the normal density function
y <- dnorm(x)

# Create the ggplot
ggplot() +
  geom_area(data = data.frame(x = x[x <= -t], y = y[x <= -t], fill = as.factor(1)),
            aes(x = x, y = y), fill = "#F8766D") +
  geom_area(data = data.frame(x = x[x >= t], y = y[x >= t], fill = as.factor(1)),
            aes(x = x, y = y), fill = "#F8766D") +
  geom_line(aes(x = x, y = y), linewidth = 1.1, color = "#00BFC4") +
  geom_vline(xintercept = -1.96, linetype = 2) + geom_vline(xintercept = 1.96, linetype = 2) + 
  geom_vline(xintercept = -t, color = "#F8766D") + 
  annotate(geom = "text", x = -t-0.1, y = 0.18, label = "Observed Test Statistic", color = "#F8766D", angle = 90) + 
  annotate(geom = "text", x = 0, y = 0.09, label = "p-value", color = "#F8766D") + 
  annotate(geom = "text", x = 1.88, y = 0.18, label = "Critical Value", angle = 90) + 
  annotate(geom = "text", x = -1.88, y = 0.18, label = "Critical Value", angle = 270) + 
  labs(x = "T", y = "") +
  theme_clean() + 
  scale_y_continuous(expand = c(0, 0), limits = c(0, 0.41)) + 
  theme(plot.background = element_blank(), legend.position="none") + 
  annotate("segment", x = 1.96, y = 0.36, xend = 3, yend = 0.36, linewidth = 1.5, arrow = arrow(type = "closed", length = unit(0.04, "npc"))) + 
  annotate("text", x = 2.6, y = 0.39, label = "Rejection Region") + 
  annotate("segment", x = -1.96, y = 0.36, xend = -3, yend = 0.36, linewidth = 1.5, arrow = arrow(type = "closed", length = unit(0.04, "npc"))) + 
  annotate("text", x = -2.6, y = 0.39, label = "Rejection Region") + 
  annotate("segment",  x = 0, y = 0.075, xend = -t, yend = 0.02, arrow = arrow(type = "closed", length = unit(0.04, "npc")), color = "#F8766D") + 
  annotate("segment",  x = 0, y = 0.075, xend = t, yend = 0.02, arrow = arrow(type = "closed", length = unit(0.04, "npc")), color = "#F8766D")
```

Note, it is not possible in the hypothesis testing framework to accept the null hypothesis. This is because our level of evidence is always assessed for the degree to which it contradicts the null hypothesis. If our observations are unlikely assuming that the null hypothesis is true, it is reasonable for us to act as though the null hypothesis is not true. If, on the other hand, our observations do not contradict the null hypothesis, we will continue to act as though it were true, even though we have not built up positive evidence in its favour. 

When we reject the null hypothesis, we often say that the result is statistically significant at the $\alpha$ significance level. Statistical significance is a measure of strength of evidence against the null hypothesis. 

:::{#def-statistical-significance}
## Statistical Significance
A result is said to be **statistically significant** if the null hypothesis is rejected. Specifically, the result is said to be statistically significant at a set level of significance, though, this is often implied from context. A result is said to be not statistically significant if the null hypothesis is not rejected.
:::

:::{#exm-drawing-conclusions}
## Sadie and Charles Care for Trees: Drawing Conclusions
Charles and Sadie have managed to calculate test statistics, $p$-values, and critical values for the hypothesis tests that they established. All that is left is for them to interpret what these test results actually mean. They are looking to draw and interpret conclusions, based on the following information, and ideally, they would do this both using $p$-values and critical values. 

a. For the pouch of aspen seeds, they are testing $H_0: \theta_1 = 50$ versus $H_A: \theta_1 \neq 50$, using $$T_1 = \frac{\overline{X} - 50}{10/\sqrt{n}} \stackrel{H_0}{\sim} N(0,1).$$ In a sample of size $4$ they find $\overline{x} = 60$. For the diameters, they test $H_0: \theta_2 = 10.5$ versus $H_A: \theta_2 \neq 10.5$, using $$T_2 = \frac{\overline{X} - 10.5}{3/\sqrt{n}} \stackrel{H_0}{\sim} N(0,1).$$ In a sample of size $9$ they observe $\overline{x} = 8.5$. They wish to use $\alpha = 0.1$ level of significance.
b. To understand the effects of their fertilizer types, they test $H_0: \theta_3 = 0$ versus $H_A: \theta_3 \neq 0$ using $$T = \frac{\overline{X}}{s/\sqrt{n}} \stackrel{H_0}{\sim} t_{n-1}.$$ In a sample of $49$ trees, they observe $\overline{x} = 3$, with a sample standard deviation of $14$.  They wish to use $\alpha = 0.05$ level of significance.
c. To investigate the extent of the bark beetle infestation, they test $H_0: \theta_4 \leq 0.1$ versus $H_A: \theta_4 > 0.1$ using $$T=\frac{\widehat{p} - 0.1}{\sqrt{0.1(0.9)/n}} \stackrel{H_0}{\sim} N(0,1).$$ In a sample of size $100$ they observe $14$ trees that are infected. They wish to use $\alpha = 0.01$ level of significance. 

::::{.callout .solution collapse='true'}
## Solution

a. From the previous examples we found that the $p$-value was approximately $0.046$ for both tests. Since $p < \alpha$, we can **reject the null hypothesis**, and we conclude that, based on the sample, there is sufficient evidence to conclude that the heights and diameters of the found aspen seeds are different from average aspen seeds. These results are statistically significant at the $0.1$ level of significance. Alternatively, we could use the critical value of $1.64$. Since $t_1 = 2 > 1.64$, and since $t_2 = -2$, so $|t_2| > 1.64$, both statistics fall into the **rejection region**, and we reject the null hypothesis at the $0.1$ level of significance. 

b. Here we find $p = 0.14$. Since $p > \alpha$, we **fail to reject the null hypothesis** at the $\alpha = 0.05$ level of significance. Thus, we conclude that there is insufficient evidence in the sample to conclude that the fertilizer has a non-zero effect on growth. Alternatively, we can consider that $t = 1.5$, compared to a critical value of $2.01$. Since $t < 2.01$, the statistic does not fall in the rejection region, and we fail to reject the null hypothesis. 

c. The $p$-value was found to be $0.182$. Since we are testing at a $0.01$ level of significance, $p > \alpha$, and so we **fail to reject the null hypothesis**. As a result, there is insufficient evidence, based on the sample, to conclude that more than $10\%$ of the pine trees are infected by bark beetles. Alternatively, we consider the critical value to be $2.33$. Here, note that since this is a one-tailed test, the rejection region is only the values of $t$, such that $t > 2.33$. Because $t < 2.33$ we fail to reject the null hypothesis.
::::
:::

Statistical significance is the foundational language of scientific inquiry. Specifically, results that are statistically significant are typically held up as *actual results*, where those that are not statistically significant are treated as though they are the result of random noise. In order for a claim to be scientifically justified, it must be statistically significant. However, it is important to note that statistical significance is not a measure of how big or meaningful a particular effect will be. Instead, it is a measure of how strong the evidence for believing a result is. That is, you can have statistically significant findings that are not particularly meaningful. For instance, suppose that the parameter of interest is the effect of a medical treatment on life expectancy. If we find that the treatment improves the life expectancy for patients, and that this result is **statistically significant** we do not know whether the treatment improves the life expectancy by a lot or a little. All we know, in this context, is that there was strong evidence in the sample that the treatment actually did improve the outcome in this case.

## Errors in Hypothesis Testing 
When conducting a hypothesis test we are still dealing with uncertainty. As a result it is always possible that the conclusions that are drawn will be incorrect. By selecting an appropriate level of significance, and by selecting a test statistic with a known null distribution, we hope to minimize the possibility of errors. However, random variation can still lead to incorrect conclusions. When considering errors in the conclusions of a hypothesis test, there are two types of errors that can be made. We can either reject a null hypothesis that is actually true, or else we can fail to reject a null hypothesis that is actually false. These are^[Unhelpfully...] referred to as Type I and Type II errors, respectively.

:::{#def-type-1-error}
## Type I Error
A **type I** error is a false positive. This occurs when the null hypothesis is rejected, but the null hypothesis is actually true.
:::

:::{#def-type-2-error}
## Type II Error
A **type II** error is a false negative. This occurs when the null hypothesis is not rejected, but the null hypothesis is actually false. 
:::

Both type I and type II errors are problematic, and test procedures are preferable when they minimize the probabilities of each error type. Focusing on the probability that a type I error occurs, we can determine how likely this is by asking what the probability of rejecting the null hypothesis is, assuming that the null is true. That is, under the null hypothesis, what is the probability that we observe a statistic beyond the critical value? By definition, this is given by the level of significance. That is, the probability of a type I error is exactly equal to the level of significance, $\alpha$. 

This gives an alternative method of interpreting the value of $\alpha$. Specifically, the value of $\alpha$ is the probability of making a false positive claim. Intuitively, we want this to be as small as possible, and so taking $\alpha$ to be smaller seems reasonable. Unfortunately, the probability of making a type I error tends to be inversely correlated with the probability of making a type II error. The probability of a type II error is, typically, less straightforward to determine. For it, we need to determine the probability, assuming that the null hypothesis is actually false, that we fail to reject the null. That is, under the alternative hypothesis, what is the probability that we observe a statistic value that is less than the critical value. While this is a challenging probability to determine^[This is challenging for several reasons. One is that the test statistic is defined so that it has a well-defined null distribution. Unfortunately, to work out the type II error probability, the null distribution is not helpful. Instead, we need to work out the distribution of the test statistic assuming the null is not true. Unfortunately, the alternative is typically a **composite hypothesis**. That is, the alternative is specified as a range of possible values, rather than a single point. If the alternative is $H_A: \mu \neq 0$, should we take $\mu = 1$ or $\mu = -1$ or $\mu = \pi$? It is not always clear. As a result, we tend to define the probability of a type II error on the basis of a function, say $\pi'(\theta)$ that gives the probability of a false negative for any given value $\theta$.] explicitly, we give this the label $\beta$. If we consider the probability $1-\beta$, this gives the probability of rejecting the null hypothesis when it is actually false, a *true* rejection. This is a quantity of particular interest in hypothesis testing, and as such, we refer to it as **the power** of a hypothesis test.

:::{#def-power}
## Power
The **power** of a hypothesis test is the probability of rejecting the null hypothesis when the null hypothesis is, in fact, false. That is it gives the probability of a true rejection. The power of a test generally gives a measure of how well a test is able to detect true effects. Typically, the power of a test is denoted by $1-\beta$, where $\beta$ gives the probability of a type II error. 
:::

A hypothesis test would ideally have a low probability of a type I error, which is to say a low level of significance, and a corresponding high power. Unfortunately, typically as the level of significance of a test decreases, so too does the power, and vice versa. As a result, in any hypothesis testing scenario there is a need to make a deliberate trade-off between the level of significance and the power. Typically, hypothesis testing proceeds by selecting an appropriate level of significance for the given setting, and then among tests that achieve this level of significance, searching for a test with the highest power. This way the type I errors are controlled at a set level, and the type II errors are minimized. By recognizing that the probabilities of type I and type II errors need to be balanced against one another, it is possible to more intentionally select an appropriate level of significance. Specifically, it is worth thinking through, in any given setting, whether the prospect of a false positive or a false negative is a worse outcome, and then balancing the selected rates accordingly. 


+-----------------------------+---------------------+-------------------+
|                             | $H_0$ is True       | $H_0$ is False    |
+=============================+:===================:+:=================:+
| **$H_0$ is Rejected**       | False Positive <br> |  <br>             |
|                             | Type I Error   <br> |  True Positive    |
|                             | ($\alpha$)          |  Power ($1-\beta$)|
+-----------------------------+---------------------+-------------------+
| **$H_0$ is Not Rejected**   |                     | False Negative<br>|
|                             | True Negative  <br> | Type II Error <br>|
|                             |                     | ($\beta$)         |
+-----------------------------+---------------------+-------------------+

: Conclusions and errors of hypothesis testing. {#tbl-error-types .borderless tbl-colwidths="[20,20,20]"}

:::{#exm-types-of-errors}
## Charles and Sadie Care for Trees: Possible Errors
With an understanding of how they can draw conclusions using null hypothesis significance testing, Charles and Sadie turn to questioning "what if we are wrong?" Specifically, they want to think about different hypothesis tests that they can run as they continue their tree nursery, with a focus on understanding what a type I or type II error would refer to in each case, and whether they should prioritize a low level of significance or a high power for the corresponding tests. 

a. Charles and Sadie wish to consider whether the germination rate of a new seed variety is any different from the historical average. The new seed is more expensive than historical seeds, and they are not currently displeased with the seeds that they have been using.
b. They wish to investigate whether a new fertilizer that is substantially cheaper than their current fertilizer is offered. They wish to test whether there is an improvement of the new fertilizer, compared with the old one. 
c. Charles is told of a new treatment that promises to increase the disease resistance in seedlings. The treatment is relatively affordable, and does not seem to have any negative impacts, but they are concerned that it may not actually work. 
d. Sadie has been researching techniques for protecting plants through the winter. Their current strategy works reasonably well, but Sadie thinks that with a little more investment, they may be able to improve this dramatically. 

::::{.callout .solution collapse='true'}
## Solution
a. In this case, a Type I error represents the conclusion that the new seed variety differs from the historical average when, in fact, it does not. A type II error would occur if Sadie and Charles fail to reject the null hypothesis, concluding that there is no evidence of a difference between the current and historical seeds, when such a difference really does exist. Because they are currently happy with their current seeds, and because the new seeds are more expensive, they would want to be very certain that the new seeds are in fact different if they are going to switch. As a result, they should be considering a **lower level of significance** here, reducing the probability of type I error. 

b. In this case, a Type I error occurs if they conclude the new fertilizer is improved, if it really is not an improvement, where the type II error occurs if they see no evidence of improvement when there really is. Because the fertilizer is cheaper, they would save money if they managed to switch, and thus it is important for them to identify if there really is an improvement. Missing an improvement that exists is a costly mistake, and so they should prioritize **higher power** in order to minimize the negative consequences associated with missing out. 

c. In this case, a Type I error occurs if they conclude that the treatment actually does increase the resistance to diseases in plants, when it does not in actual fact. A type II error would occur if they conclude that there is no evidence for suggesting that the treatment reduces diseases, but it actually does. Here, because the treatment is affordable and seems otherwise harmless, it is worse to conclude that it does nothing if it actually works compared to concluding that it does nothing if it does not. As a result, a **higher power** is preferable. 

d. In this case, a Type I error occurs if they conclude that the new strategy works better than the old strategy, when it does not. Type II errors occur if the new strategy works better, but they do not find sufficient evidence to conclude this in the sample. Here, they currently have a functioning strategy, and it would cost more to move to a new one. As a result, they need to make sure that if they do move, it is an improvement. This corresponds to a **lower level of significance**. 

::::
:::

## Hypothesis Testing for Population Means

The general procedure for hypothesis testing is versatile and applies in a wide range of scenarios. Whenever there is a need to test a particular parameter, the outlined steps can be followed and, supposing that an appropriate test statistic is found, a valid hypothesis test will be derived. With that said, there are several parameters that are tested with enough frequency that the corresponding hypothesis testing procedures are worth studying specifically. In the following sections, we outline the specific procedures for testing hypotheses relating to population means and proportions. These procedures are specific instantiations of the previously outlined procedures, and serve as example use cases for the overarching framework of hypothesis testing.

### $Z$-Tests for Population Means in Normal Populations
Suppose that we are interested in testing hypotheses relating to the mean of a normal distribution, with a known variance. In this context, we have that $X_1,\dots,X_n \sim N(\mu, \sigma^2)$, where $\mu$ is unknown. We have seen that the sample mean, $\overline{X}$, has a sampling distribution given by $N(\mu,\dfrac{\sigma^2}{n})$. As a result, suppose that we wish to test the null hypothesis $H_0: \mu = \mu_0$. Then, assuming the null hypothesis holds, we must have that $E[\overline{X}] = \mu_0$, and more specifically, $$T = \frac{\overline{X} - \mu_0}{\sigma/\sqrt{n}} \stackrel{H_0}{\sim} N(0, 1).$$ Suppose that we observe $T = t$, then owing to symmetry we can compute the relevant $p$-value as $$P(T \leq -|t|) + P(T \geq |t|) = P(Z \leq |t|) + (1 - P(Z \leq -|t|)) = 2\Phi(-|t|).$$ If, instead, we had wished to use the critical value method, then we would need to solve $$2\Phi(-t^*) = \alpha \iff t^* = Z_{1-\alpha/2},$$ where $Z_{1-\alpha/2}$ is the critical value of the standard normal. If instead a one-tailed hypothesis test were run, we would take the $p$-value to be $\Phi(t)$ (or $1-\Phi(t)$), and the critical value as either $Z_{\alpha}$ or $Z_{1-\alpha}$, depending on whether the upper or lower tail test is being run. 

```{r}
#| echo: false
#| label: fig-z-test-plot
#| fig-height: 3
#| fig-cap: Illustration of the $Z$-test, depicting the observed statistic (solid line), area for the $p$-values (shaded areas), and locations of the critical value (dotted lines). In this example, the critical value is at $1.96$, and the observed statistic is at $-2.473$. Thus, it falls outside of the critical value. Additionally, the shaded area gives the p-value, $0.013$, which is less than the implied significance level of $0.05$. Consider where the critical values would fall had $\alpha = 0.01$ been used instead. 

library(ggplot2)
library(ggthemes)

# Define the value of t
t <- 2.473

# Create a sequence of x values for plotting the density curve
x <- seq(-4, 4, length.out = 5000)

# Calculate the normal density function
y <- dnorm(x)

# Create the ggplot
ggplot() +
  geom_area(data = data.frame(x = x[x <= -t], y = y[x <= -t], fill = as.factor(1)),
            aes(x = x, y = y), fill = "#F8766D") +
  geom_area(data = data.frame(x = x[x >= t], y = y[x >= t], fill = as.factor(1)),
            aes(x = x, y = y), fill = "#F8766D") +
  geom_line(aes(x = x, y = y), linewidth = 1.5, color = "#00BFC4") +
  geom_vline(xintercept = -1.96, linetype = 2) + geom_vline(xintercept = 1.96, linetype = 2) + 
  geom_vline(xintercept = -t, color = "#F8766D") + 
  labs(x = "T", y = "") +
  theme_clean() + 
  scale_y_continuous(expand = c(0, 0), limits = c(0, 0.41)) + 
  theme(plot.background = element_blank(), legend.position="none")
```

:::{.callout-tip icon="false"}
## $Z$-Tests for Population Means in Normal Populations with Known Variances
If data are selected from a normal population, with a known variance, then the **$Z$-test** is the most common test to apply for hypotheses surrounding the population mean. Specifically, the test statistic is $$T = \frac{\overline{X} - \mu_0}{\sigma/\sqrt{n}},$$ where $\mu_0$ is the value of the mean under the null hypothesis. Then, supposing $T=t$ is observed:

1. If $H_0: \mu = \mu_0$ versus $H_A: \mu \neq \mu_0$, the $p$-value is $2\Phi(-|t|) = 2(1 - \Phi(|t|))$. The corresponding critical value is $Z_{1-\alpha/2}$.
2. If $H_0: \mu \geq \mu_0$ versus $H_A: \mu < \mu_0$, the $p$-value is $\Phi(t)$. The corresponding critical value is $Z_{\alpha}$. 
3. If $H_0: \mu \leq \mu_0$ versus $H_A: \mu > \mu_0$, the $p$-value is $1 - \Phi(t)$. The corresponding critical value is $Z_{1-\alpha}$.
:::

### One Sample $t$-Tests for Population Means
Suppose instead that we are dealing with a population with an unknown variance, but we are still only interested in the mean of the population. If the population is either normally distributed, or else if the sample size is large enough to justify the use of the Central Limit Theorem, then we know that $\overline{X} \sim N(\mu, \sigma^2)$, however, $\sigma^2$ is unknown. We have seen that we can replace $\sigma$ with $s$, the sample standard deviation, and update the corresponding sampling distribution to a $t_{n-1}$. This result suggests that we run a $t$-test, taking $$T = \frac{\overline{X} - \mu_0}{s/\sqrt{n}} \stackrel{H_0}{\sim} t_{n-1}.$$ Suppose that we observe $T=t$, then owing to symmetry we can compute the relevant $p$-value as $$P(T \leq -|t|) + P(T \geq |t|) = 2F(-|t|),$$ where $F(t)$ is the cumulative distribution function for the $t_{n-1}$ distribution. If instead we wish to use the critical value method, then we would need to solve $$2F(-t^*) = \alpha \iff t^* = t_{n-1, 1-\alpha/2},$$ where $t_{n-1,1-\alpha/2}$ is the critical value from the $t_{n-1}$ distribution. If instead a one-tailed hypothesis test were run, we would take the $p$-value to be $F(t)$ (or $1-F(t)$), and the critical value as either $t_{n-1, \alpha}$ or $t_{n-1, 1-\alpha}$.

```{r}
#| echo: false
#| label: fig-t-test-plot
#| fig-height: 3
#| fig-cap: Illustration of the $t$-test, based on $n=32$, depicting the observed statistic (solid line), area for the $p$-values (shaded areas), and locations of the critical value (dotted lines). In this example, the critical value is at $2.04$, and the observed statistic is at $1.81$. Thus, it falls inside of the critical value. Additionally, the shaded area gives the p-value, $0.080$, which is greater than the implied significance level of $0.05$. Consider where the critical values would fall had a one-tailed hypothesis test been used instead.

library(ggplot2)
library(ggthemes)

# Define the value of t
t <- 1.81

# Create a sequence of x values for plotting the density curve
x <- seq(-4, 4, length.out = 5000)

# Calculate the normal density function
y <- dt(x, 31)

# Create the ggplot
ggplot() +
  geom_area(data = data.frame(x = x[x <= -t], y = y[x <= -t], fill = as.factor(1)),
            aes(x = x, y = y), fill = "#F8766D") +
  geom_area(data = data.frame(x = x[x >= t], y = y[x >= t], fill = as.factor(1)),
            aes(x = x, y = y), fill = "#F8766D") +
  geom_line(aes(x = x, y = y), linewidth = 1.5, color = "#00BFC4") +
  geom_vline(xintercept = -2.04, linetype = 2) + geom_vline(xintercept = 2.04, linetype = 2) + 
  geom_vline(xintercept = -t, color = "#F8766D") + 
  labs(x = "T", y = "") +
  theme_clean() + 
  scale_y_continuous(expand = c(0, 0), limits = c(0, 0.41)) + 
  theme(plot.background = element_blank(), legend.position="none")
```

:::{.callout-tip icon="false"}
## $t$-Tests for Population Means in Normal Populations or Large Samples
If data are selected from a normal population, or from a non-normal population with a sufficiently large sample size, with an unknown variance, then the **$t$-test** is the most common test to apply for hypotheses surrounding the population mean. Specifically, the test statistic is $$T = \frac{\overline{X} - \mu_0}{s/\sqrt{n}},$$ where $\mu_0$ is the value of the mean under the null hypothesis. Define the sample size to be $n$. Then, supposing $T=t$ is observed, and taking $F(t)$ to be the cumulative distribution function for the $t_{n-1}$ distribution:

1. If $H_0: \mu = \mu_0$ versus $H_A: \mu \neq \mu_0$, the $p$-value is $2F(-|t|) = 2(1 - F(|t|))$. The corresponding critical value is $t_{n-1, 1-\alpha/2}$.
2. If $H_0: \mu \geq \mu_0$ versus $H_A: \mu < \mu_0$, the $p$-value is $F(t)$. The corresponding critical value is $t_{n-1, \alpha}$. 
3. If $H_0: \mu \leq \mu_0$ versus $H_A: \mu > \mu_0$, the $p$-value is $1 - F(t)$. The corresponding critical value is $t_{n-1, 1-\alpha}$.
:::

### Hypothesis Tests for Population Proportions
Suppose that instead of concern with a population mean, we are instead concerned with a population proportion. We can either view the data as coming from a sample of $n$ independent and identically distributed Bernoulli random variables, with success probability $p$, or, equivalently, we can view these data as a single realization from a $\text{Bin}(n, p)$ distribution. In the first case, take $$\widehat{p} = \frac{1}{n}\sum_{i=1}^n X_i = \overline{X},$$ and so through the Central Limit Theorem, as long as $n$ is sufficiently large, $\widehat{p}$ will be approximately normal.^[From the binomial we can justify the equivalent conclusion by first considering the normal approximation to the binomial, and then scaling the results as required.] At this point, supposing we are testing a hypothesis of $p$ as it relates to $p_0$, then under the null $\widehat{p} \sim N(p_0, p_0(1-p_0)/n)$. As such, $$T = \frac{\widehat{p} - p_0}{\sqrt{\frac{p_0(1-p_0)}{n}}} \stackrel{H_0}{\sim} N(0, 1).$$ Correspondingly, we can apply a $Z$-test for the population proportions, following exactly the same procedure outlined above.

Note that in the case of a population proportion, the specification of the proportion under the null hypothesis also specifies the population variance. This is because, in a Bernoulli distribution, the mean and variance are governed by a single parameter. This means that we need not estimate the variance from the sample, nor consider the $t$ distribution. If the null hypothesis holds, then the population mean will be exactly $p_0$ and the variance $p_0(1-p_0)/n$. However, this is still an approximate hypothesis test since it relies on the use of the Central Limit Theorem to justify normality. 

## Further Considerations of Hypothesis Testing

The process of hypothesis testing is used extensively across a wide variety of scenarios. It is quite flexible, and can accommodate any types of data or any hypotheses of interest, so long as a suitable test statistic and null distribution can be derived. With that said, hypothesis testing is not a panacea for scientific inquiry. It needs to be carefully and critically applied, informed by the context of the situation and driven by our underlying subject-matter expertise. We saw the importance of this when informing the hypothesis to test, whether to use a one-sided or two-sided alternative, and in selecting a threshold for the level of significance. At every stage of the process care is required in order to ensure that the results of our hypothesis tests are useful and valid.

### Practical Significance versus Statistical Significance
A key consideration in the application of hypothesis testing is differentiating between those results that are statistically significant^[For some significance level, $\alpha$.] and those results that are **practically significant**. Practical significance, broadly speaking, indicates whether a particular effect actually matters given the underlying context. 

:::{#def-practical-significance}
## Practical Significance
A result is said to be **practically significant** if the effect is large enough to be meaningful in the real-world. Practical significance is not achieved on the basis of statistical analyses, but rather, on the basis of the subject-matter expertise. The degree of practical significance is somewhat subjective, and will depend on the particular context.
:::

To assess practical significance in practice we should ask ourselves, supposing that the observed effects are real, do we even care? Consider a lifestyle intervention that, in medical studies, is found to increase an individual's life expectancy by $1$ day. This result may or may not be statistically significant, depending on the degree of evidence in the observed sample. However, this result is almost certainly *not* practically significant. That is because, even if it were found to be a strong statistical result, it is likely not a result that is large enough to care about it in practice. On the other hand, a lifestyle intervention that increases life expectancy by $10$ years is likely to be very practically significant. 

A result is only meaningful if it achieves both statistical significance *and* practical significance. Without statistical significance, we do not have strong enough evidence to believe that the observed result is really different from our null hypothesis. Without practical significance, we do not care about the existence of the result broadly. The combined statistical and practical significance means that, not only is there good evidence for the result, but there is also value in considering the result. 

Critically, the degree of statistical significance is not a measure of practical significance. That is, a smaller $p$-value does not necessarily mean that the result is more practically significant. The measure of statistical significance is orthogonal to the measure of practical significance. Instead, to assess practical significance, we consider the size of the effect directly. The measure of effect size will be dependent on the given scenario and the specific parameter that is being assessed. This should be measured relative to known information in the field. Thus, if we are interested in the mean result, perhaps the effect size is given by the mean difference between the sample mean and the value under the null hypothesis. If we are interested in a proportion, we may take the absolute size of the proportion to measure the effect. Whatever value we can compare to our existing experience will serve as a basis for assessing the degree of practical significance that has been attained. 

### The Connection Between Hypothesis Testing and Confidence Levels

In discussing hypothesis testing we reintroduced many of the same concepts that were introduced during our discussions of confidence intervals as well. This is not coincidental, and it is not merely an aesthetic similarity. There is a fundamental connection between confidence intervals and hypothesis testing, one that renders them to be equivalent, in a sense, mathematically. More concretely, a confidence interval and a hypothesis test that are based on the same underlying statistic will always agree with one another. 

Suppose that a hypothesis test is run, testing $H_0: \theta = \theta_0$, and the computed $p$-value is $0.01$. This means that, assuming that the null hypothesis is true, there is a probability of $0.01$ that we would observe a result as extreme as the one we actually did. Now, recall that a confidence interval gives a range of values that, prior to actually conducting the sample, achieves some set probability of containing the true value. Suppose we construct a $95\%$ confidence interval. This interval will have a probability of $0.95$ of containing the truth, or put differently, there is a probability of $0.05$ that the truth does not fall inside this interval. If we construct the interval on the same data for which we found a $p$-value of $0.01$, where should we expect $\theta_0$ to fall relative to our interval? The probability, assuming $\theta = \theta_0$, that we observe a statistic as extreme as we did is $0.01$. This is notably smaller than the $0.05$ probability that the truth would fall outside the interval, and as such, we should not expect to see $\theta_0$ in the interval. 

Concretely, if a $100(1-\alpha)\%$ confidence interval is formed, and it contains the null value $\theta_0$, then the $p$-value for a hypothesis test of $H_0: \theta = \theta_0$ will have a $p$-value that is greater than or equal to $\alpha$. If the $100(1-\alpha)\%$ confidence interval is formed and does not contain the null value $\theta_0$, then the $p$-value for the hypothesis test will be less than $\alpha$. Equivalently, if the $p$-value when testing $H_0: \theta = \theta_0$ is found to be $\alpha$, then every confidence interval that is at least as wide as $100(1-\alpha)\%$ will contain $\theta_0$, and every interval that is more narrow than this will not contain the null value.

One way to conceptualize this more concretely is to consider that a confidence interval is effectively giving you the set of values that, based on your sample, you have no evidence to differentiate between. Any value in your confidence interval is justified by the sample in being considered the true value. This justification stems from the fact that, prior to the construction of the interval, there was only an $\alpha$ probability that the true value would not be in the interval, and so the values in this interval remain plausible based on the evidence. In a hypothesis test, we are trying to assess whether our sample contradicts a particular value for the truth, at a given threshold for evidence. If the value we are trying to reject is contained within our confidence interval, the logic of the confidence interval suggests that we should not reject it as possible. On the other hand, if the value we are interested in falls outside the confidence interval, our sample seems to reject its plausibility. 

### Critiques of Null Hypothesis Significance Testing
Despite the prevalence of hypothesis testing and the seemingly intuitive derivation of the procedure it is not without its faults, and it is not free of detractors. There are many critiques of hypothesis testing that are valid, and that should be seriously considered before an uncritical application of the procedures. The critiques of hypothesis tests tend to fall into two categories: there are the critiques about the application of hypothesis testing in practice, and there are critiques about the underlying foundation of hypothesis testing. The first can be remedied through careful application of the procedures we have discussed. The second represents a more philosophical debate, one in which competing sources of evidence need to be weighed against one another. Statisticians who are particularly critical of hypothesis testing, as it has been outlined here, often propose alternative procedures to accomplish similar goals. These frameworks are often rooted in fundamentally different approaches to the understanding of uncertainty, such as those emerging from the Bayesian paradigm. Such approaches to scientific knowledge and inquiry can be quite useful, but will not be pursued further in these notes. 

While there are many critiques of hypothesis testing, we focus on four foundational concerns. 

1. **Effects are (basically) never truly zero**. Most of the time, hypothesis testing is set up with a null hypothesis of $H_0: \theta = 0$. This typically refers to the effect of some intervention (such as a medical treatment), or the difference in outcomes achieved after making some change, or similar. The hypothesis test then seeks to determine, based on the observed evidence, is the effect of interest really different from zero. Many individuals critique this framing since, in practice, we should not expect any effect to be exactly zero. Every factor likely exerts some influence, no matter how small, on any related factors. It may not be practically significant in terms of its effect size, but the only quantities that will truly have no effect on each other are quantities that are not likely to be tested against one another. As a result, for many hypothesis tests, the framework is set up to disprove a statement that no one actually believes. We are not seeking evidence of effect size, just that it is different from zero, but likely we should approach every situation expecting the effects to be different from zero.
2. **In hypothesis testing, $p$-values are prioritized above all else.** $p$-values are, at their core, a statistic computed based on a particular sample that provide a measure of the level of evidence against a null hypothesis. This is a continuous measure of evidence, with any value $(0,1)$ being theoretically possible. The process of hypothesis testing takes this continuous measure and expresses it discretely as either "statistically significant" or else "not statistically significant", treating this discretization as the sole arbiter of whether an effect is real or not. Not only is there information lost by moving from a continuous measure to a discrete one based on only two categories, but this process also ignores other important sources of information that should likely be considered alongside a $p$-value. For instance, alongside a $p$-value we may wish to consider whatever prior evidence of the claim we may have, the scientific validity or plausibility of the underlying finding, the quality of the data or the study, the practical significance, and so forth. The critique in this context is not so much that $p$-values are not useful, but rather, that a sole reliance on $p$-values ignores many important factors. 
3. **The null hypothesis is often a *straw man***. This is related to the first point, but is more broad than the idea that we can typically assume that effects are non-zero. Recall that the null hypothesis is meant to be the default belief about the world, the state of affairs that we ought to believe without sufficient evidence contradicting it. A null hypothesis that actually satisfies this criterion is required for the utility of the hypothesis testing framework. Often, however, the null hypothesis that is selected is one that effectively no one would actually believe. This is a *straw man*.^[The concept of a straw man stems from the world of debate wherein sometimes individuals argue against a position that others do not really hold. The idea is that there is a position that is constructed that appears to be the one up for debate, but that is actually different in important ways. The person is then said to be attacking a straw man, as in, a human figure made of straw that would be easy to knock over or destroy.] If the null hypothesis is not truly believed by anyone, or if it is so unlikely as to be irrelevant, then refuting it with a hypothesis test tells us nothing. At the heart of the hypothesis testing framework is the concept of *falsification*. We are not looking for positive, confirmatory support in favour of a particular hypothesis, but rather looking to falsify the null hypothesis. If the idea that we are falsifying is not particularly relevant, then the fact that we have falsified it is also not particularly relevant. Unfortunately, in practice, irrelevant null hypotheses are often used.
4. **The hypothesis testing framework is ripe for abuse.** In any statistical analysis there are many choices that need to be made by individuals conducting the analysis. Often, there are many reasonable sounding choices that could lead to valid results, if those choices are guided by substantive considerations. Different choices which may be reasonable in different settings can, if applied to the same analysis, lead to very different conclusions. This is a reality in any statistical analysis, but is particularly concerning in the hypothesis testing framework where there is often a single goal for the researcher, namely, find a result that is statistically significant. A researcher that is setting out to achieve statistical significance at all costs is able to make many choices that aid in this attempt, if so desired. This process, known as *p-hacking*, can be intentional or unintentional, and can involve decisions that are reasonable from the outside or decisions that are explicitly fraudulent. For instance, an investigator may consider using a one-sided hypothesis test, a choice that, while reasonable in some settings, has the effect of halving the observed $p$-value. Researchers may throw out certain data that they deem to be *outliers* because they do not follow the underlying trend they are looking for. Researchers may perform many different analyses, and only report those that happened to be statistically significant.^[Consider a researcher performing $n$ independent hypothesis tests, each at a significance level of $p$. Note that the number of results they would expect to find as statistically significant, even if none of them truly are, follows a $\text{Bin}(n, p)$ distribution. If $p = 0.05$, then after $n=14$ tests there is a greater than $50\%$ chance that at least one of them returns a statistically significant result, despite none being true.] The arbitrary nature of the level of significance is another area of choice for the researchers -- if they calculate a $p$-value and find it to be $0.08$, taking $\alpha = 0.1$ allows them to report a statistically significant effect. The sum total of these choices means that any particular analysis, even if well-intentioned, may be subject to an artificially deflated $p$-value, achieving the moniker of statistical significance unfairly. 

These critiques are not just theoretical. There has been, in recent years, major concern over the replicability of research in a wide range of fields. While there are many aspects that drive this so-called replication crisis, the inappropriate use of hypothesis testing is certainly among the explanations. In light of these critiques, it is worth questioning whether the framework of hypothesis testing is worthy of pursuit. Generally, it is reasonable to answer this with a cautious "yes". However, it is critically important to take seriously these criticisms. Hypothesis testing should not be used to the exclusion of other forms of evidence. The hypothesis tests should not be dichotomized as "significant" or "not significant" where all results that are "significant" are treated as gospel and those that are "not significant" are treated as false. The selected null hypothesis should be a genuine null hypothesis, and the choices made in the analysis should be made prior to collecting or analyzing data on the basis of the context of the situation, rather than on the basis of what happens to be observed. Hypothesis testing should be used to support scientific inquiry, not as the sole tool of scientific inquiry. In short, the process of hypothesis testing provides a useful, statistical framework for assessing the degree of evidence that exists within a sample, however, it should never be applied uncritically.

## Hypothesis Testing in `R`
Hypothesis testing in R can proceed in one of two ways. First, test statistics can be directly computed, and then functions (such as `pnorm` or similar) can be used to compute $p$-values according to the null distribution. Alternatively, many common hypothesis tests are built into `R` to begin. The most relevant function, given the tests that were discussed throughout this chapter, is the `t.test` function. This will perform a $t$-test, based on a sample of data, contrasting the mean against a particular value. Moreover, the `t.test` function will, by default, produce confidence intervals for the mean, allowing both to be reported alongside one another. If you wish to perform a hypothesis test that is not built-in, then this can be accomplished using the same procedures outlined for confidence intervals.

::: {.content-visible when-format='pdf'}
```{r}
set.seed(31415)

n <- 50
X <- rnorm(n, 1, 1)

# H0: mu = 0, based on a Z-test
T <- (mean(X) - 0) / (1 / sqrt(n))
p <- 2 * pnorm(-1 * abs(T))

# H0: mu = 1, based on a t-test
t.test(X, mu = 1, alternative = "two.sided")

# t.test used for a 90% confidence interval
t.test(X, conf.level = 0.9)

# For alternative tests, you can consider:
#   - var.test, which provides a test for the variance
#   - prop.test, which provides a test of proportions
```
:::
::: {.content-visible when-format='html'}
```{webr-r}
set.seed(31415)

n <- 50
X <- rnorm(n, 1, 1)

# H0: mu = 0, based on a Z-test
T <- (mean(X) - 0) / (1 / sqrt(n))
p <- 2 * pnorm(-1 * abs(T))

# H0: mu = 1, based on a t-test
t.test(X, mu = 1, alternative = "two.sided")

# t.test used for a 90% confidence interval
t.test(X, conf.level = 0.9)

# For alternative tests, you can consider:
#   - var.test, which provides a test for the variance
#   - prop.test, which provides a test of proportions
```
:::


## Exercises {.unnumbered}

:::{#exr-15.1}
To test the hypothesis that a coin is fair, the following decision rule is adopted: 
Reject $H_0$ if the number of heads in a sample of size $100$ is not at least $40$ and at most $60$ and fail to reject $H_0$ otherwise.

a. Find the Type I error probability.
a. What conclusion would you draw if the sample of $100$ tosses yielded $53$ heads? $60$ heads? 
a. Could your conclusions in (b) be wrong? Explain.

:::

:::{#exr-15.2}
a. Design a decision rule, based on counting the number of heads, to test the hypothesis that a coin is fair if a sample of $64$ tosses of the coin is taken, assuming $\alpha=0.05$. 
a. Does your proposed decision rule have significance of exactly $\alpha$? If yes, explain. If not, how could the rule be modified to achieve this?
a. How could you design the decision rule to completely avoid a Type II error?
:::

:::{#exr-15.3}
For the following scenarios, indicate a reasonable $\alpha$ level for a significance test, assuming that the results of a significance test will translate into decisions. Justify your answer. 

a. A new drug is being developed for a common condition with multiple treatment options available. Safety tests have been completed, and the current study aims to evaluate its effectiveness.
a. In the field of civil engineering, a new construction method is being tested for building stability. The cost of potential failure is high. 
a. A clinical trial is conducted to determine whether a new medical device is more effective than the current standard of care for a common health condition.
a. A software development team is testing a new algorithm for data compression. The current compression method is widely accepted and used. 
a. An environmental study is conducted to assess the impact of a new manufacturing process on air quality in a city.
a. A pharmaceutical company is conducting a study to evaluate the effectiveness of a new vaccine against a highly contagious disease. Safety data is available.
a. A new dietary supplement is being tested for its potential to improve cognitive function in adults. Safety data is limited.
a. In the context of transportation engineering, a new traffic management system is being evaluated for its impact on reducing congestion.
a. A study is conducted to determine whether a new teaching method improves student performance in mathematics compared to the traditional method.
a. A marketing team is testing the effectiveness of two different advertising campaigns for a popular consumer product.
a. A public health study examines the association between a specific lifestyle factor and the risk of a common chronic disease. 
a. A survey is conducted to assess public opinion on a proposed policy change in a democratic society. 
a. In the context of entertainment and media, a study aims to determine whether a new type of streaming service subscription increases customer satisfaction compared to traditional cable TV. 
a. An architectural firm is testing a new building material for its energy efficiency in construction. The current material is well-established.
a. A study investigates whether a new social media platform leads to increased user engagement compared to existing platforms.
a. A new drug is being trialed to treat a rare disease which does not currently have any viable treatment pathways. The drug has been proven to be safe, and the current testing is simply around efficacy.
a. A new drug is being trialed to treat a rare disease which currently has a moderately effective treatment. The drug has been proven to be safe, and the current testing is simply around the efficacy in comparison to the existing treatment.
a. A new drug is being trialed to treat a rare disease which currently has a moderately effective treatment. The drug has not been tested for safety. The current testing is around the efficacy is comparison to the existing treatment. 
a. You have a bet with a friend, with no wager beyond "bragging rights", over whether green skittles appear less frequently than red skittles in bags. 
:::

:::{#exr-15.4}
For each of the following, indicate whether it is a valid statistical hypothesis or not, and why. 

a. $H: \sigma > 100$;
a. $H: s^2 \leq 0.2$;
a. $H: \overline{X} - \overline{Y} = 5$;
a. $H: \lambda \leq 2$;
a. $H: \theta = 10$;
a. $H: \frac{\sigma_1}{\sigma_2} < 1$.
:::

:::{#exr-15.5}
For each of the following, draw the correct conclusion (with statement) regarding the test of the hypothesis with the associated p-value. Suppose that each test is for $H_0: \theta = 0$, where $\theta$ represents the average error in length from a manufacturing process. 

a. $p = 0.084$, $\alpha = 0.05$;
a. $p = 0.003$, $\alpha = 0.001$;
a. $p = 0.498$, $\alpha = 0.05$;
a. $p = 0.084$, $\alpha = 0.10$;
a. $p = 0.039$, $\alpha = 0.01$;
a. $p = 0.218$, $\alpha = 0.10$.
:::

:::{#exr-15.6}
To determine whether the pipe welds in a nuclear power plant meet specifications, a random sample of welds is selected, and tests are conducted on each weld in the sample. Weld strength is measured as the force required to break the weld. Suppose the specifications state that mean strength of welds should exceed $100$ pounds per square inch. The inspection team decides to test $H_0: \mu \leq 100$ versus $H_1: \mu > 100$. Explain why it might be preferable to use this $H_1$ rather than $\mu < 100$.
:::

:::{#exr-15.7}
Let $\mu$ denote the true average radioactivity level. The value $5$ is considered the dividing line between safe and unsafe water. Would you recommend testing $H_0: \mu \leq 5$ or $H_0: \mu \geq 5$? Why?
:::

:::{#exr-15.8}
Before agreeing to purchase a large order of a material, a company wants to see conclusive evidence that the true standard deviation of the material thickness is less than $.05$mm. What hypotheses should be tested, and why? In this context, what are the type I and type II errors?
:::

:::{#exr-15.9}
Many older homes have electrical systems that use fuses rather than circuit breakers. A manufacturer of $40$-amp fuses wants to make sure that the mean amperage at which its fuses burn out is in fact 40. To verify the amperage of the fuses, a sample of fuses is to be selected and inspected. If a hypothesis test were to be performed on the resulting data, what null and alternative hypotheses would be of interest to the manufacturer? Describe type I and type II errors in the context of this problem situation.
:::

:::{#exr-15.10}
Water samples are taken from water used for cooling as it is being discharged from a power plant into a river. It has been determined that as long as the mean temperature of the discharged water is at most $150$F, there will be no negative effects on the river's ecosystem. To investigate whether the plant is in compliance with regulations, $50$ water samples will be taken at randomly selected times and the temperature of each sample recorded. The resulting data will be used to test the hypotheses $H_0: \mu \leq 150$ versus $H_1: \mu > 150$. In the context of this situation, describe type I and type II errors. Which type of error would you consider more serious? Explain.
:::

:::{#exr-15.11}
A regular type of laminate is currently being used by a manufacturer of circuit boards. A special laminate has been developed to reduce warping. The regular laminate will be used on one sample of specimens and the special laminate on another sample, and the amount of warping will then be determined for each specimen. The manufacturer will then switch to the special laminate only if it can be demonstrated that the true average amount of warping for that laminate is less than for the regular laminate. State the relevant hypotheses, and describe the type I and type II errors in the context of this situation.
:::


:::{#exr-15.12}
The breaking strength of cables produced by a manufacturer have mean $1800$ lbs and standard deviation of $100$ lbs. By a new technique in the manufacturing process it is claimed that the breaking strength can be increased. To test this claim a sample of $50$ cables is tested, and it is found that the mean breaking strength is $1850$. Can we support the claim with a $0.01$ level of significance?
:::

:::{#exr-15.13}
Recently, many companies have been experimenting with telecommuting. One possible benefit from the company's perspective is to reduce the number of sick days taken. Suppose that it is known that at one company employees have taken a mean of $5.4$ sick days over the past year. A sample of $80$ employees is taken after the policy change, and these employees average $4.5$ sick days with a standard deviation of $2.7$. Let $\mu$ represent the mean number of sick days for all employees. Test the hypothesis $H_0: \mu \geq 5.4$.
:::

:::{#exr-15.14}
A certain type of stainless steel powder is supposed to have a mean particle diameter of $\mu = 15\nu$m. A random sample of $87$ particles had a mean diameter of $15.2\mu$m, with standard deviation of $1.8\mu$m. Test the hypothesis $H_0: \mu = 15$. 
:::

:::{#exr-15.15}
An engineer takes several independent measurements of the length of a component and obtains $\overline{X} = 5.2$mm and standard deviation $0.1$mm. Use this information to find the p-value for testing $H_0: \mu = 5.0$.
:::

:::{#exr-15.16}
Consider the following output from statistical software. Using this information: 

a. State the details of the test that was performed, along with the conclusion.
a. Use the output to test $H_0: \mu \geq 73.6$. 
a. Use the output to compute a $99\%$ confidence interval for $\mu$.

> **One-sample:**
> 
> $H_0: \mu = 73.5$. 

| Variable |   N   |   Mean    | Std. Dev | S.E. Mean |        95% CI        |    Z    |    P    |
| :------- | :---: | :-------: | :------: | :-------: | :------------------: | :-----: | :-----: |
| X        | $145$ | $73.2461$ | $2.364$  | $0.1963$  | $(72.8614, 73.6308)$ | $-1.29$ | $0.196$ |

:::

:::{#exr-15.17}
In the past a machine has produced washers having a mean thickness of $0.05$ inches. To determine whether the machine is in proper working order, a sample of $10$ washers is chosen for which the mean thickness is $0.053$ inches and the standard deviation is $0.003$ inches. Test the hypothesis that the machine is in proper working order using a level of significance of $0.05$.
:::

:::{#exr-15.18}
A test of the breaking strengths of $6$ ropes manufactured by a company showed a mean breaking strength of $7750$ lb and a standard deviation of $145$ lb, whereas the manufacturer claimed a mean breaking strength of $8000$ lb. Do the data contradict the manufacturers claim?
:::

:::{#exr-15.19}
A machine produces marbles whose diameters are normally distributed with mean $12.00$mm. After modification of the machine, the diameters of a random sample of $105$ marbles produced were found to have a mean of $12.010$mm and a standard deviation of $0.05$mm. Would you conclude that the change has affected the mean diameter produced by the machine?
:::

:::{#exr-15.20}
The following hypothetical data sets represent the results from weighing a standard weight that is known to have a mass of $100$g. Assume that the readings are a random sample from a population that follows the normal curve. For each of the following perform a hypothesis test indicating whether the scale is properly calibrated, or explain why it is not possible to do so. 

a. $\{100.02, 99.98, 100.03\}$;
a. $\{100.01\}$.

:::

:::{#exr-15.21}
A certain manufactured product is supposed to contain $23\%$ potassium by weight. A sample of $10$ specimens of this product has an average percentage of $23.2\%$ with a standard deviation of $0.2\%$. Run a hypothesis test to determine whether the process is correctly calibrated or not. 
:::

:::{#exr-15.22}
Consider the following output from statistical software. Using this information: 

a. State the details of the test that was performed, along with the conclusion.
a. Use the output to test $H_0: \mu \geq 6.5$. 
a. Use the output to compute a $99\%$ confidence interval for $\mu$.

> **One-sample:**
> 
> $H_0: \mu \leq 5.5$. 

| Variable |  N  |   Mean    | Std. Dev  | S.E. Mean | 95% Lower Bound |   Z    |    P    |
| :------- | :-: | :-------: | :-------: | :-------: | :-------------: | :----: | :-----: |
| X        | $5$ | $5.92563$ | $0.15755$ | $0.07046$ |    $5.77542$    | $6.04$ | $0.002$ |
:::

:::{#exr-15.23}
A random sample of $300$ electronic components manufactured by a certain process are tested, and $25$ are found to be defective. Let $p$ represent the proportion of components manufactured by this process that are defective. The process engineer claims that $p \leq 0.05$. Does the sample provide enough evidence to reject this claim?
:::

:::{#exr-15.24}
A survey of $444$ HIV-positive smokers was taken, with $281$ male and $163$ female respondents. Consider this to be a simple random sample. Can you conclude that more than $60\%$ of HIV-positive smokers are male?
:::

:::{#exr-15.25}
A weight that was known to be $150$g was placed on each of $50$ kitchen scales. The readings on $29$ of the scales were too light, and the remaining $21$ were too heavy. Can you conclude that more than half of kitchen scales underestimate weight?
:::

:::{#exr-15.26}
A study of $304$ individuals were asked to choose a physician based on hypothetical descriptions. One was described as having high technical skills, while the other was described as having high interpersonal skills. $62\%$ of the people chose the physician with high technical skills. Can you conclude that more than half of patients prefer a physician with high technical skills?
:::

:::{#exr-15.27}
In a survey of $500$ residents in a certain town, $274$ said they were opposed to the construction of a new mall. Test whether more than half of the residents are opposed to the construction.
:::

:::{#exr-15.28}
Of $113$ people undergoing a certain hip procedure, $65$ had surgery on their right hip. Can you conclude that there is a difference in frequency of this procedure between the right and left hips?
:::

:::{#exr-15.29}
A grinding machine will be qualified for a particular task if it can be shown to produce less than $8\%$ defective parts. In a random sample of $300$ parts, $12$ were defective. Can the machine be qualified?
:::

:::{#exr-15.30}
The manufacturer for a patented medicine claimed that it was $90\%$ effective in relieving an allergy for a period of $8$ hours. In a sample of $200$ people who had the allergy, the medicine provided relief for $160$ people. Determine the legitimacy of the manufacturer's claim.
:::

:::{#exr-15.31}
Describe the difference between practical significance and statistical significance.
:::

:::{#exr-15.32}
Give an example of something which is statistically significant but not practically significant.
:::

:::{#exr-15.33}
Can you find a result which is practically significant but not statistically significant? Explain.
:::

:::{#exr-15.34}
Consider carrying out $m$ tests of hypotheses, independently, based on a significance level of $0.01$. 

a. What is the probability of committing at least $1$ type I error with $m=5$? With $m=10$?
a. How many tests would need to be performed until the probability of a type I error exceeds $0.5$?
:::

:::{#exr-15.35}
Suppose that a $90\%$ confidence interval for $\theta$ is found to be $(a, b)$. Describe how this can be used to test the hypothesis $H_0:\theta = c$. 
:::

::: {.content-visible when-format='html'}

## Self-Assessment {.unnumbered}

Note: the following questions are still experimental. Please contact me if you have any issues with these components. This can be if there are incorrect answers, or if there are any technical concerns. Each question currently has an ID with it, randomized for each version. If you have issues, reporting the specific ID will allow for easier checking!

For each question, you can check your answer using the checkmark button. You can cycle through variants of the question by pressing the arrow icon. 


```{r}
#| echo: false
#| message: false
#| warning: false

library(exams2forms)
set.seed(31415)

```

:::{#sa-15.01}
```{r}
#| echo: false
#| message: false
#| results: 'asis'
#| warning: false
#| cache: true
exams2forms("15.033.CriticalValueTechnique-TwoTail.Rmd", 
            edir = "../PracticeQuestions/Chapter15", 
            n = 100)
```
:::

:::{#sa-15.02}
```{r}
#| echo: false
#| message: false
#| results: 'asis'
#| warning: false
#| cache: true
exams2forms("15.033.CriticalValueTechnique-OneTailLower.Rmd", 
            edir = "../PracticeQuestions/Chapter15", 
            n = 100)
```
:::

:::{#sa-15.03}
```{r}
#| echo: false
#| message: false
#| results: 'asis'
#| warning: false
#| cache: true
exams2forms("15.033.CriticalValueTechnique-OneTailUpper.Rmd", 
            edir = "../PracticeQuestions/Chapter15", 
            n = 100)
```
:::

:::{#sa-15.04}
```{r}
#| echo: false
#| message: false
#| results: 'asis'
#| warning: false
#| cache: true
exams2forms("15.034.TypeIandTypeIIErrors-schoice.Rmd", 
            edir = "../PracticeQuestions/Chapter15", 
            n = 70)
```
:::

:::{#sa-15.05}
```{r}
#| echo: false
#| message: false
#| results: 'asis'
#| warning: false
#| cache: true
exams2forms("15.039.PowerCalculations-Z-test.Rmd", 
            edir = "../PracticeQuestions/Chapter15", 
            n = 100)
```
:::

:::{#sa-15.06}
```{r}
#| echo: false
#| message: false
#| results: 'asis'
#| warning: false
#| cache: true
exams2forms("15.042.PValueConversions.Rmd", edir = "../PracticeQuestions/Chapter15", n = 50)
```
:::


:::