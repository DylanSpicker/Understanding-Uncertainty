<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>5&nbsp; Summarizing Statistical Experiments with Random Variables – Understanding Uncertainty</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../notes/chapter6.html" rel="next">
<link href="../notes/chapter4.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-2486e1f0a3ee9ee1fc393803a1361cdb.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-9cf7f73ae95708c47935cfd4a35bc870.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script src="../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../notes/chapter1.html">Part 1: Probability</a></li><li class="breadcrumb-item"><a href="../notes/chapter5.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Summarizing Statistical Experiments with Random Variables</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Understanding Uncertainty</a> 
        <div class="sidebar-tools-main">
    <a href="../Understanding-Uncertainty.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Part 1: Probability</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/chapter1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction to Probability</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/chapter2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">The Mathematical Foundations of Statistical Experiments</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/chapter3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">The Core Concepts of Probability</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/chapter4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Probabilities with More than One Event</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/chapter5.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Summarizing Statistical Experiments with Random Variables</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/chapter6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">The Expected Value, Location Summaries, and Measures of Variability</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/chapter7.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Expectations and Variances with Multiple Random Variables</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/chapter8.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">The Named Discrete Distributions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/chapter9.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Continuous Random Variables</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Part 2: Statistics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/chapter10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Introduction to Statistics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/chapter11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">An Introduction to Descriptive Statistics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/chapter12.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Sampling Distributions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/chapter13.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Methods of Estimation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/chapter14.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Confidence Intervals</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/chapter15.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">The Basics of Null Hypothesis Significance Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/chapter16.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Hypothesis Testing and Confidence Intervals in Two Populations</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/chapter17.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Nonparametric Hypothesis Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/chapter18.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">The Analysis of Categorical Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/chapter19.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Simple Linear Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/chapter20.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">The One-Way ANOVA</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#the-need-for-random-variables" id="toc-the-need-for-random-variables" class="nav-link active" data-scroll-target="#the-need-for-random-variables"><span class="header-section-number">5.1</span> The Need for Random Variables</a></li>
  <li><a href="#probability-distributions-and-probability-mass-functions" id="toc-probability-distributions-and-probability-mass-functions" class="nav-link" data-scroll-target="#probability-distributions-and-probability-mass-functions"><span class="header-section-number">5.2</span> Probability Distributions and Probability Mass Functions</a></li>
  <li><a href="#multiple-random-variables-and-joint-probability-mass-functions" id="toc-multiple-random-variables-and-joint-probability-mass-functions" class="nav-link" data-scroll-target="#multiple-random-variables-and-joint-probability-mass-functions"><span class="header-section-number">5.3</span> Multiple Random Variables and Joint Probability Mass Functions</a>
  <ul class="collapse">
  <li><a href="#joint-probability-distributions-as-contingency-tables" id="toc-joint-probability-distributions-as-contingency-tables" class="nav-link" data-scroll-target="#joint-probability-distributions-as-contingency-tables"><span class="header-section-number">5.3.1</span> Joint Probability Distributions as Contingency Tables</a></li>
  </ul></li>
  <li><a href="#independence-of-random-variables" id="toc-independence-of-random-variables" class="nav-link" data-scroll-target="#independence-of-random-variables"><span class="header-section-number">5.4</span> Independence of Random Variables</a></li>
  <li><a href="#conditional-probability-distributions" id="toc-conditional-probability-distributions" class="nav-link" data-scroll-target="#conditional-probability-distributions"><span class="header-section-number">5.5</span> Conditional Probability Distributions</a></li>
  <li><a href="#manipulating-probabilities-with-random-variables" id="toc-manipulating-probabilities-with-random-variables" class="nav-link" data-scroll-target="#manipulating-probabilities-with-random-variables"><span class="header-section-number">5.6</span> Manipulating Probabilities with Random Variables</a></li>
  <li><a href="#independent-and-identically-distributed-a-framework-for-interpretation" id="toc-independent-and-identically-distributed-a-framework-for-interpretation" class="nav-link" data-scroll-target="#independent-and-identically-distributed-a-framework-for-interpretation"><span class="header-section-number">5.7</span> Independent and Identically Distributed: A Framework for Interpretation</a></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises">Exercises</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../notes/chapter1.html">Part 1: Probability</a></li><li class="breadcrumb-item"><a href="../notes/chapter5.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Summarizing Statistical Experiments with Random Variables</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Summarizing Statistical Experiments with Random Variables</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="the-need-for-random-variables" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="the-need-for-random-variables"><span class="header-section-number">5.1</span> The Need for Random Variables</h2>
<p>When introducing probability originally we worked from a sample space and then corresponding events. This is a very general framework which allows us to effectively analyze any statistical experiment. Sample spaces are not restricted to be numeric, for instance, and events are simply subsets of the sample space. As a result, this framework provides the tools for capturing uncertainty quantification in almost any setting. Still, the need to enumerate sample spaces and events over complex sets of arbitrary items is cumbersome and may prevent succinct representations of the underlying phenomena. Often, rather than caring about the entire space of outcomes from an experiment of interest, we are primarily concerned we a summary of the experiment. When we can summarize the experiment using a numerical quantity, we are able to define a random variable.</p>
<div id="def-random-variable" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.1 (Random Variable)</strong></span> A random variable is a numeric quantity whose specific value depends on chance through the outcome of a statistical experiment. Formally, a random variable is a mapping from the result of an experiment to a set of numbers.</p>
</div>
<p>By reporting the numeric value of the random variable, we are able to summarize the key part of the experiment, succinctly.</p>
<p>For instance, suppose that we are repeatedly tossing a coin. If we toss the coin <span class="math inline">\(100\)</span> times, then the sample space is going to consist of <span class="math inline">\(2^{100}\)</span> total possible outcomes, each of which is a sequence of <span class="math inline">\(100\)</span> heads and tails.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> Instead, it may be more convenient to assign a random variable to be the number of heads that show up on the <span class="math inline">\(100\)</span> tosses of the coin. In this case, the random variable takes on a non-negative integer between <span class="math inline">\(0\)</span> and <span class="math inline">\(100\)</span>. In many situations, such a summary may be all the is relevant from the experiment.</p>
<p>It is important to recognize that information <em>is</em> lost when we define this random variable. If <span class="math inline">\(X\)</span> summarizes the number of heads in <span class="math inline">\(100\)</span> tosses of a coin, then provided <span class="math inline">\(X\)</span> you are not able to answer the question “what was the 23rd toss of the coin?” As a result, random variables smooth over the unnecessary information, summarizing the parts of the statistical experiment that we care for. For any statistical experiment, however, we need to carefully define the random variables which are truly of interest to us.</p>
<p>For instance, if the <span class="math inline">\(23\)</span>rd toss of the coin was integral to our decision-making, then perhaps we define a random variable <span class="math inline">\(Y\)</span> which counts the number of heads that show up on the <span class="math inline">\(100\)</span> tosses of the coin, but does so with negative numbers if the <span class="math inline">\(23\)</span>rd toss was a tail, and with positive numbers if it was a head. Then, provided <span class="math inline">\(Y\)</span> you can answer “how many heads showed up in the tosses of the coin? by calculating <span class="math inline">\(|Y|\)</span>, and you can answer”what was the <span class="math inline">\(23\)</span>rd toss of the coin?” by looking at <span class="math inline">\(\text{sign}(Y)\)</span>.</p>
<div id="exm-random-var" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.1 (Random Variables at the Coffee Shop)</strong></span> Back at their favourite coffee shop, Charles and Sadie are sitting in their favourite chairs, discussing random variables and watching the cash register for the inevitable sequence of customers who will arrive there. Charles suggests playing a game together, which is creatively called “how many random variables can we name that have to do with the statistical experiment of watching for customers at a cash register?” Seeing as how catchy the name is, Sadie is excited to play along, and so they start.</p>
<p>Name several distinct random variables which could be observed via the described statistical experiment.</p>
<div class="solution callout callout-style-simple callout-none no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Solution
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>There are essentially countless different random variables that could be named here, depending on what is of interest. The important concept is that each random variable needs to be a numeric quantity which is calculable from the statistical experiment. For instance:</p>
<ul>
<li>The number of people who arrive at the cashier in the next hour.</li>
<li>The length of time until the next customer arrives at the cashier.</li>
<li>The amount of money that the next customer spends when paying at the cashier.</li>
<li>A value of <span class="math inline">\(1\)</span> if the next customer is wearing a hat, and a value of <span class="math inline">\(0\)</span> otherwise.</li>
<li>The number of different items that are ordered by all customers over the next hour.</li>
<li>The number of words that the cashier says to the next customer, prior to payment.</li>
<li>A count of the number of red items of clothing that can be seen being worn by the next <span class="math inline">\(15\)</span> customers.</li>
</ul>
</div>
</div>
</div>
</div>
<p>Because of their ability to summarize effectively and flexibly the pertinent components of a statistical experiment, random variables are the default paradigm for discussing randomness. When discussing a random variable we will typically use capital letters, such as <span class="math inline">\(X\)</span>, to represent the random quantity with an unknown value. In the event that an experiment is actually performed, and a value is realized for the random variable, we will record this value as a lower case letter, such as <span class="math inline">\(x\)</span>. For instance, the number of heads showing in <span class="math inline">\(100\)</span> flips of a coin is an unknown quantity depending on chance which we call <span class="math inline">\(X\)</span>. Once we have flipped the coin <span class="math inline">\(100\)</span> times and observed <span class="math inline">\(57\)</span> heads, we denote this as <span class="math inline">\(x=57\)</span>.</p>
<p>The importance of this notation is merely to emphasize what values are unknown and random, and what values are known numeric quantities. Because <span class="math inline">\(x\)</span> is a known value taking on some set number we will not often speak of probabilities involving <span class="math inline">\(x\)</span>.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> Instead, we wish to translate the language of probability that we have built to statements regarding the random variable <span class="math inline">\(X\)</span>.</p>
<p>The random variable <span class="math inline">\(X\)</span> has a corresponding “sample space” of possible values that it can take on. We will typically refer to this as the <em>support</em> of the random variable, though borrowing other terms from math courses (such as <em>range</em>) will work also. This support, which we can think of as directly analogous to the sample space of arbitrary elements from before, will depend on the possible realizations from the underlying experiment. We typically will denote the support of a random variable <span class="math inline">\(X\)</span> as <span class="math inline">\(\mathcal{X}\)</span>. After the experiment has been performed the random variable will take on a single value from this set.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> We can sometimes compactly describe the set of possible values for a random variable, for instance, by stating all of the integers, or integers between <span class="math inline">\(5\)</span> and <span class="math inline">\(10\)</span>, or even values less than <span class="math inline">\(1000\)</span>. This allows for compact descriptions of <span class="math inline">\(\mathcal{X}\)</span> even when the set <span class="math inline">\(\mathcal{S}\)</span> is challenging to describe. The probability of realizing any outcome in <span class="math inline">\(\mathcal{X}\)</span> is dictated by the underlying probability model.</p>
<p>When introducing the concepts of probability we indicated that probability was assigned to events. With random variables, this remains true. As a result we need to define events in terms of random variables. When we have a random variable, <span class="math inline">\(X\)</span>, an event is defined as any set of values that it can take on. For instance, we may have the event <span class="math inline">\(X=4\)</span>, or the event <span class="math inline">\(X \geq 18\)</span>, or the event <span class="math inline">\(2 \leq X \leq 93\)</span>, or the event <span class="math inline">\(X \in \{2,4,6,8,10\}\)</span>. In each case these are subsets of the possible values that the random variable can take on, based on the outcome of the experiment.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<p>Just as before these events can be simple events (comprised of a single outcome) or compound events (comprising of multiple outcomes). The event <span class="math inline">\(\{X=5\}\)</span> is a simple event, whereas the event <span class="math inline">\(\{X \geq 25\}\)</span> is a compound event. With events defined in this way, we can translate the other concepts from the explicit event-based probability. Specifically, we think of the experiment producing a numerical outcome, and then use the same sets of tools as before applied to numeric events.</p>
<div id="exm-rv-recap" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.2 (Expanding on Random Variables at the Coffee Shop)</strong></span> After tiring of their game of “how many random variables can we name that have to do with the statistical experiment of watching for customers at a cash register?”, Charles and Sadie fall into a deeper discussion of some of the some of their favourite random variables named during the play through. They take turns describing the support of a chosen random variable, as well as listing examples of simple and compound events that can be translated into the language of random variables.</p>
<p>Choose a random variable identified in <a href="#exm-random-var" class="quarto-xref">Example&nbsp;<span>5.1</span></a> and indicate its support, as well as possible simple and compound events that could be observed in terms of it.</p>
<div class="solution callout callout-style-simple callout-none no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Solution
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Suppose that we take <span class="math inline">\(X\)</span> to be a random variable representing “The number of people who arrive at the cashier in the next hour.” The support of this random variable is likely best select as the non-negative integers. That is, we take <span class="math display">\[\mathcal{X} = \mathbb{N} = \{0,1,2,\dots\}.\]</span> There may be a reasonable maximal value for the random quantity (such as knowing the number of people who are within an hour radius of the coffee shop, or else knowing the number of people who are presently alive), however, the simpler “all non-negative integers” will likely suffice.</p>
<p>For simple events we may consider <span class="math inline">\(\{X = 0\}\)</span> or <span class="math inline">\(\{X = 10\}\)</span> or <span class="math inline">\(\{X = 31415\}\)</span>. In every case, the simple event takes the form <span class="math inline">\(\{X = x\}\)</span>.</p>
<p>For compound events we could consider <span class="math inline">\(\{X &gt; 0\}\)</span>, representing the event that at least one customer arises, we could consider <span class="math inline">\(\{X \leq 10\}\)</span>, the event where no more than <span class="math inline">\(10\)</span> customers show up, or we could consider something a little more abstract, like <span class="math inline">\(X \in \{0, 3, 5, 9, 19, 25\}\)</span> or <span class="math inline">\(X\)</span> is an even number. In all these cases the key point is that compound events have more than one way of being satisfied (in that they correspond to more than one event).</p>
</div>
</div>
</div>
</div>
<p>When considering random variables there is a key distinction between two types of random variables, discrete and continuous.</p>
<div id="def-discrete-random-variable" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.2 (Discrete Random Variable)</strong></span> A discrete random variable is any random variable whose support is either finite or else countably infinite.</p>
</div>
<div id="def-continuous-random-variable" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.3 (Continuous Random Variable)</strong></span> A continuous random variable is any random variable whose support is uncountably infinite.</p>
</div>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Countable and Uncountable Sets
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>We say that a set is <strong>countable</strong> whenever we can enumerate the elements of the set using the positive integers. If we have a set with a finite number of elements in it, then we say that it is countable because each element in the set can get assigned an integer value (just using <span class="math inline">\(1\)</span> through to the cardinality of the set).</p>
<p>If we take a set like the positive integers, <span class="math inline">\(\{1,2,3,\dots\}\)</span>, then this is an infinitely large set. However, it is still countable because we can assign each element of the set an integer value (just use the same integer it is corresponding to!). What if we have the set of positive, even integers ({2,4,6,8,})? Each of these is just <span class="math inline">\(2\)</span> times one of the counting numbers, in order, and so these too are countable.</p>
<p>What if we took the set of real numbers<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> in the interval <span class="math inline">\([0,1]\)</span>? In this case there is <strong>no way</strong> to map each of these values to the values <span class="math inline">\(\{1,2,3,4,\dots\}\)</span> and as a result the interval <span class="math inline">\([0,1]\)</span> is <strong>not countable</strong>. It is infinitely large, but it remains infinitely large even after we have enumerated an infinite set of the items in it.</p>
<p>A key difference between countable and uncountable sets is that, with a countable set, we can<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> sum over the elements of the set. We cannot sum over the elements of an uncountable set, as there would be no way to actually order and enumerate the items. In these notes, the detailed mathematics of countable versus non-countable sets is tangential to the main point which is in distinguishing between discrete and continuous random variables.</p>
</div>
</div>
</div>
<p>Typically, discrete random variables will take on some collection of the integers, where continuous random variables will be defined on some interval (or set of intervals). That is, we may take discrete random variables to be defined on <span class="math inline">\(\{0,1,2,3,\dots,100\}\)</span> or <span class="math inline">\(\{0,1,2,3,\dots\}\)</span> or <span class="math inline">\(\mathbb{Z}\)</span><a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>. For continuous random variables we may take <span class="math inline">\(X \in [0,1]\)</span> or <span class="math inline">\(X \in (0,\infty)\)</span> or <span class="math inline">\(X\in (-\infty,129]\)</span>.</p>
<div id="exm-discrete-vs-continuous-rv" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.3 (The Countability of Coffee Shop Random Variables)</strong></span> Charles and Sadie have been discussing the features of all of the random variables they identified at the coffee shop for a long time when Sadie points out that they have not been differentiating between discrete and continuous random variables. Charles thinks “how could we have overlooked this?” and seeks to remedy the situation, immediately!</p>
<p>List at least one discrete and one continuous random variable that could arise via the statistical experiment of watching the cashier at a coffee shop over time (as described in <a href="#exm-random-var" class="quarto-xref">Example&nbsp;<span>5.1</span></a>).</p>
<div class="solution callout callout-style-simple callout-none no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Solution
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>For this we categorize the random variables originally defined in the solution to <a href="#exm-random-var" class="quarto-xref">Example&nbsp;<span>5.1</span></a>.</p>
<ul>
<li><p>“The number of people who arrive at the cashier in the next hour.”</p>
<p>“The number of different items that are ordered by all customers over the next hour.”</p>
<p>“The number of words that the cashier says to the next customer, prior to payment.”</p>
<p>“A count of the number of red items of clothing that can be seen being worn by the next <span class="math inline">\(15\)</span> customers.”</p>
<p>These are all <strong>discrete</strong> random variables, as the only values they can take on are the counting integer values.</p></li>
<li><p>“The length of time until the next customer arrives at the cashier.” This is a <strong>continuous</strong> random variable, as it can take on any value over an interval (say, the interval <span class="math inline">\((0,28800)\)</span> if the store is open for <span class="math inline">\(8\)</span> hours and the customer were there the whole time).</p></li>
<li><p>“The amount of money that the next customer spends when paying at the cashier.” This is a <strong>discrete</strong> random variable, as it must take a finite set of values. One way to make this clear is to count the value in cents, and then it will be only integer values.</p></li>
<li><p>“A value of <span class="math inline">\(1\)</span> if the next customer is wearing a hat, and a value of <span class="math inline">\(0\)</span> otherwise.” This is a <strong>discrete</strong> random variable as it can only be from <span class="math inline">\(\{0,1\}\)</span>.</p></li>
</ul>
</div>
</div>
</div>
</div>
</section>
<section id="probability-distributions-and-probability-mass-functions" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="probability-distributions-and-probability-mass-functions"><span class="header-section-number">5.2</span> Probability Distributions and Probability Mass Functions</h2>
<p>We will discuss continuous random variables later. For now, we turn our focus to discrete random variables. One of the major utilities of random variables is that they provide a shorthand for summarizing the results of a statistical experiment. To this end, there are several key concepts relating to random variables which help to expedite the analysis of the corresponding probabilities. Chief among these tools is the concept of a probability distribution.</p>
<div id="def-probability-distribution" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.4 (Probability Distribution)</strong></span> A probability distribution is a mathematical statement describing the probabilistic behaviour of a random variable.</p>
</div>
<p>Distributions capture the underlying random behaviour of the random variables of interest, and in so doing, summarize information regarding the experiment or process that is being considered. When concerned with discrete random variables, we typically summarize probability distributions through the use of a probability mass function.</p>
<div id="def-probability-mass-function" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.5 (Probability Mass Function)</strong></span> A probability mass function is a function, <span class="math inline">\(p(x)\)</span>, which maps possible values for a discrete random variable (the set <span class="math inline">\(\mathcal{X}\)</span>) to the probabilities corresponding to those events.<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a></p>
</div>
<p>If a random variable <span class="math inline">\(X\)</span> has support <span class="math inline">\(\mathcal{X} = \{x_1,x_2,\dots,x_k\}\)</span>, then a probability mass function is a function <span class="math inline">\(p(x)\)</span> such that <span class="math inline">\(p(x_1) = P(X = x_1)\)</span>, <span class="math inline">\(p(x_2) = P(X = x_2)\)</span>, and so on through to <span class="math inline">\(p(x_k) = P(X=x_k)\)</span>. Once a probability mass function is known, all of the probabilistic behaviour of the random variable is fully described.</p>
<div id="exm-probability-mass-function" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.4 (The Coin Game for Three)</strong></span> Sometimes Charlie and Sadie are joined by their friend Garth on their trips to the coffee shop. Also a probability aficionado, Garth contently joins in the game with Charles and Sadie where three coins are flipped, and depending on the results, one friend pays for the group. Garth’s order is typically less than Charles and Sadie and so Sadie proposes the following modified game.</p>
<blockquote class="blockquote">
<p>A fair coin is tossed three times. If all tosses of the coin show the same symbol, Garth pays. Otherwise, if there are more heads than tails, Charles pays. Finally, if there are more tails than heads, Sadie pays.</p>
</blockquote>
<p>Help to simplify this statistical experiment by first defining a random variable, <span class="math inline">\(X\)</span> which can be used to encode this game and then record the probability mass function of <span class="math inline">\(X\)</span>.</p>
<div class="solution callout callout-style-simple callout-none no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Solution
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>One choice of <span class="math inline">\(X\)</span> is to allow <span class="math inline">\(X\)</span> to be the number of heads that show on three tosses of the coin. In this case we have <span class="math inline">\(\mathcal{X} = \{0,1,2,3\}\)</span>. If <span class="math inline">\(x = 0\)</span> or <span class="math inline">\(x = 3\)</span> is observed then Garth pays. If <span class="math inline">\(x = 2\)</span> is observed then Charles pays. Finally, if <span class="math inline">\(x = 1\)</span> is observed, Sadie pays. Thus writing down the probability mass function for <span class="math inline">\(X\)</span> also provides an easy way for computing the probability of each player having to pay.</p>
<p>In order for <span class="math inline">\(X = 0\)</span>, we must have all tails come up. There are <span class="math inline">\(2^3 = 8\)</span> total possible sequences of <span class="math inline">\(3\)</span> coin tosses, and only <span class="math inline">\(1\)</span> of these results in <span class="math inline">\(X = 0\)</span>, therefore <span class="math inline">\(P(X = 0) = p(0) = \dfrac{1}{8}\)</span>. The same logic applies to <span class="math inline">\(X = 3\)</span> where we need to toss all heads. There are several techniques for finding <span class="math inline">\(p(1)\)</span> and <span class="math inline">\(p(2)\)</span>.<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a> The most direct way is to recognize that there are exactly <span class="math inline">\(3\)</span> ways of observing <span class="math inline">\(1\)</span> head (it can be in the first, second, or third toss). Thus, <span class="math inline">\(p(1) = \dfrac{3}{8}\)</span>. For <span class="math inline">\(p(2)\)</span>, the same logic applies except we ask “where is the one tail?” to give <span class="math inline">\(p(2) = \dfrac{3}{8}\)</span>. Then, taken together, this results in <span class="math display">\[p(x) = \begin{cases}
\dfrac{1}{8} &amp; x \in \{0, 3\} \\
\dfrac{3}{8} &amp; x \in \{1, 2\} \\
0 &amp; \text{otherwise}.\end{cases}\]</span> We can also read off from this result that Garth pays <span class="math inline">\(\dfrac{1}{4}\)</span> of the time in the long run, while Sadie and Charles each pay <span class="math inline">\(\dfrac{3}{8}\)</span> of the time.<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a></p>
</div>
</div>
</div>
</div>
<p>The previously outlined conditions for probabilities must still hold when using probability mass functions. As a result, we know that probabilities are all between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>, and so we must have <span class="math inline">\(0 \leq p(x) \leq 1\)</span>, for all <span class="math inline">\(x\in\mathcal{X}\)</span>. Moreover, we saw previously that summing the probabilities over the full sample space gave a value of <span class="math inline">\(1\)</span>. Correspondingly, we must have that <span class="math display">\[\sum_{x\in\mathcal{X}} p(x) = 1.\]</span> These two properties are often used to define a <em>valid</em> probability mass function, and we can use these properties to both check whether a given mass function is valid and to turn a given function into a valid probability mass function.</p>
<div id="exm-finding-a-pmf" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.5 (Charles’s Messy Writing Strikes Again)</strong></span> Charles is reading through some notes regarding a new game under development which, like most good games, relies at least a little bit on chance. In the notes there is a probability mass function written down, which, the best Charles can make out, reads <span class="math display">\[p(x) = \begin{cases} 3c &amp; x = 0 \\ 0.6 &amp; x = 1 \\ 7c &amp; x = 2.\end{cases}\]</span> Frustrated with the illegibility, Charles brings the problem to Sadie who points out, if it really is <span class="math inline">\(p(0) = 3c\)</span> and <span class="math inline">\(p(2) = 7c\)</span>, they actually have all the information required to solve the problem.</p>
<p>What is the probability mass function, assuming Charles is reading it correctly.</p>
<div class="solution callout callout-style-simple callout-none no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Solution
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>We know that two properties must be true of all probability mass functions. First <span class="math inline">\(0 \leq p(x) \leq 1\)</span>. Second, <span class="math inline">\(\sum_{x\in\mathcal{X}} p(x) = 1\)</span>. The first property tells us that <span class="math inline">\(0 &lt; 3c &lt; 1\)</span> and that <span class="math inline">\(0 &lt; 7c &lt; 1\)</span>. This means that <span class="math inline">\(c &gt; 0\)</span>, and that <span class="math inline">\(c &lt; 1/3\)</span> and <span class="math inline">\(c &lt; 1/7\)</span>. Using the second property we get that <span class="math display">\[1 = 3c + 0.6 + 7c \implies 0.4 = 10c.\]</span> This tells us that <span class="math inline">\(c = 0.04\)</span>, which satisfies all of the previous properties. Taking <span class="math inline">\(c = 0.04\)</span> gives a probability mass function of <span class="math display">\[p(x) = \begin{cases} 0.12 &amp; x = 0 \\ 0.6 &amp; x = 1 \\ 0.28 &amp; x = 2.\end{cases}\]</span></p>
</div>
</div>
</div>
</div>
<p>When solving questions related to probabilities using a probability mass function, the same secondary properties apply. Notably, if we want to know <span class="math inline">\(P(X\in A)\)</span>, for some set of possible values <span class="math inline">\(A\)</span>, then we can write <span class="math display">\[P(X\in A) = \sum_{x\in A}p(x).\]</span> This can be particularly helpful, for instance, if we want to know <span class="math inline">\(P(X \leq c)\)</span> for some constant value <span class="math inline">\(c\)</span>. In this case we know that the possible values range from the smallest value <span class="math inline">\(X\)</span> can take on, through to <span class="math inline">\(c\)</span>, giving for instance, <span class="math display">\[P(X\leq c) = \sum_{x=0}^c p(x),\]</span> if <span class="math inline">\(X \geq 0\)</span>. These probabilities that consider the cumulative likelihood of <span class="math inline">\(X\)</span> taking on a value up to (or including) some number arise frequently in probability, and correspondingly, are themselves named.</p>
<div id="def-cumulative-distribution-function" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.6 (Cumulative Distribution Function (Discrete))</strong></span> The cumulative distribution function of a discrete random variable, <span class="math inline">\(X\)</span>, is denoted <span class="math inline">\(F_X(x)\)</span>, and is defined as the probability that <span class="math inline">\(X\)</span> is less than or equal to a particular value. That is, <span class="math display">\[F_X(x) = P(X \leq x) = \sum_{x = -\infty}^x p_X(x).\]</span> The summation is typically written starting at the smallest value of <span class="math inline">\(X\)</span> rather than starting from <span class="math inline">\(-\infty\)</span>.</p>
</div>
<p>The cumulative distribution function will be explored more thoroughly in <a href="chapter9.html" class="quarto-xref"><span>Chapter 9</span></a>. Generally, for discrete random variables, the cumulative distribution function is more complex than the probability mass function, and it often is only expressible directly through a summation. Despite this, you can always convert between the probability mass function and the cumulative distribution function, with each uniquely specifying the distribution of the random variable.</p>
<p>The same rules regarding the complements of events continue to hold when working with probability mass functions and cumulative distribution functions. For instance, <span class="math display">\[P(X &gt; c) = 1 - P(X\leq c) = 1 - F_X(c),\]</span> giving a useful avenue for simplifying probability calculations.</p>
<div id="exm-probability-calc" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.6 (Charles and Sadie: Amateur Ornithologists)</strong></span> Charles and Sadie watched a documentary about birds together, which had Sadie become quite interested in bird watching. Charles is skeptical<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a> but agrees to go along with Sadie, supposing that there is a high enough probability of something interesting happening. Sadie scours the scholarly literature and determines that, in the area, the probability of seeing <span class="math inline">\(x\)</span> rare birds over a five hour birding session has a probability mass function given by <span class="math display">\[p(x) = \dfrac{e^{-3}3^x}{x!},\]</span> where <span class="math inline">\(x\geq 0\)</span>.<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a></p>
<p>What is the probability that during their bird watching adventure, Charles and Sadie see at least <span class="math inline">\(1\)</span> rare bird?</p>
<div class="solution callout callout-style-simple callout-none no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Solution
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>We are interesting in the probability <span class="math inline">\(P(X \geq 1)\)</span>, which we can write down explicitly as <span class="math display">\[P(X \geq 1) = \sum_{x=1}^\infty \dfrac{e^{-3}3^x}{x!}.\]</span> While, strictly speaking, it is possible to solve this infinite summation, it is an infinite summation and we would rather not. Instead we can realize that, if we take the event <span class="math inline">\(A = \{X \geq 1\}\)</span> then <span class="math inline">\(A^C\)</span> is the event <span class="math inline">\(\{X &lt; 1\} = \{X = 0\}\)</span>. As a result, using our elementary rules of probability we get that <span class="math display">\[P(X \geq 1) = 1 - P(X = 0) = 1 - \dfrac{e^{-3}3^{0}}{0!} = 1 - e^{-3} \approx 0.95.\]</span> As a result, there is an approximately <span class="math inline">\(0.95\)</span> probability that they see a rare bird while bird watching for five hours.</p>
</div>
</div>
</div>
</div>
<p>With events defined in terms of random variables, we can talk about events as being independent of each other or mutually exclusive using the familiar definitions. On a related note, we can talk of joint and conditional probabilities, relating to multiple events. With joint probabilities, it is often easiest to combine the event into a single, compound event, and find the marginal probability of that event. For instance, if you have the events <span class="math inline">\(X\)</span> is even and <span class="math inline">\(X \leq 15\)</span>, then the intersection of these events is <span class="math inline">\(X \in \{2,4,6,8,10,12,14\}\)</span> (supposing <span class="math inline">\(X &gt; 0\)</span>).</p>
<div id="exm-condition-joint-probability" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.7 (Charles: The Ornithological Photographer)</strong></span> After a single time bird watching Charles is hooked! The next time that they go out, Charles brings a camera to snag some beautiful memories of the majesty they are witnessing. The only trouble is that Charles is not particularly good at taking photographs of still subjects, let alone of creatures that can fly. Charles is not yet certain, but suspects that every time a photograph is snapped, there is a <span class="math inline">\(0.1\)</span> probability that it turns out good. Because the camera is a film camera, the number of photos taken is a key metric, and Charles works out that, if <span class="math inline">\(X\)</span> is the number of bad photographs taken for every good photograph, the probability mass function for <span class="math inline">\(X\)</span> is given by <span class="math display">\[p(x) = (0.1)\times (0.9)^{x},\]</span> where <span class="math inline">\(x \geq 0\)</span> is an integer.</p>
<ol type="a">
<li>What is the probability that Charles takes exactly three bad photos before the first good one?</li>
<li>If <span class="math inline">\(A\)</span> is the event that Charles takes three bad photos before the first good one, and <span class="math inline">\(B\)</span> is the event that Charles takes more than four photos before the first good one, what is <span class="math inline">\(P(A,B)\)</span>? What does this means about <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>?</li>
<li>Suppose that Charles has taken <span class="math inline">\(2\)</span> photos, both of which are bad. What is the probability that at least two more bad photos are taken before a good one?</li>
<li>What is the probability that at least <span class="math inline">\(2\)</span> bad photos are taken?</li>
<li>Do the results of (c) and (d) suggest an independence?</li>
<li>List two events, not previously described, which are independent of one another.</li>
</ol>
<div class="solution callout callout-style-simple callout-none no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Solution
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ol type="a">
<li><p>Here, we want <span class="math inline">\(p(3) = (0.1)\times(0.9)^3 = 0.0729\)</span>.</p></li>
<li><p>Considering <span class="math inline">\(A \cap B\)</span> we have <span class="math inline">\(\{X = 3\} \cap \{X \geq 4\} = \emptyset\)</span>. As a result, <span class="math inline">\(P(A,B) = 0\)</span> since there is no overlap between <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>. As a result, these events are mutually exclusive.</p></li>
<li><p>Here, we can frame this probability as <span class="math display">\[P(X \geq 4 | X \geq 2)\]</span> This can be directly computed using the definition of conditional probability, <span class="math display">\[\begin{align*}
P(X \geq 4 | X \geq 2) &amp;= \dfrac{P(X \geq 4, X \geq 2)}{P(X \geq 2)} \\
&amp;= \dfrac{P(X \geq 4)}{1 - P(X &lt; 2)} \\
&amp;= \dfrac{1 - P(X &lt; 4)}{1 - (0.1 + 0.1\times0.9)} \\
&amp;= \dfrac{1 - (0.1 + 0.1\times 0.9 + 0.1\times 0.9^2 + 0.1\times0.9^3)}{1 - (0.1 + 0.1\times 0.9)} \\
&amp;= 0.81.
\end{align*}\]</span></p></li>
<li><p>Through direct calculation we get <span class="math display">\[P(X \geq 2) = 1 - P(X &lt; 2) = 1 - (0.1 + 0.1\times 0.9) = 0.81.\]</span></p></li>
<li><p>Combining (c) and (d) we find that the probability that an additional two photos are required, given that two have already been taken, is the same as the probability that two photos are required, without knowing that any have been taken. While this feels like an independence type of property, it is not precisely independence. Note that the event <span class="math inline">\(\{X \geq 4\}\)</span> immediately gives the event <span class="math inline">\(\{X \geq 2\}\)</span> and so they cannot be independent of one another.<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a> For this to be independent we would require <span class="math inline">\(P(A,B) = P(A)P(B)\)</span> which is not true of any of the events discussed.</p></li>
<li><p>One option is to consider the event <span class="math inline">\(X \geq 0\)</span> with any other event. Because <span class="math inline">\(\{X \geq 0\} = \mathcal{X}\)</span>, no information can be gained conditioning on this event.</p></li>
</ol>
</div>
</div>
</div>
</div>
</section>
<section id="multiple-random-variables-and-joint-probability-mass-functions" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="multiple-random-variables-and-joint-probability-mass-functions"><span class="header-section-number">5.3</span> Multiple Random Variables and Joint Probability Mass Functions</h2>
<p>While we can discuss independence, joint probabilities, and conditional probabilities relating to events on the same random variable, it is often of interest to combine multiple random variables. Sometimes these random variables will be multiple versions coming from the same distribution, and other times they will be coming from multiple different distributions. In either event, frequently our main concern is in summarizing the probabilistic behaviour of two or more random quantities.</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
A Discussion of Distributional Language
</div>
</div>
<div class="callout-body-container callout-body">
<p>Note that when we talk of a random variable “following” a particular distribution, we are saying that the probability mass function of the random variable is described by that distribution’s mass function. Thus, if two random variables “share a distribution”, we just mean that their probabilistic behaviour is described by the same underlying mass function.</p>
<p>For instance, if I flip a coin <span class="math inline">\(10\)</span> times, and you flip a different coin <span class="math inline">\(10\)</span> times, and we each count the number of heads that show up, we can say that the random quantity for the number of heads I observe will have the <strong>same distribution</strong> as the random quantity for the number of heads that you observe.</p>
<p>These two quantities are not equal, in general, but they are described by the same random processes.</p>
</div>
</div>
<p>When we describe the distribution of a particular random variable, we are implicitly describing the marginal probabilities associated with that quantity. Just as before, the marginal probabilities describe the behaviour of the random variable alone. What happens when we want to be able to describe multiple components of an experiment, together? For this we require extending the idea of a joint probability beyond the concept of events.</p>
<p>Suppose that we roll two six-sided fair dice. Let <span class="math inline">\(X\)</span> denote the sum of the two dice, and let <span class="math inline">\(Y\)</span> denote the maximum value showing on the two dice. <span class="math inline">\(X\)</span> is a discrete random variable taking on values between <span class="math inline">\(2\)</span> and <span class="math inline">\(12\)</span>, while <span class="math inline">\(Y\)</span> is a discrete random variable taking on values between <span class="math inline">\(1\)</span> and <span class="math inline">\(6\)</span>. The supports for the two random variables are different from one another and so immediately we know that their probabilistic behaviour must be different, despite the fact that both random variables summarize the same statistical experiment.</p>
<p>We can also immediately see that the two random variables, while not equal to each other, are dependent. For instance, if you know that <span class="math inline">\(Y=1\)</span> then you know that <span class="math inline">\(X = 2\)</span>, and if you know that <span class="math inline">\(Y = 3\)</span>, then you know that <span class="math inline">\(X \leq 6\)</span>. To begin to capture the joint behaviour of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> we introduce the joint distribution and joint probability mass function.</p>
<div id="def-joint-distribution" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.7 (Joint Distribution)</strong></span> A joint probability distribution describes the joint probabilistic behaviour of two or more random variables, simultaneously.</p>
</div>
<div id="def-joint-pmf" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.8 (Joint Probability Mass Function)</strong></span> A joint probability mass function describes the behaviour of the joint distribution of multiple random variables. For a set of random variables, <span class="math inline">\(X_1, \dots, X_n\)</span>, the joint probability mass function assigns a probability value for every <em>tuple</em> of values that <span class="math inline">\((X_1,\dots,X_n)\)</span> can take on.</p>
</div>
<p>The joint probability mass function is analogous to the marginal probability mass function, only it considers joint events rather than marginal ones. Suppose that you have two random variables, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. The joint probability mass function assigns a probability value for every pair of values that <span class="math inline">\((X,Y)\)</span> can take on. That is, <span class="math inline">\(p_{X,Y}(x,y) = P(X = x, Y = y)\)</span>. Then, once you know the joint behaviour of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, you can fully summarize the combined behaviour of the underlying experiment.</p>
<div id="exm-joint-pmf" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.8 (Charles and Sadie’s Bird Outings)</strong></span> Charles and Sadie have both gotten deeply into their ornithological adventures. They go on trips together, Sadie is responsible for spotting the rare birds, and then Charles for snapping the photos. Charles has settled on a camera setup that allows for <span class="math inline">\(10\)</span> pictures to be taken before changing the film. The strategy that they follow is to go out and look for a rare bird. When one is spotted, Charles takes <span class="math inline">\(10\)</span> photos, trying to get as many good photos as possible. Then, the film is replaced and they repeat the process.</p>
<p>If <span class="math inline">\(X\)</span> is a random variable representing the number of good photos that are taken, and <span class="math inline">\(Y\)</span> is the number of birds that are seen on the trip, then the joint probability mass function of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is <span class="math display">\[p_{X,Y}(x, y) = \binom{10y}{x}\times(0.25)^{x}\times(0.75)^{10y - x}\times\dfrac{e^{-3}3^y}{y!},\]</span> with <span class="math inline">\(0 \leq x \leq 10y\)</span>, and <span class="math inline">\(y \geq 0\)</span>.</p>
<ol type="a">
<li>What is the probability that one bird is seen and there are no good photos taken?</li>
<li>What is the probability that there is one or fewer good photos taken and two birds seen?</li>
</ol>
<div class="solution callout callout-style-simple callout-none no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-10-contents" aria-controls="callout-10" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Solution
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-10" class="callout-10-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ol type="a">
<li><p>Here we want <span class="math inline">\(Y = 1\)</span> and <span class="math inline">\(X = 0\)</span>. Thus, we compute <span class="math display">\[p_{X,Y}(0, 1) = \binom{10(1)}{0}\times(0.25)^{0}\times(0.75)^{10(1) - 0}\times\dfrac{e^{-3}3^1}{1!} \approx 0.008411.\]</span></p></li>
<li><p>Here we want <span class="math inline">\(\{X \leq 1\}\)</span> and <span class="math inline">\(Y = 2\)</span>. This is the same as <span class="math inline">\(X = 0\)</span> with <span class="math inline">\(Y = 2\)</span> or <span class="math inline">\(X = 1\)</span> with <span class="math inline">\(Y = 2\)</span>. Thus, <span class="math display">\[\begin{align*}
p_{X,Y}(0,2) + p_{X,Y}(1,2) &amp;= \binom{10(2)}{0}\times(0.25)^{0}\times(0.75)^{10(2) - 0}\times\dfrac{e^{-3}3^2}{2!} \\
&amp;\quad + \binom{10(2)}{1}\times(0.25)^{1}\times(0.75)^{10(2) - 1}\times\dfrac{e^{-3}3^2}{2!} \\
&amp;\approx 0.00547.\end{align*}\]</span></p></li>
</ol>
</div>
</div>
</div>
</div>
<section id="joint-probability-distributions-as-contingency-tables" class="level3" data-number="5.3.1">
<h3 data-number="5.3.1" class="anchored" data-anchor-id="joint-probability-distributions-as-contingency-tables"><span class="header-section-number">5.3.1</span> Joint Probability Distributions as Contingency Tables</h3>
<p>In practice, joint probability mass functions can be thought of as analogous to the contingency tables we previously saw. If the first variable represents the first random variable being considered, and the second variable represents the second random variable, then each cell of the contingency table assigns a probability to one of the joint events that could be observed in the experiment. Joint distributions are a useful generalization of contingency tables as they allow us to compactly represent not only two different random variables, but sometimes many more. All of the definitions used for the case of two random variables extend naturally to three, four, and beyond.</p>
<div id="exm-contingency-table-as-joint-pmf" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.9 (Charles and Sadie: Rock-Paper-Scissors Experimentation)</strong></span> Charles and Sadie were once asked if rock-paper-scissors may be an easier way to solve their “who pays for coffee” dilemmas. Though both of them rejected the description of their coffee games as a “dilemma”, they had never really given it much thought. One day while bird watching, during a particularly long break with no birds, they begin to think through whether this could work or not. To this end, the friends play <span class="math inline">\(1000\)</span> games, recording the results of each game into a contingency table. They suspect that this provides the true long run proportion of occurrences. To encode these games numerically, they take <span class="math inline">\(X = -1\)</span>, <span class="math inline">\(X=0\)</span>, and <span class="math inline">\(X=1\)</span> to represent Charles playing rock, paper, and scissors respectively, and they take <span class="math inline">\(Y = -1\)</span>, <span class="math inline">\(Y=0\)</span>, and <span class="math inline">\(Y=1\)</span> to represent the same events from Sadie.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th></th>
<th style="text-align: center;"><span class="math inline">\(Y=-1\)</span></th>
<th style="text-align: center;"><span class="math inline">\(Y = 0\)</span></th>
<th style="text-align: center;"><span class="math inline">\(Y = 1\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(X=-1\)</span></td>
<td style="text-align: center;">400</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">50</td>
</tr>
<tr class="even">
<td><span class="math inline">\(X=0\)</span></td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">40</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(X=1\)</span></td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">100</td>
</tr>
</tbody>
</table>
<p>What is the joint probability mass function that corresponds to this contingency table?</p>
<div class="solution callout callout-style-simple callout-none no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-11-contents" aria-controls="callout-11" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Solution
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-11" class="callout-11-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>We can explicitly enumerate the possibilities to define the joint probability mass function. That is <span class="math display">\[p_{X,Y}(x,y) = \begin{cases}
0.4 &amp; x = -1, y = -1 \\
0.1 &amp; x = -1, y = 0 \\
0.05 &amp; x = -1, y = 1 \\
0.01 &amp; x = 0, y = -1 \\
0.2 &amp; x = 0, y = 0 \\
0.04 &amp; x = 0, y = 1 \\
0.05 &amp; x = 1, y = -1 \\
0.05 &amp; x = 1, y = 0 \\
0.1 &amp; x = 1, y = 1.
\end{cases}\]</span> While it may be possible to express this in a more compact way, this fits the criteria for a joint probability mass function.</p>
</div>
</div>
</div>
</div>
</section>
</section>
<section id="independence-of-random-variables" class="level2" data-number="5.4">
<h2 data-number="5.4" class="anchored" data-anchor-id="independence-of-random-variables"><span class="header-section-number">5.4</span> Independence of Random Variables</h2>
<p>If we continue to consider the case of a bivariate (two variable) joint distribution, we can use this setting to introduce the independence of random variables. Recall that the joint probability mass function of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is a function, <span class="math inline">\(p_{X,Y}(x,y) = P(X = x, Y = y)\)</span>. We have also introduced the marginal mass functions, <span class="math inline">\(p_X(x) = P(X = x)\)</span> and <span class="math inline">\(p_Y(y) = P(Y = y)\)</span>. Further, we have said that two events, <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, are independent if their joint probability is equal to the product of their marginal probabilities, that is <span class="math inline">\(P(A,B) = P(A)P(B)\)</span>.</p>
<p>If we take <span class="math inline">\(A=\{X=x'\}\)</span> and <span class="math inline">\(B=\{Y=y'\}\)</span>, then if <span class="math inline">\(A\perp B\)</span> we can write <span class="math inline">\(p_{X,Y}(x',y')=p_X(x')p_Y(y')\)</span>. If this holds for every possible <span class="math inline">\(x'\)</span> and every possible <span class="math inline">\(y'\)</span>, then we say that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent random variables, and we write <span class="math inline">\(X \perp Y\)</span>.</p>
<div id="def-independent-rv" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.9 (Independent Random Variables)</strong></span> Two random variables, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are said to be independent random variables if their joint probability mass function is given by the product of their marginal probability mass functions, <span class="math display">\[p_{X,Y}(x,y) = p_X(x)p_Y(y).\]</span> We write <span class="math inline">\(X\perp Y\)</span>, and read: <span class="math inline">\(X\)</span> is independent of <span class="math inline">\(Y\)</span>.</p>
</div>
<p>In words, two random variables are independent whenever all possible combinations of events between them are independent.</p>
<div id="exm-pmf-independence" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.10 (Charles the Sports Fan)</strong></span> Charles is a major sports fan and has been particularly fond of <em>hurling</em> ever since a trip to Ireland. Each week, Charles watches with deep investment, becoming very attached to the outcome. Because of this attachment, Charles is willing to do just about anything to help out from behind the television set, which mostly consists of wearing the right coloured clothing. Charles’s theory is that by increasing the number of articles of green clothing that are worn, the number of scores in the game will also increase.</p>
<p>Let <span class="math inline">\(X\)</span> represent the number of articles of green clothing that Charles is wearing, with <span class="math inline">\(X \in \{0,1,2,3,4\}\)</span>, and <span class="math inline">\(Y\)</span> is the number of scores in the game. Suppose that <span class="math display">\[p_X(x) = \dfrac{(x-2)^2}{10},\]</span> and that <span class="math display">\[p_Y(y) = \dfrac{e^{-45}\times 45^y}{y!}.\]</span></p>
<ol type="a">
<li>What is the joint probability mass function for <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, assuming that they are independent?</li>
<li>If it is determined that, assuming Charles wears no green clothing, the probability of <span class="math inline">\(50\)</span> scores is <span class="math inline">\(0.01\)</span>, are these random variables independent?</li>
</ol>
<div class="solution callout callout-style-simple callout-none no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-12-contents" aria-controls="callout-12" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Solution
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-12" class="callout-12-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ol type="a">
<li><p>If <span class="math inline">\(X\perp Y\)</span>, then the joint probability mass function is the product of the two, which is to say <span class="math display">\[p_{X,Y}(x,y) = \dfrac{(x-2)^2}{10}\cdot\dfrac{e^{-45}\times 45^y}{y!} = \dfrac{e^{-45}}{10}\times (x-2)^2 \times \dfrac{45^y}{y!}.\]</span></p></li>
<li><p>Using the joint probability mass function found in (a), <span class="math display">\[p_{X,Y}(0, 15) = \dfrac{e^{-45}}{10}\times (0-2)^2 \times \dfrac{45^{50}}{(50)!} \approx 0.017.\]</span> This is not equal to <span class="math inline">\(0.01\)</span>, and so it must be the case that <span class="math inline">\(X\not\perp Y\)</span>.</p></li>
</ol>
</div>
</div>
</div>
</div>
<div id="exm-independence-arg" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.11 (The Independence Argument)</strong></span> Charles and Sadie are having a disagreement about the nature of the probabilities on their bird outings. Charles claims that, without knowing the marginal probability mass functions of the number of good pictures taken, and the number of rare birds that are seen, there is no way to tell whether the the two random quantities are independent or not. Sadie cannot say exactly why this argument feels wrong, but insists that the two quantities must not be independent.</p>
<p>Who is correct, and why? Recall that <span class="math display">\[p_{X,Y}(x, y) = \binom{10y}{x}\times(0.25)^{x}\times(0.75)^{10y - x}\times\dfrac{e^{-3}3^y}{y!},\]</span> with <span class="math inline">\(0 \leq x \leq 10y\)</span>, and <span class="math inline">\(y \geq 0\)</span></p>
<div class="solution callout callout-style-simple callout-none no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-13-contents" aria-controls="callout-13" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Solution
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-13" class="callout-13-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>In this case Sadie is correct. The most clear way of seeing this is by noting that the support set for <span class="math inline">\(X\)</span> depends on <span class="math inline">\(Y\)</span>. The question to ask yourself is: how could two functions which are independent of one another multiply together so that one’s support depends on the other’s? It cannot happen. Put differently: we know that these cannot be independent as, if <span class="math inline">\(Y=0\)</span> then <span class="math inline">\(P(X=0)=1\)</span> as Charles will not take any pictures.</p>
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-warning no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-14-contents" aria-controls="callout-14" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Equivalent Definition of Independence
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-14" class="callout-14-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>There is an equivalence between the described definition, and a slightly more intuitive definition for independence.</p>
<p>Whenever <span class="math inline">\(X\perp Y\)</span> any two events corresponding to <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, say <span class="math inline">\(X \in A\)</span> and <span class="math inline">\(Y \in B\)</span> are independent. The subtle distinction is that in our previous definition, we were only concerned with simple events of the form <span class="math inline">\(X=x'\)</span> or <span class="math inline">\(Y=y'\)</span>. Here we allow any two arbitrary events.</p>
<p>Note that if we have the above definition holding for simple events, then <span class="math display">\[\begin{align*}
P(X \in A, Y \in B) &amp;= \sum_{x\in A}P(X=x,Y\in B) \\
&amp;= \sum_{x\in A}\sum_{y \in B} P(X=x,Y=y)\\
&amp;= \sum_{x\in A}\sum_{y \in B} p_X(x)p_Y(y) \\
&amp;= \left(\sum_{x\in A}p_X(x)\right)\left(\sum_{y\in B}p_Y(y)\right)\\
&amp;= P(X\in A)P(Y\in B).
\end{align*}\]</span></p>
<p>Even by only making the assumption for simple events, the conclusion regarding compound events follows naturally. Whenever any two random variables are known to be independent we know that any two events corresponding to these random variables will be independent. Moreover, we can directly write down the joint probability mass function by taking the product of the two marginals.</p>
</div>
</div>
</div>
</section>
<section id="conditional-probability-distributions" class="level2" data-number="5.5">
<h2 data-number="5.5" class="anchored" data-anchor-id="conditional-probability-distributions"><span class="header-section-number">5.5</span> Conditional Probability Distributions</h2>
<p>When introducing events, we discussed how the concepts of independence and dependence could be understood more intuitively through the use of conditional probabilities. The same is true for random variables.</p>
<div id="def-conditional-distribution" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.10 (Conditional Distributions)</strong></span> A conditional distribution of a random variable captures the probabilistic behaviour of a random variable, given information regarding another (or several other) random variables.</p>
</div>
<div id="def-conditional-pmf" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.11 (Conditional Probability Mass Function)</strong></span> A conditional probability mass function assigns probability values associated with any conditional event between multiple random variables. For instance, if there are two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, the conditional probability mass function of <span class="math inline">\(X\)</span> given <span class="math inline">\(Y\)</span> characterizes events of the form <span class="math inline">\(X\)</span> given <span class="math inline">\(Y=y\)</span>. Mathematically, the conditional mass function is <span class="math display">\[p_{X|Y}(x|y) = \dfrac{p_{X,Y}(x,y)}{p_Y(y)}.\]</span></p>
</div>
<p>This definition is analogous to the formula for conditional probabilities more generally, give by the joint distribution over the marginal distribution. To determine the probability of any event (for <span class="math inline">\(X\)</span>) given some information about <span class="math inline">\(Y\)</span>, you plug-in <span class="math inline">\(X=x\)</span> and <span class="math inline">\(Y=y\)</span> into the conditional probability mass function. If you want to condition on more than one random variable, the quantities extend in exactly the same way, where for instance <span class="math display">\[P(X=x|Y=y,Z=z)=\dfrac{P(X=x,Y=y,z=z)}{P(Y=y,Z=z)}.\]</span></p>
<div id="exm-conditional-pmf" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.12 (Charles Accepts the Argument)</strong></span> After some convincing, Charles accepts the argument that the number of good pictures taken and the number of rare birds seen cannot be independent events. What really makes this clear, however, is when Sadie points out that the marginal probability mass function for the number of rare birds seen is given by <span class="math display">\[p_{Y} = \dfrac{e^{-3}3^y}{y!}.\]</span> Taken together with the fact that <span class="math display">\[p_{X,Y}(x, y) = \binom{10y}{x}\times(0.25)^{x}\times(0.75)^{10y - x}\times\dfrac{e^{-3}3^y}{y!},\]</span> with <span class="math inline">\(0 \leq x \leq 10y\)</span>, and <span class="math inline">\(y \geq 0\)</span>, Charles realizes that the conditional distribution of <span class="math inline">\(X|Y\)</span> can be computed.</p>
<ol type="a">
<li>What is the conditional probability mass function of <span class="math inline">\(X\)</span> given <span class="math inline">\(Y\)</span>?</li>
<li>Given that <span class="math inline">\(2\)</span> birds are seen, what is the probability that any good photos are taken?</li>
</ol>
<div class="solution callout callout-style-simple callout-none no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-15-contents" aria-controls="callout-15" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Solution
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-15" class="callout-15-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ol type="a">
<li><p>For the conditional probability mass function we take <span class="math inline">\(p_{X,Y}(x, y)/p_{Y}(y)\)</span>$. This gives <span class="math display">\[p_{X|Y}(x|y) = \dfrac{\binom{10y}{x}\times(0.25)^{x}\times(0.75)^{10y - x}\times\dfrac{e^{-3}3^y}{y!}}{\dfrac{e^{-3}3^y}{y!}} = \binom{10y}{x}\times(0.25)^{x}\times(0.75)^{10y - x}.\]</span></p></li>
<li><p>To represent this, we want <span class="math inline">\(P(X \geq 1 | Y = 2)\)</span>. From properties of probability, we know that <span class="math inline">\(P(X \geq 1 | Y = 2) = 1 - P(X &lt; 1 | Y = 2) = P(X = 0 | Y = 2)\)</span>, and so using the conditional probability mass function found previously, this gives <span class="math display">\[P(X = 0 | Y = 2) = \binom{20}{0}(0.25)^{0}\times(0.75)^{20} = 0.75^{20}.\]</span> As a result, the probability of interest is <span class="math inline">\(1-0.75^{20} \approx 0.9968\)</span>, which is the probability that Charles takes at least one good picture if two birds are seen.</p></li>
</ol>
</div>
</div>
</div>
</div>
<p>As was discussed, the joint probability mass function of two independent random variables is given by the product of the marginals. If <span class="math inline">\(X\perp Y\)</span>, then <span class="math inline">\(p_{X,Y}(x,y) = p_{X}(x)p_{Y}(y)\)</span>. Combined with the expression for the conditional probability mass function this results in <span class="math inline">\(p_{X|Y}(x|y) = p_X(x)\)</span>. That is, whenever two variables are independent, the conditional probability mass function is exactly equal to the marginal probability mass function.</p>
<p>We saw that this was true for events, and the same reasoning applies here. This result gives an intuitive method for interpreting the independence of random variables. Two random variables are independent whenever any information about the one does not provide information about the other; when they are completely uninformative for one another. With this intuitive description it is easier to infer when independence of random variables is reasonable. Doing so is a useful skill for manipulating probability expressions.</p>
<div id="exm-independence-via-pmfs" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.13 (Charles Exploration of Independence)</strong></span> A little shaken after the previous independence mishap, Charles is committed to better understanding independence at an intuitive level. As a result, every time a pair of random quantities are seen together Charles has taken to deciding whether or not the underlying random variables would be independent.</p>
<p>For each of the following, indicate whether the pairs of random quantities are likely to be independent (and why).</p>
<ol type="a">
<li>Charles is buying a butternut squash, and is considering the length of the squash (<span class="math inline">\(X\)</span>) and the weight of the squash (<span class="math inline">\(Y\)</span>) as the most relevant measurements.</li>
<li>Charles is sitting at an intersection which has a separated bike path in front of it, and a lane of vehicular traffic. Take <span class="math inline">\(X\)</span> to be the number of bikes passing by over a length of time, and <span class="math inline">\(Y\)</span> to be the number of cars passing by over a certain time.</li>
<li>As the weekend comes around Charles prepares for the hurling match, once again considering whether the number of green items of clothing worn (<span class="math inline">\(X\)</span>) has an impact on the number of scores (<span class="math inline">\(Y\)</span>).</li>
<li>Charles and Sadie are playing battle dice, where Charles rolls a die and compares the result (<span class="math inline">\(X\)</span>) to the result of a separate die rolled by Sadie (<span class="math inline">\(Y\)</span>).</li>
</ol>
<div class="solution callout callout-style-simple callout-none no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-16-contents" aria-controls="callout-16" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Solution
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-16" class="callout-16-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ol type="a">
<li><p>The length of a butternut squash and its weight are likely dependent. The longer a squash is, all else equal, the more likely it will be heavier, and vice versa. Both of these measure the size of the squash, and as a result, will be connected.</p></li>
<li><p>This is an interesting example which likely could be argued either way. On one hand, if cars and bikes do not mix, then the traffic of one will not likely impact the traffic on another. However, there are plenty of reasons why a larger number of bikes may suggest a certain number of cars: (1) perhaps more people bike on the weekends and fewer people drive on the weekends; (2) perhaps a city with more bikers has more cars; (3) perhaps more bikes represents a busy time of the day, which would also lead to more cars. It is challenging to know precisely where the impact stems from, it seems likely that there would be some relationship.</p></li>
<li><p>There is likely no impact of what Charles is doing and what is happening in a sporting match being watched on the TV. As a result, these two quantities should be independent.</p></li>
<li><p>The die roll from Charles has no impact on the die roll from Sadie, and vice versa. As a result, these two random variables will almost certainly be independent.</p></li>
</ol>
</div>
</div>
</div>
</div>
</section>
<section id="manipulating-probabilities-with-random-variables" class="level2" data-number="5.6">
<h2 data-number="5.6" class="anchored" data-anchor-id="manipulating-probabilities-with-random-variables"><span class="header-section-number">5.6</span> Manipulating Probabilities with Random Variables</h2>
<p>Seeing as the joint, marginal, and conditional probability mass functions are exactly analogous to the corresponding concepts when they were introduced regarding events, it is reasonable to assume that we can extend the multiplication rule, the law of total probability, and Bayes’ theorem to the framework of probability functions. Indeed, each of these relationships continues to hold for random variables in much the way that would be expected.</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Multiplication Rule (with Probability Mass Functions)
</div>
</div>
<div class="callout-body-container callout-body">
<p>Translating the multiplication rule to use probability mass functions gives <span class="math display">\[p_{X,Y}(x,y) = p_{X|Y}(x|y)p_Y(y) = p_{Y|X}(y|x)p_X(x).\]</span> This can be seen by rearranging the relationship defining the conditional probabilities.</p>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Bayes’ Theorem (with Probability Mass Functions)
</div>
</div>
<div class="callout-body-container callout-body">
<p>Bayes’ Theorem can be rewritten using probability mass functions as well. Specifically, <span class="math display">\[p_{Y|X}(y|x) = \dfrac{p_{X|Y}(x|y)p_Y(y)}{p_X(x)}.\]</span> This follows directly from the definition of conditional probabilities, as well as the multiplication rule.</p>
</div>
</div>
<p>These rules give the ability to compute the joint distribution and the other conditional information, when we have information regarding some of the marginals and some of the conditionals. These properties are used less <em>explicitly</em> when dealing with probability mass functions directly. Instead, they almost become absorbed into the fabric of the defining relationships themselves. That is to say, you are less likely to see Bayes’ theorem invoked directly when moving between conditional distributions; however, moving between conditional distributions <em>is</em> an important skill, and requires the use of Bayes’ Theorem.</p>
<div id="exm-pmg-multiplication-and-bayes" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.14 (Charles and the Chores)</strong></span> Charles has decided to take a break from having paid employment, instead taking time to help ensure that Sadie’s house runs smoothly as Sadie has been busy trying to write a novel. Unfortunately, sometimes chores are not done in a timely manner, despite Charles’s best efforts. While Sadie has been nothing but supportive, Charles decides to turn to probability to ease uncertainties.</p>
<p>Define <span class="math inline">\(X=1\)</span> if a given chore was done by Charles, with <span class="math inline">\(X=0\)</span> if it was done by Sadie. Let <span class="math inline">\(Y=1\)</span> denote a chore being done on time with <span class="math inline">\(Y = 0\)</span> if it was late. Suppose that <span class="math display">\[\begin{align*}
p_{Y|X}(y|x) &amp;= (0.5 + 0.4x)^{y}\times(0.5 - 0.4x)^{1-y},\\
p_X(x) &amp;= 0.85^{x}\times(0.15)^{1-x},\\
p_Y(y) &amp;= .85\times(0.9)^y(0.1)^{1-y} + 0.075.
\end{align*}\]</span></p>
<ol type="a">
<li>What is the probability that a chore is late and done by Charles?</li>
<li>What is the probability that, given a chore was done by Charles, it was done late? What about with Sadie?</li>
<li>What is the probability that, given a chore was late, it was done by Charles? What about with Sadie?</li>
<li>Explain how the results of (b) and (c) may result in Charles feeling responsible for the late chores. Is that accurate?</li>
</ol>
<div class="solution callout callout-style-simple callout-none no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-19-contents" aria-controls="callout-19" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Solution
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-19" class="callout-19-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ol type="a">
<li><p>For this question, we require the joint probability mass function we apply the product rule. That is, <span class="math display">\[p_{X,Y}(x,y) = p_{Y|X}(y|x)p_{X}(x) = (0.5 + 0.4x)^{y}\times(0.5 - 0.4x)^{1-y}\times0.85^{x}\times(0.15)^{1-x}.\]</span> Then, to find the probability that a chore is late and done by Charles we take <span class="math inline">\(p_{X,Y}(1,0) = 0.1\times 0.85 = 0.085.\)</span></p></li>
<li><p>For Charles, we want <span class="math inline">\(P(Y=0|X=1)\)</span>, which can be solved for directly using the provided conditional probability mass function. That is, <span class="math display">\[p_{Y|X}(0|1) = (0.5 + 0.4)^{0}\times(0.5 - 0.4)^{1} = 0.1.\]</span> For Sadie, we want <span class="math inline">\(P(Y=0|X=0)\)</span>. This gives <span class="math display">\[p_{Y|X}(0|0) = (0.5)^{0}\times(0.5)^{1} = 0.5.\]</span></p></li>
<li><p>Here we require the alternative conditional distribution, <span class="math inline">\(P(X|Y)\)</span>. For this we can leverage Bayes’ Theorem. Specifically, <span class="math display">\[p_{X|Y} = \dfrac{p_{Y|X}(y|x)p_{X}(x)}{p_{Y}(y)} = \dfrac{(0.5 + 0.4x)^{y}\times(0.5 - 0.4x)^{1-y}\times0.85^{x}\times(0.15)^{1-x}}{.85\times(0.9)^y(0.1)^{1-y} + 0.075}.\]</span> Then, the probability that Charles did the chore, given it was late, is <span class="math inline">\(p_{X|Y}(1|0) = 0.53125\)</span> and the probability that Sadie did the chore, given it was late, is <span class="math inline">\(p_{X|Y}(0|0) = 0.46875\)</span>.</p></li>
<li><p>Two things are true with the given probabilities. First, if a chore was done late, it was more likely done by Charles than Sadie. Second, Sadie is more likely to do chores late than Charles. This explains why Charles may feel responsibility for the late chores: a higher proportion of late chores are Charles’s chores. However, that is only because Charles does so many more chores to begin (the probability that Charles does a chore is <span class="math inline">\(0.85\)</span>).<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a></p></li>
</ol>
</div>
</div>
</div>
</div>
<p>Unlike the multiplication rule and Bayes’ theorem, the extension of the law of total probability is frequently cited when manipulating probability mass functions. It is a process which is important enough so as to warrant its own name: marginalization.</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Marginalization
</div>
</div>
<div class="callout-body-container callout-body">
<p>The idea with marginalization is that we are going to take a joint probability mass function and <em>marginalize it</em>, turning it into a marginal distribution. This is analogous to the law of total probability.</p>
<p>When dealing with a random variable, <span class="math inline">\(Y\)</span>, there is some set of numeric values that <span class="math inline">\(Y\)</span> can take on, namely the support of <span class="math inline">\(Y\)</span>, which we denote <span class="math inline">\(\mathcal{Y}\)</span>. A natural partition of the space is to then take each possible value for <span class="math inline">\(Y\)</span> as the event, and enumerate through the elements of <span class="math inline">\(\mathcal{Y}\)</span>.</p>
<p>Using this partition, we can ask about the ways that <span class="math inline">\(X\)</span> can take on any particular value. In order for <span class="math inline">\(X\)</span> to be some specific value, <span class="math inline">\(x'\)</span>, we know that <span class="math inline">\(Y\)</span> must be one of the values, <span class="math inline">\(y'\in\mathcal{Y}\)</span>. Thus, if we add up all the possible combinations, <span class="math inline">\((X=x',Y=y_1)\)</span>, <span class="math inline">\((X=x',Y=y_2)\)</span>, and so forth, we will have covered every possible way of making <span class="math inline">\(X=x'\)</span>. This is the exact same process we used when looking at contingency tables, where we summed a row or column to get the marginal probabilities.</p>
<div id="fig-basic-partition-rv" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-basic-partition-rv-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.1: This graphic shows the partitioning of the sample space using a random variable. It is entirely analogous to <a href="chapter4.html#fig-basic-partition" class="quarto-xref">Figure&nbsp;<span>4.1</span></a>, where in place of many different events, we partition over the set of values for <span class="math inline">\(Y\)</span>. Here, <span class="math inline">\(\mathcal{Y} = \{1,2,3,4,5,6,7\}\)</span>.
</figcaption>
<div aria-describedby="fig-basic-partition-rv-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="..\graphics/ch5-basic-partition.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure&nbsp;5.1: This graphic shows the partitioning of the sample space using a random variable. It is entirely analogous to Figure&nbsp;fig-basic-partition, where in place of many different events, we partition over the set of values for Y. Here, \mathcal{Y} = \{1,2,3,4,5,6,7\}."><img src="..\graphics/ch5-basic-partition.svg" class="img-fluid figure-img"></a>
</div>
</figure>
</div>
<p>Taking this argument and encoding it with mathematical notation we get that <span class="math display">\[P(X=x) = \sum_{y\in\mathcal{Y}} P(X=x, Y=y) \implies p_X(x) = \sum_{y\in\mathcal{Y}} p_{X,Y}(x,y).\]</span> The process of marginalization involves summing over one of the random variables in a joint distribution, leaving behind only the marginal of the other. This is often a very effective way of determining a marginal distribution when information about two random variables is easier to discern than information about only one.</p>
<p>To complete the analogy to the law of total probability, recall that the multiplication rule tells us that <span class="math inline">\(p_{X,Y}(x,y) = p_{X|Y}(x|y)p_Y(y)\)</span>, and so we may also marginalize by taking <span class="math display">\[p_X(x) = \sum_{y\in\mathcal{Y}} p_{X|Y}(x|y)p_Y(y).\]</span> This makes it clear that marginalization is often accomplished via arguments based on conditioning.</p>
</div>
</div>
<p>When confronted with questions from statistics and probability, it will often be the case that the natural answer to the question is “it depends.” For instance, if asked “what is the probability that a student passes their next exam?” the likely response is “it depends.” One very useful technique for solving these questions in a satisfactory way is to continue that line of thought and explicitly specify “on what.” In the previous example, you may say “it depends on how much they study.” The conceit here is that, if you knew how much the student studied, then you would better understand the outcomes of the student’s exam.</p>
<p>In mathematical terms this means you have a firm belief about the conditional distribution between the random quantities of <em>exam performance</em> and <em>study time</em>. The process of marginalization, and the law of total probability before that, provide useful ways of being able to translate these “it depends” statements into concrete beliefs about the marginal probabilities. Recall that marginal distributions are distributions which do not depend on any other quantity, and instead, thy capture the overall behaviour of a random variable. They have, in some sense, averaged out all other factors and give you the beliefs which do not depend on anything else at all. The technique for accomplishing this is marginalization.<a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a></p>
<div id="exm-conditioning" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.15 (Photos and Bird Sightings on their Own)</strong></span> Recall that when Charles and Sadie go on their bird watching adventures, Charles takes photos hoping for as many good ones as possible, and Sadie spots the birds. We take <span class="math inline">\(X\)</span> to be the number of good photos taken on these trips, and <span class="math inline">\(Y\)</span> to be the number of rare birds that are seen. We noted that <span class="math display">\[p_{X,Y}(x, y) = \binom{10y}{x}\times(0.25)^{x}\times(0.75)^{10y - x}\times\dfrac{e^{-3}3^y}{y!},\]</span> with <span class="math inline">\(0 \leq x \leq 10y\)</span>, and <span class="math inline">\(y \geq 0\)</span>.</p>
<ol type="a">
<li>Write down an expression for the marginal probability mass function of <span class="math inline">\(Y\)</span>.</li>
<li>Write down an expression for the marginal probability mass function of <span class="math inline">\(X\)</span>.</li>
<li><strong>Challenge:</strong> solve for the marginal probability mass function of <span class="math inline">\(Y\)</span>.</li>
</ol>
<div class="solution callout callout-style-simple callout-none no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-21-contents" aria-controls="callout-21" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Solution
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-21" class="callout-21-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ol type="a">
<li><p>To find <span class="math inline">\(p_Y(y)\)</span>, we marginalize over <span class="math inline">\(X\)</span>. That is, we sum the joint probability distribution function over all values for <span class="math inline">\(X\)</span>. This gives <span class="math display">\[p_Y(y) = \sum_{x = 0}^{10y} p_{X,Y}(x, y) = \sum_{x = 0}^{10y} \binom{10y}{x}\times(0.25)^{x}\times(0.75)^{10y - x}\times\dfrac{e^{-3}3^y}{y!}.\]</span></p></li>
<li><p>To find <span class="math inline">\(p_X(x)\)</span>, we marginalize over <span class="math inline">\(Y\)</span>. That is, we sum the joint probability distribution function over all values for <span class="math inline">\(Y\)</span>. This gives <span class="math display">\[p_X(x) = \sum_{y = 0}^{\infty} p_{X,Y}(x, y) = \sum_{y = 0}^{\infty} \binom{10y}{x}\times(0.25)^{x}\times(0.75)^{10y - x}\times\dfrac{e^{-3}3^y}{y!}.\]</span></p></li>
<li><p>Note, that you will generally not be expected to manipulate these types of sums, however, it is possible to do. First, for <span class="math inline">\(p_Y(y)\)</span>, <span class="math display">\[\begin{align*}
p_Y(y) &amp;= \sum_{x = 0}^{10y} \binom{10y}{x}\times(0.25)^{x}\times(0.75)^{10y - x}\times\dfrac{e^{-3}3^y}{y!} \\
&amp;= \dfrac{e^{-3}3^y}{y!}\underbrace{\sum_{x = 0}^{10y} \binom{10y}{x}\times(0.25)^{x}\times(0.75)^{10y - x}}_{=1} \\
&amp;= \dfrac{e^{-3}3^y}{y!}.
\end{align*}\]</span></p></li>
</ol>
</div>
</div>
</div>
</div>
</section>
<section id="independent-and-identically-distributed-a-framework-for-interpretation" class="level2" data-number="5.7">
<h2 data-number="5.7" class="anchored" data-anchor-id="independent-and-identically-distributed-a-framework-for-interpretation"><span class="header-section-number">5.7</span> Independent and Identically Distributed: A Framework for Interpretation</h2>
<p>A very common assumption when addressing questions in statistics and in probability is that we have a set of random variables which are independent and identically distributed (often written iid). We now have the tools to understand concretely what this means.</p>
<div id="def-iid" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.12 (Independent and Identically Distributed (iid))</strong></span> A set of random variables, <span class="math inline">\(X_1,X_2,\dots,X_n\)</span> are said to be independent and identically distributed (denoted iid) whenever:</p>
<ol type="i">
<li>every subset of random variables in the collection is independent of every other subset of random variables in the collection; and</li>
<li>the marginal distribution for each of the random variables are exactly the same.</li>
</ol>
</div>
<p>The assumption of iid random quantities will often come up when we are repeating a process many times over, and thinking about what observations will arise from this. Suppose that <span class="math inline">\(X_1\)</span> is a random variable that takes the value <span class="math inline">\(1\)</span> if a flipped coin comes up heads, and <span class="math inline">\(0\)</span> otherwise. If we imagine flipping this coin <span class="math inline">\(100\)</span> times then it is reasonable to assume that each sequential coin flip will be independent of each other coin flip, since the result on one flip of a coin should not influence the result of any other flip of a coin. Moreover, every time the coin is flipped, it is reasonable to assume that the probability it shows up heads remains the same. As a result, these <span class="math inline">\(100\)</span> random quantities, (<span class="math inline">\(X_1, X_2, \dots, X_{100}\)</span>), are said to be iid.</p>
<div id="exm-iid" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.16 (Reframing the Bird Photos)</strong></span> Charles and Sadie had, to this point, been considering their bird photo taking adventures in terms of the joint and conditional distributions. After learning of independent and identically distributed random variables, Charles thinks that there is another way to frame the setup, and describes the following to Sadie:</p>
<blockquote class="blockquote">
<p>Suppose it is known that there will be <span class="math inline">\(y\)</span> birds seen on any given day. Then, the total number of good pictures of birds can be viewed as the sum of the number of good pictures of each bird. Thus, we can take a set of iid random variables, say <span class="math inline">\(X_1,\dots,X_y\)</span> which represent the number of good pictures taken of each bird.</p>
</blockquote>
<ol type="a">
<li>Is this an accurate description? In reality, do we think that this will hold?</li>
<li>Supposing that this description is accurate, and that the probability mass function for each <span class="math inline">\(X_j\)</span> is given by <span class="math display">\[p_X(x) = \binom{10}{x}(0.25)^{x}(0.75)^{10-x},\]</span> what is the joint probability mass function of <span class="math inline">\((X_1,\dots,X_y)\)</span>?</li>
</ol>
<div class="solution callout callout-style-simple callout-none no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-22-contents" aria-controls="callout-22" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Solution
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-22" class="callout-22-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ol type="a">
<li><p>Yes, this is a reasonable explanation. In this case, if Charles is willing to assume that good photos for one bird do not impact the chances of good photos of another bird, then this is a perfectly valid way to frame the setting. Note that in this case, <span class="math inline">\(Y=y\)</span> is still random, so this whole description is conditioning on knowing <span class="math inline">\(Y=y\)</span>. In the real world, it is likely that photos may not actually be independent. If Charles takes some really bad photos of one bird, that may impact the chances of taking good photos of the next. However, it is likely close enough to correct in order to be true.</p></li>
<li><p>Recall that for independent random variables, the joint probability mass function is given by the product of the marginal probability mass functions. To simplify the notation, we will define <span class="math inline">\(T(X) = \sum_{i=1}^y x_i\)</span>. Thus the probability mass function of <span class="math inline">\(X_1,\dots,X_y\)</span> is given by <span class="math display">\[\begin{align*}
p_{X_1,X_2,\dots,X_y}(x_1,\dots,x_y) &amp;= \prod_{i=1}^y \binom{10}{x_i}(0.25)^{x_i}(0.75)^{10-x_i} \\
&amp;= \left(\prod_{i=1}^y\binom{10}{x_i}\right)\times(0.25)^{\sum_{i=1}^y x_i}(0.75)^{10y - \sum_{i=1}^yx_i} \\
&amp;= \left(\prod_{i=1}^y \dfrac{10!}{x_i!(10-x_i)!}\right)\times 0.25^{T(X)}\times 0.75^{10y - T(X)} \\
&amp;= \left(\dfrac{(10!)^y}{\prod_{i=1}^y x_i!(10 - x_i)!}\right)\times 0.25^{T(X)}\times 0.75^{10y - T(X)}.
\end{align*}\]</span></p></li>
</ol>
</div>
</div>
</div>
</div>
<p>While we will use the assumption of iid random variables explicitly at a later point, the iid assumption also provides an intuitive method for interpreting probability functions and distributions.</p>
<p>Suppose that we were to take a probability mass function, <span class="math inline">\(p_X(x)\)</span>. If we were able to generate independent and identically distributed realizations from this probability mass function, then the function <span class="math inline">\(p_X(x)\)</span> describes the behaviour for these repeated realizations. Specifically, <span class="math inline">\(p_X(x)\)</span> will give the long-run proportion of realizations of the iid random variables which take the value <span class="math inline">\(x\)</span>.</p>
<p>This type of statement is always the flavour of interpretation statements that are made with respect to probability and statistics. It is always be the case that, in order to understand what is meant by a statement of probability, we consider the repetition of some statistical experiment over and over again. When we were discussing sample spaces and experiments directly, we talked about repeating the experiment over and over again. When we begin to work with random variables instead, it becomes more natural to think about the replication procedures coming through the use of independent and identically distributed random variables.</p>
<p>As your study of probability progresses, you will begin to work with random quantities in a strictly theoretical sense. In introductory level problems, we are often holding in mind very concrete examples to illustrate the procedures and concepts. In this setting it is easy enough to hold in mind the experiment of interest: for instance, we may have a random variable representing the result of a coin toss, and you can envision repeatedly tossing a coin. As the concepts become less concrete, more abstract, and harder to draw direct parallels to tangible scenarios, it becomes more and more important to rely on the interpretations rooted in a series of independent and identically distributed random variables.</p>
<p>A large component of statistics as an area of study is making explicit the assumptions we are working with, and doing our best to ensure that these are reasonable. By interpreting probability mass functions as the proportion of independent and identically distributed random variables that take on a particular value when we repeatedly take realizations of these random variables, this philosophy is made clear and explicit.</p>
</section>
<section id="exercises" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="exercises">Exercises</h2>
<div id="exr-5.1" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 5.1</strong></span> Suppose that you sit in the library, observing the front desk until a patron takes out books. Describe at least <span class="math inline">\(5\)</span> different random variables that could correspond to this experiment.</p>
</div>
<div id="exr-5.2" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 5.2</strong></span> Consider the data collection that goes on at a weather station. Describe as many different random variables as you can think of corresponding to this experiment.</p>
</div>
<div id="exr-5.3" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 5.3</strong></span> For each of the following random variables, identify whether they are discrete or continuous.</p>
<ol type="a">
<li>In a survey, the number of siblings each participant has is recorded.</li>
<li>A thermometer measures temperature in degrees Celsius.</li>
<li>The number of cars passing through a toll booth in an hour.</li>
<li>In a casino game, the amount of money won or lost on a single bet.</li>
<li>The weight of apples harvested from an orchard.</li>
<li>In a classroom, the number of students who own a laptop.</li>
<li>The time it takes for a light bulb to burn out.</li>
<li>When flipping a coin, the number of heads obtained in <span class="math inline">\(10\)</span> flips.</li>
<li>The height of students in a class.</li>
<li>The number of emails received per day.</li>
<li>In a factory, the number of defective products in a batch.</li>
<li>The distance traveled by a car in a specific time interval.</li>
<li>When selecting a card from a standard deck, the card’s face value.</li>
<li>The age at which individuals first learn to ride a bicycle.</li>
<li>The number of customers entering a store in one hour.</li>
<li>In a soccer match, the time it takes for the first goal to be scored.</li>
<li>The number of books a person reads in a month.</li>
<li>When rolling two dice, the sum of the numbers rolled.</li>
<li>The number of text messages sent in a day.</li>
<li>The volume of water in a reservoir.</li>
</ol>
</div>
<div id="exr-5.4" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 5.4</strong></span> Suppose that a probability mass function is given by <span class="math display">\[p(x) = \begin{cases}
    k(7x + 3) &amp; x = 0, 1, 2, 3\\
    0 &amp; \text{otherwise}
\end{cases}.\]</span> Find the value of <span class="math inline">\(k\)</span>.</p>
</div>
<div id="exr-5.5" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 5.5</strong></span> Consider a probability mass function given by <span class="math display">\[p(x) = \begin{cases}
    z &amp; x = 1 \\
    \dfrac{1}{x^2} &amp; x = 2, 3, 4, 5, 6, 7, 8, 9, 10 \\
    0 &amp; \text{otherwise}
\end{cases}.\]</span></p>
<ol type="a">
<li>Find <span class="math inline">\(z\)</span>.</li>
<li>What is the probability that <span class="math inline">\(X\)</span> is an even number?</li>
</ol>
</div>
<div id="exr-5.6" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 5.6</strong></span> Consider the probability mass function, defined for all non-negative integers, given by <span class="math display">\[p(x) = \dfrac{2^xe^{-2}}{x!}.\]</span></p>
<ol type="a">
<li>What is <span class="math inline">\(P(X = 0)\)</span>?</li>
<li>What is <span class="math inline">\(P(X = 5)\)</span>?</li>
<li>What is <span class="math inline">\(P(X \geq 2)\)</span>?</li>
</ol>
</div>
<div id="exr-5.7" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 5.7</strong></span> For each of the following, indicate and explain whether the following properties could belong to a valid probability mass function.</p>
<ol type="a">
<li><span class="math inline">\(p(x) &lt; 0\)</span> for some <span class="math inline">\(x \in \mathbb{Z}\)</span>.</li>
<li><span class="math inline">\(p(x) &gt; 1\)</span> for some <span class="math inline">\(x \in\mathbb{Z}\)</span>.</li>
<li><span class="math inline">\(p(x) &gt; \dfrac{\pi}{\ell}\)</span> for all elements of an <span class="math inline">\(\ell\)</span> element set.</li>
<li><span class="math inline">\(p(-|x|) &gt; 0\)</span> for all <span class="math inline">\(x\in\mathbb{Z}\)</span>.</li>
</ol>
</div>
<div id="exr-5.8" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 5.8</strong></span> Suppose that, for some fixed integer, <span class="math inline">\(y\)</span>, you define the mathematical function <span class="math inline">\(p(x) = \delta_y = I(x = y)\)</span>. That is, it takes a value of <span class="math inline">\(1\)</span> if <span class="math inline">\(x = y\)</span> and <span class="math inline">\(0\)</span> otherwise. Is this a valid probability mass function? Why?</p>
</div>
<div id="exr-5.9" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 5.9</strong></span> Consider the joint probability mass function of two random variables, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, given by <span class="math display">\[P(X=x, Y=y) = \dfrac{1}{150}(x + y), 1\leq x, y\leq 5.\]</span></p>
<ol type="a">
<li>Show that this is a valid joint probability mass function.</li>
<li>What is <span class="math inline">\(P(X = 2, Y = 3)\)</span>?</li>
<li>What is the <span class="math inline">\(P(X = 4)\)</span>?</li>
<li>What is the marginal probability mass function of <span class="math inline">\(Y\)</span>?</li>
<li>Are <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> independent? Explain.</li>
<li>What is the conditional probability mass function of <span class="math inline">\(X\)</span> given <span class="math inline">\(Y\)</span>?</li>
<li>What is the conditional probability mass function of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>?</li>
</ol>
</div>
<div id="exr-5.10" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 5.10</strong></span> Consider the joint probability mass function of two random variables, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, given by <span class="math display">\[P(X=x, Y=y) = \dfrac{1}{12}(y - x)^2, 1\leq x, y\leq 3.\]</span></p>
<ol type="a">
<li>Show that this is a valid joint probability mass function.</li>
<li>What is <span class="math inline">\(P(X = 1, Y = 2)\)</span>?</li>
<li>What is the <span class="math inline">\(P(X = 2)\)</span>?</li>
<li>What is the marginal probability mass function of <span class="math inline">\(Y\)</span>?</li>
<li>Are <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> independent? Explain.</li>
<li>What is the conditional probability mass function of <span class="math inline">\(X\)</span> given <span class="math inline">\(Y\)</span>?</li>
<li>What is the conditional probability mass function of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>?</li>
</ol>
</div>
<div id="exr-5.11" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 5.11</strong></span> Consider the following joint probability mass function represented as a contingency table:</p>
<p><span class="math display">\[
\begin{array}{c|ccc}
    &amp; Y = 1 &amp; Y = 2 &amp; Y = 3 \\
\hline
X = 1 &amp; 0.1 &amp; 0.2 &amp; 0.3 \\
X = 2 &amp; 0.2 &amp; 0.1 &amp; 0.1 \\
\end{array}
\]</span></p>
<ol type="a">
<li>What is the probability that <span class="math inline">\(X = 1\)</span> and <span class="math inline">\(Y = 2\)</span>?</li>
<li>Calculate <span class="math inline">\(P(X = 2)\)</span>.</li>
<li>Find the marginal probability mass function of <span class="math inline">\(Y\)</span>.</li>
<li>Are <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> independent? Justify your answer.</li>
<li>What is <span class="math inline">\(P(Y=1|X=2)\)</span>?</li>
<li>What is <span class="math inline">\(P(X = 2| Y = 2)\)</span>?</li>
</ol>
</div>
<div id="exr-5.12" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 5.12</strong></span> Suppose that a particular disease is associated with two common types of genetic mutations, say type <span class="math inline">\(A\)</span> and type <span class="math inline">\(B\)</span>. Let <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> correspond to the random variables which count the locations at which each type of mutation has occurred. In order for a type <span class="math inline">\(B\)</span> mutation to occur, a type <span class="math inline">\(A\)</span> must have also occurred at the same location, and so we can say that <span class="math display">\[P(B=b|A=a) = \binom{a}{b}(0.25)^b(0.75)^{a-b}, \quad b\in\{0,1,2,\dots, a\}.\]</span> Moreover, suppose that <span class="math display">\[P(A=a) = \dfrac{10-a}{45} \quad a\in\{0,1,2,3,4,5\}.\]</span></p>
<ol type="a">
<li>What is the joint probability mass function of <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>?</li>
<li>What is the marginal probability mass function of <span class="math inline">\(B\)</span>?</li>
<li>What is the conditional probability mass function of <span class="math inline">\(A\)</span> given <span class="math inline">\(B\)</span>?</li>
<li>Is <span class="math inline">\(B\)</span> independent of <span class="math inline">\(A\)</span>? Explain.</li>
<li>Is <span class="math inline">\(A\)</span> independent of <span class="math inline">\(B\)</span>? Explain.</li>
<li>Suppose that you are at a substantially increased risk of the disease if the total, <span class="math inline">\(A+B \geq 8\)</span>. What is the probability that an individual is at an increased risk?</li>
</ol>
</div>
<div id="exr-5.13" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 5.13</strong></span> Suppose a factory produces two types of products: Widgets and Gadgets. Let <span class="math inline">\(W\)</span> and <span class="math inline">\(G\)</span> represent the random variables denoting the number of units produced for each type, in a particular hour. Suppose that the following is observed as the joint probability mass function <span class="math display">\[P(W=w, G=g) = \dfrac{1}{80}, \quad w \in \{1,2,3,\dots,8\}, g \in \{1,2,3,\dots,10\}.\]</span></p>
<ol type="a">
<li>What is the marginal probability mass function, <span class="math inline">\(P(W)\)</span>?</li>
<li>What is the marginal probability mass function, <span class="math inline">\(P(G)\)</span>?</li>
<li>Is <span class="math inline">\(W\perp G\)</span>?</li>
<li>Let <span class="math inline">\(T = W + G\)</span>. What is the probability mass function of <span class="math inline">\(T\)</span>?</li>
<li>What is the conditional probability mass function, <span class="math inline">\(P(W|T)\)</span>?</li>
<li>What is the conditional probability mass function, <span class="math inline">\(P(G|T)\)</span>?</li>
<li>What is the conditional probability mass function, <span class="math inline">\(P(T|G)\)</span>?</li>
<li>What is the conditional probability mass function, <span class="math inline">\(P(T|W)\)</span>?</li>
</ol>
</div>
<div id="exr-5.14" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 5.14</strong></span> &nbsp;</p>
<ol type="a">
<li>List an example of a real-world scenario where a set of random variables is likely to be iid. Explain.</li>
<li>List an example of a real-world scenario where a set of random variables is likely to be identically distributed, but not independent. Explain.</li>
<li>List an example of a real-world scenario where a set of random variables is likely to be independent, but not identically distributed. Explain.</li>
</ol>
</div>
<div id="exr-5.15" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 5.15</strong></span> Suppose that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are iid, with <span class="math inline">\(p_X(x)\)</span> and <span class="math inline">\(p_Y(y)\)</span>. If <span class="math inline">\(Y\)</span> takes on a specific value, <span class="math inline">\(y'\)</span>, with <span class="math inline">\(p_Y(y') = 0.25\)</span>, what is the probability that <span class="math inline">\(X\)</span> takes on the same value? Explain.</p>
</div>
<div id="exr-5.16" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 5.16</strong></span> Suppose that <span class="math inline">\(X_1,\dots,X_n\)</span> are iid with a probability mass function, <span class="math inline">\(p_X(x)\)</span>.</p>
<ol type="a">
<li>What is the joint probability mass function of <span class="math inline">\((X_1,\dots,X_n)\)</span>?</li>
<li>What is the conditional probability mass function, <span class="math inline">\(P(X_1 = x | X_2 = y)\)</span>?</li>
<li>What is the probability mass function of <span class="math inline">\(X_1\)</span> conditional on <span class="math inline">\(\sum_{i=2}^n X_i\)</span>?</li>
</ol>
</div>


</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>Recall our discussions of combinatorics.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>We can actually discuss probabilities revolving around <span class="math inline">\(x\)</span>, but they are all deeply uninteresting. Every probability will either be <span class="math inline">\(1\)</span>, or <span class="math inline">\(0\)</span>. If I tell you that <span class="math inline">\(x = 5\)</span>, then <span class="math inline">\(P(x=5) = 1\)</span> and <span class="math inline">\(P(x = 2) = 0\)</span>. While this is not a particularly <em>interesting</em> probability statement, it is true, and somewhat surprisingly, can arise in meaningful ways.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>The support of a random variable is directly analogous to the sample space from the statistical experiment. That is, it is the set of possible observations which can be made of the random variable. As a result, it is <em>sometimes</em> permissible to call the support the “sample space of the random variable”, in an informal manner. The informality here is important. Formally, a random variable <span class="math inline">\(X\)</span> is a function which maps from the sample space to its support, which is to say <span class="math inline">\(X \colon \mathcal{S}\to\mathcal{X}\)</span>.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>That is to say, events in the case of random variables are subsets of <span class="math inline">\(\mathcal{X}\)</span>.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>Recall that “real numbers” are just all of the standard numbers that we think of (decimals, whole numbers, fractions, negative values, etc.).<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>(in principle)<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>This notation refers to the set of all integers<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>That is, <span class="math inline">\(p(x)\)</span> is a function, <span class="math inline">\(p\colon\mathcal{X}\to[0,1]\)</span>.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>One particularly elegant technique is to realize that <span class="math inline">\(p(1)\)</span> and <span class="math inline">\(p(2)\)</span> must be the same by interchanging the roles of heads and tails. Thus, we have <span class="math inline">\(p(1) + p(2) + 0.25 = 1\)</span> by the unitary property of probability, and rearranging gives <span class="math inline">\(p(1) = p(2) = 0.375\)</span>.<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>We can actually write down this probability mass function more succinctly as <span class="math inline">\(p(x) = \binom{3}{x}\dfrac{1}{8}\)</span>. You can check this holds for yourself, and we will understand why later on!<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p>Charles is still not entirely sure if birds are even real, let alone worth watching.<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12"><p>How Sadie found such a concrete answer, I will never know.<a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13"><p>Instead of independence, we may describe this distribution as being “memoryless”. If you know that you have been waiting a certain amount of time for something to happen (a good picture to get taken) that does not change your belief about how much longer you have to wait; the process “forgets” that it has been ongoing for sometime.<a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn14"><p>This is another example of <em>confusion of the inverse</em>.<a href="#fnref14" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn15"><p>We shall see other forms of these “conditioning arguments” in the next chapter, where we try to summarize the behaviour of a random variable.<a href="#fnref15" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      const annoteTargets = window.document.querySelectorAll('.code-annotation-anchor');
      for (let i=0; i<annoteTargets.length; i++) {
        const annoteTarget = annoteTargets[i];
        const targetCell = annoteTarget.getAttribute("data-target-cell");
        const targetAnnotation = annoteTarget.getAttribute("data-target-annotation");
        const contentFn = () => {
          const content = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          if (content) {
            const tipContent = content.cloneNode(true);
            tipContent.classList.add("code-annotation-tip-content");
            return tipContent.outerHTML;
          }
        }
        const config = {
          allowHTML: true,
          content: contentFn,
          onShow: (instance) => {
            selectCodeLines(instance.reference);
            instance.reference.classList.add('code-annotation-active');
            window.tippy.hideAll();
          },
          onHide: (instance) => {
            unselectCodeLines();
            instance.reference.classList.remove('code-annotation-active');
          },
          maxWidth: 300,
          delay: [50, 0],
          duration: [200, 0],
          offset: [5, 10],
          arrow: true,
          trigger: 'click',
          appendTo: function(el) {
            return el.parentElement.parentElement.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'quarto',
          placement: 'right',
          positionFixed: true,
          popperOptions: {
            modifiers: [
            {
              name: 'flip',
              options: {
                flipVariations: false, // true by default
                allowedAutoPlacements: ['right'],
                fallbackPlacements: ['right', 'top', 'top-start', 'top-end', 'bottom', 'bottom-start', 'bottom-end', 'left'],
              },
            },
            {
              name: 'preventOverflow',
              options: {
                mainAxis: false,
                altAxis: false
              }
            }
            ]        
          }      
        };
        window.tippy(annoteTarget, config); 
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../notes/chapter4.html" class="pagination-link" aria-label="Probabilities with More than One Event">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Probabilities with More than One Event</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../notes/chapter6.html" class="pagination-link" aria-label="The Expected Value, Location Summaries, and Measures of Variability">
        <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">The Expected Value, Location Summaries, and Measures of Variability</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<script type="application/javascript" src="../webex.js"></script>
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>