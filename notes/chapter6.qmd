# The Expected Value, Location Summaries, and Measures of Variability
## Summarizing the Location of a Distribution
Until this point in our discussions of probability we have relied upon characterizing the behaviour of a random variable via the use of probability mass functions. In some sense, a probability mass function captures all of the probabilistic behaviour of a discrete random variable. Using the mass function you are able to characterize how often, in the long run, any particular value will be observed, and answer any questions associated with this. As a result, the mass function remains a critical area of focus for understanding how random quantities behave.

However, these functions need to be explored and manipulated in order for useful information to be extracted from them. They do not summarize this behaviour effectively, as they are not intended to be a summary tool. We may wish to have numeric quantities which are able to *concisely* express the behaviour of a distribution. Put differently, provided with a probability mass function it is hard to immediately answer "what do we expect to happen with this random variable?" despite the fact that this is a very obvious first question.

To address questions related to our expectations, we turn towards the statistical concept of central tendency or location.

:::{#def-location}
## Location (Central Tendency)
The location of a distribution is a measure of a typical value, or a central value, for that distribution. A measure of location may also be referred to as a measure of central tendency. 
:::

With measures of location we are trying to capture, with one single number, what value is expected when we make observations from the random quantity. There are many ways one might think to describe our expectations, and it is worth exploring these concepts in some detail. In particular, we want to explore how we might think to define the **expected value** of a distribution.^[As a general rule when learning it is often helpful to consider exercises of discovery. That is, try to determine *why* particular definitions are the way that they are, or where they have from.]

## Deriving the Expected Value
### The Mode
One way that we may think to define our expected value is by asking what value is the most probable. This is a question which can be directly answered using the probability mass function. The process for this requires looking at the function and determining which value for $x$ corresponds to the highest probability. This is the value that we are most likely to see. Sometimes this procedure is fairly straightforward, sometimes it is quite complicated. Regardless of the complexity of the specific scenario, the most probable value has a straightforward interpretation. As intuitive as this may seem, this is not the value that will be used as *the* expected value generally. Instead, this quantity is referred to as the mode. 

:::{#def-mode}
## Mode
The mode of a distribution is the most probable value of that distribution. Specifically, if $X$ is a discrete random variable with mass function $p_X(x)$, then the mode is the value of $x$ such that $p_X(x)$ is maximized. 
:::

:::{#exm-mode}
## Charles and Sadie Investigate Cupcake Sprinkles
Charles and Sadie have noticed that their favourite coffee shop has been experiencing less foot traffic ever since *Probability Patisserie: Cupcake Conundrums* has opened up next door.^[You should picture the owner of this fictitious cupcake store as a giant, multinational, faceless corporation. A corporation that seems to take pleasure in stepping on the local businesses. That way, as Charles and Sadie try to fight back, we can remain on their side!] This cupcake shop has been attracting many of the regular customers, though, Charles and Sadie think that it has more to do with fancy marketing than with a better product. To this end, they go into the store and workout the probability mass function for $X$, the number of sprinkles on top of the cupcakes. They find the following $$p_X(x) = \begin{cases} 0.25 & x = 0 \\ 0.3 & x = 1 \\ 0.1 & x = 2 \\ 0.05 & x \in \{3,4,5,6,7,8,9\} \\ 0 & \text{otherwise}.\end{cases}$$

What is the mode for the number of sprinkles on the cupcakes?

::::{.callout .solution collapse='true'}
## Solution
The mode is the value $x$ such that $p_X(x)$ is a maximum. In this case the maximum occurs when $x=1$, with $p_X(1) = 0.3$, and so the mode of the number of sprinkles is $1$.
::::
:::

While the mode is a useful quantity, and for some decisions will be the most relevant summary value, there are some major issues with it as a general measure of location. For starters, consider that our most common probability model considered until this point has been that of equally likely outcomes. Here, there is no well-defined mode.^[When multiple modes exist, convention tends to report the set of all of the modes. In the equally likely outcome model, this is the entire support for the random variable, and as a result, the mode is exactly the probability mass function. There is no summary provided.] While the case of equally likely outcomes is a fairly strong example highlighting the issues with the mode, it need not be so dramatic to undermine its utility. It is possible for a distribution to have several modes which are quite distinct from one another, even if it's not all values in the support. 

Moreover, it is quite common for the modal value to be not particularly likely itself. Consider a random variable that can take on a million different values. If all of the probabilities are approximately $0.000001$ then presenting the mode as the most probable value does not translate to saying that the mode is particularly probable. 

:::{#exm-triangular-distributions}
## Charles and Sadie Investigate Cupcake Happiness
After realizing that the cupcake shop did not provide very many sprinkles, Charles and Sadie decided to turn to the marketing material. In it, *Cupcake Conundrums* claims that, on a scale from $0$ to $100,000$ happiness points, most of their customers experience the maximum happiness after eating their cupcakes. Charles and Sadie are well aware of the ways in which these types of reports can be misleading, and so they investigate, finding the following probability mass function for the number of happiness points. 
$$p_X(x) = \begin{cases}
    \frac{x}{5,000,050,000} & x \in \{1,\dots,100 000\} \\
    0 & \text{otherwise}.
\end{cases}$$

a. Is the mode reported by the company correct?
b. Is the mode an accurate depiction of the distribution in this setting?

::::{.callout .solution collapse='true'}
## Solution

a. Yes. Note that $p_X(x)$ is increasing in $x$. As a result, the mode of the distribution is the highest value that the distribution can take on, which in this case is $x = 100,000$. 

b. The mode is likely not a good indication of the distribution. Note that the maximal value happens only $100000/5000050000$ of the time, which is a very very small value. If you look at the probability that a customer is only halfway happy, this is $50000/5000050000$. This value is half the likelihood of the modal value, but the difference between these two probabilities is also only $50000/5000050000 \approx 0.000001$. That is, it is only $1$ in $100,000$ more likely to observe perfect happiness compared with half happiness. By extension, it is only about $2$ in $100,000$ more likely to observe perfect happiness compared with no happiness. The mode simply is not a good indicator of behaviour in this setting.
::::
:::


### The Median
If the mode has these shortcomings, what else might work? Another intuitive concept is to try to select the "middle" of the distribution. One way to define the middle would be to select the value such that, in the long run, half of observations from the distribution are beneath it and half are above it. That way, when you are told this value, you immediately know that it is equally likely to observe values on either side of this mark. This is also a particularly intuitive definition for expected value, and is important enough to be named, the median.

:::{#def-median}
## Median
The median of a distribution is the value $m$ which attempts to have $P(X \leq m) = 0.5$ and $P(X \geq m) = 0.5$. This value may not always exist, and so formally for discrete random variables, the median $m$ is any value such that $P(X \leq m) \geq 0.5$ and $P(X \geq m) \geq 0.5$.
:::

The median is the midpoint of a distribution and is very important for describing the behaviour of random variables. Medians are often the most helpful single value to report to indicate the typical behaviour of a distribution, and they are frequently used. When people interpret averages it is often the median that they are actually interpreting. It is very intuitive to be given a value and know that half of all realizations are above that point, and half of all realizations are below that point.

:::{#exm-median}
## Charles and Sadie Investigate Cupcake Sprinkles (Again)
After seeing how the cupcake shop used the mode to misrepresent the happiness of its customers, Charles and Sadie worry that they may have been unfair with using the mode. As a result, they turn back to the cupcake sprinkle distribution and try to summarize it differently. Recall that the probability mass function for $X$, the number of sprinkles on top of the cupcakes, is $$p_X(x) = \begin{cases} 0.25 & x = 0 \\ 0.3 & x = 1 \\ 0.1 & x = 2 \\ 0.05 & x \in \{3,4,5,6,7,8,9\} \\ 0 & \text{otherwise}.\end{cases}$$

What is the median number of sprinkles on the cupcakes?

::::{.callout .solution collapse='true'}
## Solution
Note that we are looking for a value, $m$, such that $P(X \geq m) \geq 0.5$ and $P(X \leq m) \geq 0.5$. To find this we can consider the cumulative probability sums. We know that $P(X \leq k) = \sum_{x=0}^k p_X(x)$, and as a result, stopping this value at the first time that the sum goes beyond $0.5$ satisfies the second condition. In this case our cumulative sums are $0.25$ then $0.55$. As a result, $P(X \leq 1) = 0.55 > 0.5$. If we check $P(X \geq 1) = 1 - P(X < 1) = 1 - P(X = 0) = 0.75 > 0.5$. As a result, $m=1$ satisfies the required conditions and is a median. 
::::
:::

Despite the advantages of medians, they have their own drawbacks. For starters, the median can be exceptionally challenging to compute in certain settings. As a result, even when a median is appropriate, it may not be desirable if it is too challenging to determine.

Beyond the difficulties in computation, medians have a feature which is simultaneously a major benefit and a major drawback. Specifically, medians are less influenced by extreme values in the probability distribution. Consider two different distributions. The first is equally likely to take any value between $1$ and $10$. The second is equally likely to take any value between $1$ and $9$ or $1,000,000$. In both of these settings, the median is $5$ since $P(X\leq 5) = 0.5$ and $P(X \geq 5) = 0.5$. However, in the second setting we may observe a value as high as $1,000,000$. Moreover, this value will be observed as often as the median will be. 

The median, in some sense, ignores the extreme value in the probability distribution. In certain settings, this can be very desirable.^[You will find many introductory sources that say that you should always use the median when you are concerned with extreme values. This is a great oversimplification of the truth, and points to a general rule in Statistics: there are very few general rules. The guidance comes from the fact that often extreme values skew our perceptions of the underlying truth. While this may be true in general, it is not true often enough to warrant being given as universal guidance.] Consider the distribution of household incomes. There are a few households that earn an incredibly large amount, compared to the remaining households. If you are interested in understanding the "average household", the median may be a more appropriate measure, as those households with extreme incomes would otherwise distort the picture provided by most families. In this sense, the median's robustness to extreme values is a positive feature of it in terms of a summary measure for distributional behaviour.

Suppose instead that you work for an insurance company and are concerned with understanding the value of insurance claims that your company will need to pay out. The distribution will look quite similar to the income distribution. Most of the probability will be assigned to fairly small claims, with a small chance of a very large one. As an insurance company, if you use the median this large claim behaviour will be smoothed over, perhaps leaving you unprepared for the possibility of extremely large payouts. In this setting, the extreme values are informative and important, and as a result the median's robustness becomes a hindrance to correctly describing the important behaviour.^[An insurance company ignoring these massive claims would almost certainly go out of business very, very quickly.]

:::{#exm-median-two}
## Charles and Sadie Investigate Cupcake Public Relations
Charles and Sadie feel as though they may be finally catching a break when word gets around that the cupcake shop was using spoiled ingredients, making the patrons sick! The cupcake shop, in hearing this, sent their massive public relations team into damage control mode. They claimed that the median number of illnesses per week associated with the cupcake store was the same as most local food services businesses. Doing some digging, Charles and Sadie find the following two probability mass functions, for $X$ and $Y$, where $X$ is the number of weekly illnesses at the cupcake shop, and $Y$ is the number from a local business that has been around for a while. \begin{align*}
p_X(x) &= \begin{cases}
    0.1 & x = 0 \\ 
    0.1 & x = 1 \\
    0.35 & x = 2 \\
    0.05 & x = 3 \\ 
    0.4 & x = 25 \\
    0 & \text{otherwise};
\end{cases} \\
p_Y(y) &= \begin{cases}
    0.45 & x = 0 \\
    0.04 & x = 1 \\
    0.5 & x = 2 \\
    0.01 & x = 3
\end{cases}.
\end{align*}

a. What is the median of $X$?
b. What is the median of $Y$?
c. Is the claim made by the public relations team an accurate depiction of the world? Why or why not?

::::{.callout .solution collapse='true'}
## Solution

a. We can consider cumulative sums here again. In order we get, $0.1$, $0.2$, $0.55$. This suggests that $x=2$ is a candidate. If we check $P(X \geq 2) = 1 - 0.2 = 0.8$, and so $x=2$ is a median.

b. Using cumulative sums we get $0.45$, $0.49$, $0.99$. Again, this gives $x=2$ as a candidate. If we check $P(X \geq 2) = 1 - 0.49 = 0.51$, and so $x=2$ is a median.

c. The claim is accurate in that the median number for both establishments in $2$. However, here the median is a bad representation for $X$. The probability that more than $2$ cases occur in a week at the cupcake shop is $0.8$, compared with $0.51$ at the local business. What's more, the probability that $X$ is as high as $25$ is $0.4$, where $Y$ is *never* higher than $3$. 

::::
:::

Between the median and the mode we have two measures which capture some sense of expected value, each with their own set of strengths and drawbacks. Neither capture what it is that is referred to as *the* expected value. For this, we need to take inspiration from the median, and consider another way that we may think to find the center of the distribution.

### The Mean
If the median gives the middle reading along the values sequentially, we may also wish to think about trying to find the *center of gravity* of the numbers. Suppose you take a pen, or marker, or small box of chocolates, and you wish to balance this object on a finger or an arm. To do so, you do not place the item so that half of its length sits on one side of the appendage and half on the other. You adjust the location so that half of the *mass* sits on either side of the appendage.

Throughout our discussion of discrete random variables we have referred to probability as *mass*. We use the *probability mass function* to generate our probability values. This metaphor can be extended when we try to find the center of the distribution. If we imagine placing a mass with weight equal to the probability mass at each value that a random variable can take on, we may ask, "where would we have to place a fulcrum to have this number line be balanced?" The answer to this question serves as another possible measure of center. It turns out that this notion of center is the one that we are all most familiar with, the simple average, or mean. 

:::{#def-mean}
## Mean
The mean of a distribution is the center of mass of the distribution. For a random variable, $X$ with support $\mathcal{X}$, and probability mass function $p_X(x)$, the mean of $X$ is given by $$\sum_{x \in \mathcal{X}} xp_X(x).$$
:::

It is this measure of location which ends up being called the **expected value** in statistics. We will use average, expected value, mean, and expectation interchangeably. In terms of notation, the expected value of a random variable $X$ is denoted $E[X]$. Mathematically, the expected value is desirable for many reasons, some of which we will study in more depth later on. One of these desirable features, which stands in contrast with the median, is the comparative ease with which expected values can be computed. The summation for the expected value is easy to write down, and typically can be solved (either analytically, or readily with a computer).

:::{#exm-expected-value-one}
## Charles and Sadie Investigate Cupcake Sprinkles (One Last Time)
While the median and mode number of sprinkles on the cupcakes were the same, Charles and Sadie realize that this does not end up connecting well with the total number of sprinkles given out. Perhaps customers like the possibility of getting a large number of sprinkles, if they get lucky. As a result, they decide to round out the summary of the distribution by considering the mean number of sprinkles as well. Recall that the probability mass function for $X$, the number of sprinkles on top of the cupcakes, is $$p_X(x) = \begin{cases} 0.25 & x = 0 \\ 0.3 & x = 1 \\ 0.1 & x = 2 \\ 0.05 & x \in \{3,4,5,6,7,8,9\} \\ 0 & \text{otherwise}.\end{cases}$$

What is the mean number of sprinkles?

::::{.callout .solution collapse='true'}
## Solution
Recall that the mean of a distribution is given by $E[X] = \sum_{x\in\mathcal{X}}xP_X(x)$. In this case, this results in \begin{align*}
    &\sum_{x=0}^9 xp_X(x) \\
    &= 0(0.25) + 1(0.3) + 2(0.1) + (3+4+5+6+7+8+9)(0.05) \\
    &= 2.6.
\end{align*} As a result, the mean number of sprinkles used is $2.6$. This is higher than the median and mode, but is still not particularly high. 
::::
:::

In the case of an equally likely probability model, the expected value becomes the standard average that is widely used. Suppose that there are $n$ options in the support with $\mathcal{X} = \{x_1,\dots,x_n\}$. We can write $$E[X] = \sum_{i=1}^n x_i\frac{1}{n} = \frac{1}{n}\sum_{i=1}^nx_i.$$ This is the formula for the average that is most commonly applied. When the probability models are more complex, the formula is not precisely the standard average -- instead, it becomes a weighted average, where the weights are the probabilities.^[While less commonly applied than the simple average, a weighted average is familiar to most students for a crucial purpose: grade calculations. If you view the weight of each item in a course as a probability mass, and the grade you scored as the value, then your final grade in the course is exactly the expected value of this distribution.] The frequency with which expected values are used make them attractive as a quick summary for the center of a distribution.

### How is the Mean "Expected"?
While the mean provides a useful, intuitive measure of center of the distribution, it is perhaps counterintuitive to name it the "expected value." To understand the naming convention it is easiest to consider the application which has likely spurred more development of statistics and probability than any other: gambling.

Suppose that there is some game of chance that can pay out different amounts with different probabilities. A critical question for a gambler in deciding whether or not to play such a game is "how much can I expect to earn, if I play?" This is crucial to understanding, for instance, how much you should be willing to pay to participate, or if you are the one running the game, how much you should charge to ensure that you make a profit. 

If you want to understand what you expect to earn, the intuitive way of accomplishing this is to weight each possible outcome by how likely it is to occur. This is exactly the expected value formula that has been provided, and so the expected value can be thought of as the expected payout of a game of chance where the outcomes are payouts corresponding to each probability. 

:::{.callout-tip icon="false"}
## Cost for a Game of Chance
Suppose that a game of chance is being run. It will cost $\$m$ to play, and the game will pay out values from $\mathcal{X}$, according to a random variable $X$ with probability mass function $p_X(x)$. The question that we want to know is what should the value be for $m$, such that, in the long term, a player playing the game will earn no money?

Note that if $X=x$ then a player playing the game will earn $x - m$ for that play, and this will happen with probability $p_X(x)$. Thus, in the long run, in $p_X(x)$ proportion of games the player earns $x-m$. If the player plays $N$ games, then as $N\to\infty$, the total contribution from these winnings will be $(x-m)\cdot N\cdot p_X(x)$. If we add up this contribution for all possible results, $x$, we get \begin{align*}
\sum_{x\in\mathcal{X}} N(x-m)p_X(x) &= N\left[\sum_{x\in\mathcal{X}}xp_X(x) - \sum_{x\in\mathcal{X}} mp_X(x)\right] \\
&= N\left[\sum_{x\in\mathcal{X}}xp_X(x) - m\sum_{x\in\mathcal{X}}p_X(x)\right] \\
&= N\left[E[X] - m\right].
\end{align*} Thus, in the long run, the total that the player wins, as $N\to\infty$ will be $N(E[X] - m)$. Thus, in order for the long run earnings to be zero, $m$ should be set to be equal to $E[X]$.

If $m$ is set to be $0$, then per game the player *expects* to earn $E[X]$. This also represents the cost at which a rational actor should be willing to pay to participate. If a game of chance costs more than the expected value to play, in the long run you will lose money. If a game of chance costs less than the expected value, in the long run you will earn money. It is hard to overstate the utility of gambling in developing probability theory, and as such these types of connections are expected.
:::

To interpret the expected value of a random variable, one possibility is using the intuition that we used to derive the result. Notably, the expected value is the center of mass of the distribution, where the masses correspond to probabilities. This means that it is not necessarily an actual central number over the range, but rather that it sits in the weighted middle. While this interpretation is useful in many situations, there are times where the point of balance is a less intuitive description. For these, it can sometimes be useful to frame the expected value as the long term simple average from the distribution.

If we imagine observing many independent and identically distributed random variables, then as the number of samples tends to infinity, the expected value of $X$ and the simple average will begin to coincide with one another. That is the distance between $E[X]$ and $\frac{1}{n}\sum_{i=1}^n X_i$ will shrink to $0$. As a result, we can view the expected value as the average over repeated experiments. This interpretation coincides nicely with the description based on games of chance. Specifically, if you were to repeatedly play the same game of chance, the average payout per game will be equal to the expected value, if you play for long enough.

```{r}
#| echo: false
#| label: fig-plot
#| fig-cap: "This plot produces 50000 realizations of the random variable described in @exm-expected-value-one. As computed in the example, the expected value is 2.6. As many repeated observations are averaged, the empirical average converges to this computed value."

set.seed(31415)
d_draws <- 50000
vals <- 0:9
probs <- c(0.25, 0.3, 0.1, rep(0.05, 7))
realizations <- sample(x = vals, size = d_draws, prob = probs, replace = TRUE)

x <- 1:d_draws
y <- cumsum(realizations)/x

plot(y ~ x, main = "Long run average and expected value.", ylab = "Average", xlab = "Number of Realizations", type = 'l')
abline(h=2.6, lty=3)
```


## Which Measure of Central Tendency Should be Used?
Where the median demonstrated robustness against extreme values in the distribution, the mean does not. For instance, if we consider the distribution of incomes across a particular region, the mean will be much higher than the median, since those families with exceptionally high incomes will not be smoothed over as they were with medians. In this case, the lack of robustness for the expected value will render the mean a less representative summary for the true behaviour of the random quantity. 

To see this concretely consider a random variable which with equal probability takes a value between $1$ and $9$. This will have $E[X] = 5$. Now, if the $9$ is made to be $1,000,000$, the expected value will now be $E[X] = 111115.\dot1$. This is a far cry from the median which does not change from $5$ in either case. This lack of robustness is desirable in the event of the insurance example from the median discussion, but will be less desirable in other settings. 

The mean, median, and mode are the three standard measures of central tendency. They are single values which describe the standard behaviour of a random quantity. Each of the three has merits as a measure, and each has drawbacks for certain settings. The question of which to use and when depends primarily on the question of interest under consideration, rather than on features of the data alone.^[The data is one consideration for which measure to use, but not the only one (and not the most important one).] Often, presenting more than one measure can give a better sense of the distributional behaviour that any one individual will.

:::{#exm-choosing-measure-of-central-tendency}
## Charles and Sadie Reflect on the Cupcake Adventures
Charles and Sadie decide it is worth stepping back and summarizing all that has happened with regards to their cupcake adventures, trying to ensure that distributions are always summarized fairly. 

a. For the number of sprinkles per cupcake is the mean, median, or mode the best measure of central tendency?
b. For the amount happiness in customers, is the mean, median, or mode the best measure of central tendency?
c. For the number of customers who become ill eating at establishments, is the mean, median, or mode the best measure of central tendency? 

::::{.callout .solution collapse='true'}
## Solution
a. There is an argument that any of the measures here could work the best, it would depend on the individual's preferences. The mode makes a lot of sense as, if you are a regular customer, the mode is what you will experience day-to-day; most days you will have the modal number of sprinkles. The median may make sense as it provides a benchmark for measuring what constitutes a lot of sprinkles and a little. Half of the time you'll end up with a more sprinkled donut, half the time a less. The mean would be particularly interesting to note for the store owners themselves, since the mean is directly tied to totals: if the store knows that they sell $100$ cupcakes per day, most days, then they also can expect to use $100$ times the mean number of sprinkles in a day. This is helpful for planning.

b. The median is likely the most useful measure in this setting. The mode could be useful if happiness were not measured on a $100,000$ point scale. The mean is likely a less relevant value here as we are not particularly concerned with the total happiness, and if there was a large skew or major outliers in this distribution, we would want to determine where the majority fall. This is more analogous to the income example rather than the insurance example.

c. In this case either the mode or mean are likely the best gauges. Here we do care about totals, and in particular, we do not want to smooth over outliers. It is very relevant if sometimes a lot of individuals become ill after eating an establishment, and the median would hide this information.
::::
:::

Despite the utility of all three measures, the expected value holds a place of more central importance in probability and statistics. A lot of this has to do with further mathematical properties of the mean. Because of its central role, it is worth studying the expected value in some more depth.

## Expected Values of Functions of Random Variables
Sometimes the value of a random variable needs to be mapped through a function to give the value which is most relevant to us. Consider, for instance, a situation wherein the side lengths of boxes being manufactured by a specific supplier are random, due to incorrectly calibrated tolerances in the machines. The resulting boxes are perfect cubes. Suppose we are interested in the volume of the produced box not the side length. If a box has side length $x$, then its volume will be $x^3$, and so we may desire some way of computing $E[X^3]$ rather than $E[X]$.

Generally, for a function $g(X)$, we may want to compute $E[g(X)]$. It is important to recognize that $E[g(X)] \neq g(E[X])$. This is a common mistake.^[and an attractive one, but a mistake nonetheless.] If we are unable to apply the function to the expected value, then the question of how to compute the expected value remains. Instead of applying the function to overall expected value, instead, we apply the function to *each* value in the defining relationship for the expected value. That is, $$E[g(X)] = \sum_{x\in\mathcal{X}} g(x)p_X(x).$$ This is sometimes referred to as the "law of the unconscious statistician," a name which may be aggressive enough to help remember the correct way to compute the expectation.^[Some statisticians dislike this name. I find it to be rather cute.]

:::{.callout-tip icon="false"}
## The Law of the Unconscious Statistician
The law of the unconscious statistician (LOTUS) states that, for a random variable $X$, if we wish to find $E[g(X)]$, then we compute $$E[g(X)] = \sum_{x\in\mathcal{X}} g(x)p_X(x).$$
:::

:::{#exm-expected-value-transformation}
## The Happiness Scale Inversion
Charles and Sadie have made really great strides working to protect their favourite coffee shop from the new cupcake store. One day when digging through the material more, they realize that the happiness report produced by the company is even less accurate than they had originally reported! The company reported the following probability mass function for happiness points 
$$
p_X(x) = \begin{cases}
    \frac{x}{5,000,050,000} & x \in \{1,\dots,100 000\} \\
    0 & \text{otherwise}.
\end{cases}$$ Charles and Sadie track down the source of this expression and they find that, in fact, this does not measure happiness points at all. Instead, the number of happiness points is a function of $X$, specifically, $Z = 1/X$.

a. What is the expected value of $X$? Note, it may be helpful to recall that $\sum_{x=1}^{k} x^2 = \frac{k(k+1)(2k+1)}{6}$.
b. What is the expected value of $Z$?

::::{.callout .solution collapse='true'}
## Solution
a. Using directly the formula for $E[X]$ gives \begin{align*}
E[X] = \sum_{x=1}^{100000} xp_X(x) &= \sum_{x=1}^{100000} x\frac{x}{5000050000} \\
&= \frac{1}{5000050000}\sum_{x=1}^{100000}x^2 \\
&= \frac{1}{5000050000}\cdot\frac{100000(100000+1)(2(100000)+1)}{6} \\
&= 66 667.
\end{align*}

b. Applying the law of the unconscious statistician, gives \begin{align*}
E[Z] = E[1/X] = \sum_{x=1}^{100000} \frac{1}{x}p_X(x) &= \sum_{x=1}^{100000} \frac{1}{x}\frac{x}{5000050000} \\
&= \frac{1}{5000050000}\sum_{x=1}^{100000} 1 \\
&= \frac{100000}{5000050000} \\
&= \frac{2}{100001}.
\end{align*}
::::
:::

These functions applied to random variables are often thought of as "transformations" of the random quantities. For instance, we *transformed* a side length into a volume. While the law of the unconscious statistician will apply to any transformation for a random variable, we can sometimes use shortcuts to circumvent its application. In particular, when $g(X) = aX + b$, for constant numbers $a$ and $b$, we can greatly simplify the expected value of the transformation. To see this note \begin{align*}
E[aX + b] &= \sum_{x\in\mathcal{X}}(ax + b)p_X(x) \\
&= \sum_{x\in\mathcal{X}}axp_X(x) + bp_X(x) \\
&= a\sum_{x\in\mathcal{X}}xp_X(x) + b\sum_{x\in\mathcal{X}}p_X(X) \\
&= aE[X] + b.
\end{align*} That is, in general, we have that $E[aX + b] = aE[X] + b$. Note that part of the property of the linearity of expectation that we can immediately see if that the expected value of any constant is always that constant. If we take $a = 0$, then we see that $E[aX + b] = E[b] = b$. Thus, any time that we need to take the expected value of any constant number, we know that it is just that number.


This is particularly useful as linear transformations like $aX+b$ arise very commonly. For instance, most unit conversions are simple linear combinations. If a random quantity is measured in one unit then this result can be used to quickly convert expectations to another.

:::{#exm-temperature-conversion}
## Sadie's Trip to America
Sadie has recently returned from a long trip to America. The trip was long enough that temperatures measured in Fahrenheit started to make sense. When Sadie and Charles begin to talk about the weather, Charles brings up the temperature distribution of a possible summer vacation spot. Unfortunately for Sadie, these temperatures are all in Celsius. The distribution Charles provides is 
$$p_X(x) = \begin{cases}
    0.1 & x \in \{10, 11, 12, 13, 14\} \\
    0.05 & x \in \{15, 16, 17, 18, 19, 20, 21, 22, 23, 24\} \\
    0 & \text{otherwise}
\end{cases}$$

a. What is the expected temperature, in Celsius?
b. Supposing that the temperature in Fahrenheit is given by $Y = 1.8X + 32$, what is the expected temperature in Fahrenheit?

::::{.callout .solution collapse='true'}
## Solution
a. Here we find \begin{align*}
    \sum_{x=10}^{24} xp_X(x) &= 0.1\sum_{x=10}^{14}x + 0.05\sum_{x=15}^{24}x \\
    &= 15.75.
\end{align*}

b. Since $E[X] = 15.75$ then $E[Y] = E[1.8X + 32] = 1.8E[X] + 32 = 60.35$.
::::
:::

This type of linear transformation also frequently comes up with games of chance and payouts, or with scoring more generally.^[For instance, suppose you are betting a certain amount on the results of a coin toss, or that you are taking a multiple choice test that gives $2$ points for a correct answer.] Beyond being linear over simple transformations, summations in general behave nicely with expectations. Specifically, for any quantities separated by addition, say $g(X) + h(X)$, the expected value will be the sum of each expected value. Formally, \begin{align*}
E[g(X) + h(X)] &= \sum_{x\in\mathcal{X}} (g(x) + h(x))p_X(x) \\
&= \sum_{x\in\mathcal{X}} g(x)p_X(x) + h(x)p_X(x) \\
&= \sum_{x\in\mathcal{X}} g(x)p_X(x) + \sum_{x\in\mathcal{X}} h(x)p_X(x) \\
&= E[g(X)] + E[h(X)].
\end{align*}

Behaving well under linearity is one of the very nice properties of expectations. It will come in useful when dealing with a large variety of important quantities, and as we will see shortly, this linearity will also extend to multiple different random quantities.

Measures of central tendency are important to summarize the behaviour of a random quantity. Whether using the mean, median, or mode, these measures of location describe, on average, what to expect from observations of the random quantity. However, understanding a distribution requires understanding far more than simply the measures of location. As was discussed previously, the probability mass function captures the complete probabilistic behaviour of a discrete random variable, it is only intuitive that some information would be lost with a single numeric summary.

:::{.callout-warning icon="false"}
## Equal Location Across Different Distributions
Consider the following three distributions for three random variables, $X$, $Y$, and $Z$: \begin{align*}
p_X(x) = \begin{cases}
    0.25 & x = -1 \\
    0.5 & x = 0 \\ 
    0.25 & x = 1 \\
    0 & \text{otherwise}.
\end{cases} \quad
p_Y(y) &= \begin{cases}
    0.05 & x = -5 \\
    0.1 & x = -4 \\
    0.1 & x = -3 \\
    0.1 & x = -2\\
    0.1 & x = -1\\
    0.15 & x = 0 \\ 
    0.1 & x = 1\\
    0.1 & x = 2\\
    0.1 & x = 3\\
    0.1 & x = 4 \\
    0.05 & x = 5 \\
    0 & \text{otherwise}.
\end{cases} \quad
p_Z(z) = \begin{cases}
    0.25 & x = -400 \\
    0.5 & x = 0 \\
    \frac{1}{3196} & x \in \{1,\dots,799\}\\
    0 & \text{otherwise}.
\end{cases}.
\end{align*}

In each of these distributions we have the mean, median, and mode all equalling $0$.^[Try working this out!] However, even just from a quick glance, these distributions are all very, very differently behaved. The location summaries here clearly miss much of the important information about these different distributions. Spend some time trying to think of the ways in which they differ from one another, and see if you can determine *what* is missing from relying solely on measures of location.
:::

## Summarizing the Variability of a Random Variable
A key characteristic of the behaviour of a random variable which is not captured by the measures of location is the variability of the quantity. If we imagine taking repeated realizations of a random variable, the variability of the random variable captures how much movement there will be observation to observation. If a random variable has low variability, we expect that the various observations will cluster together, becoming not too distant from one another. If a random variable has high variability, we expect the observations to jump around each time.

## The Range
Just as was the case with measures of location, there are several measures of variability which may be applicable in any given setting. One fairly basic measure of this variability is the range of possible values: what is the highest possible value, what is the lowest possible value, and how much distance is there between those two points? 

:::{#def-range}
## Range

For a random variable, $X$, the range of the random variable is defined as $\text{Range}(X) = \max(X) - \min(X)$. That is, it is the distance between the maximum value that the random variable can take on, and the minimum value that the random variable can take on. Sometimes the range is reported with these outpoints explicitly specified.
:::

This is a fairly intuitive notion, and is particularly useful in the equal probability model over a sequence of numbers. Consider dice. Dice are typically defined by the range of values that they occupy, say $1$ to $6$, or $1$ to $20$. Once you know the values present on any die, you have a sense for how much the values can move observation to observation. 

:::{#exm-range-works}
## Charles and Sadie Explore Ice Cream Flavors

Charles and Sadie have decided to spend their sunny afternoon exploring various ice cream flavors at their local parlor, *Symmetric Scoops*. They notice that Scoops Galore offers a wide variety of flavors, from classic vanilla to exotic dragon fruit swirl. Intrigued by the selection, they decide to investigate the probability distribution of a random variable, $Y$, representing the number of unique flavors a customer selects.

After discreetly observing several customers, Charles and Sadie jot down their findings:
$$
p_Y(y) = \begin{cases} 
0.2 & y = 1 \\
0.35 & y = 2 \\
0.3 & y = 3 \\
0.15 & y = 4 \\
0 & \text{otherwise}
\end{cases}
$$

What is the range of the random variable $Y$, representing the number of unique ice cream flavors a customer selects at Scoops Galore?

::::{.callout .solution collapse='true'}
## Solution
Looking at the defining relationship for this random variable, we can see that the maximum value the distribution can take on is $4$ (with probability $0.15$) and the minimum value is $1$ (with probability $0.2$). As a result, we say that $\text{Range}(Y) = 4 - 1 = 3$.
::::
:::

While the range is an important measure to consider to determine the behaviour of a random variable, it is a fairly crude measurement. It may be the case that, while the extreme values are possible, they are sufficiently unlikely so as to come up very infrequently and not remain representative of the likely spread of observations. Alternatively, many random variables have a theoretically infinite range. In these cases, providing the range will likely not provide much utility.

:::{#exm-range-dnw}
## Charles and Sadie Explore Ice Cream Flavors, Again

Having had so much fun at *Symmetric Scoops* the first time, Charles and Sadie return once more to continue to investigate customers behaviour. When they arrive they notice a temporary promotion going on, which happens on one randomly selected Sunday a year, where customers can buy a *Mega Sunda(y)e Extravaganza*, a wonderfully extravagant creation that features scoops from all $50$ flavours on offer. Working on this today they find the following probability mass function for $Y$.
$$
p_Y(y) = \begin{cases} 
0.2 & y = 1 \\
0.35 & y = 2 \\
0.3 & y = 3 \\
0.145 & y = 4 \\
0.005 & y = 50 \\
0 & \text{otherwise}
\end{cases}
$$

What is the range of the random variable $Y$ now? Does this accurately represent the dispersion we expect from $Y$?

::::{.callout .solution collapse='true'}
## Solution
Looking at the defining relationship for this random variable, we can see that the maximum value the distribution can take on is $50$ (with probability $0.005$) and the minimum value is $1$ (with probability $0.2$). As a result, we say that $\text{Range}(Y) = 50 - 1 = 49$. This does **not** accurately represent the dispersion of the random variable. Not only do very few customers order this when it is available, it is also almost never available. As a result, the maximum value (while possible) is not reflective of the usual behaviour of this random quantity, and this displays one of the concerns with the range as a measure of spread.
::::
:::

## The Interquartile Range
To remedy these two issues, we may think of techniques to modify the range. Instead of taking the minimum and maximum possible values, we can instead consider ranges of values which remain more plausible. A common way to do this is to extend our concept of a median beyond the half-way point. The median of a random variable $X$, is the value, $m$, such that $P(X \leq m) = 0.5$^[And, as a result, $P(X > m) = 0.5$ as well.]. While there is good reason to care about the midpoint, we can think of generalizing this to be *any* probability.

That is, we could find a number $z$, such that $P(X \leq z) = 0.1$. We could then use this value to conclude that the probability of observing a value less than $z$ is $10\%$. These values are referred to, generally, as **percentiles** and they are the natural extension of medians. 

:::{#def-percentile}
The $100p$th percentile (e.g., $70$th percentile for $p=0.7$, or $20$th percentile for $p=0.20$), is denoted $\zeta(p)$ and is the value such that $P(X \leq \zeta(p)) = p$. Thus, the median is given by $\zeta(0.5)$ and is also called the $50$th percentile.^[Note that, when dealing with discrete random variables, it may not be possible to find a value $\zeta(p)$ such that $P(X \leq \zeta(p)) = p$ exactly. Instead, we typically define the percentile here to be such that $P(X < \zeta(p)) < p \leq P(X \leq \zeta(p))$.]
:::

:::{#exm-percentile}
## Perplexed at the Ice Cream Parlour
Charles and Sadie remained somewhat disappointed by their lack of ability to accurately capture the behaviour of a random variable using the range. To this end, they spend a lot more time at the ice cream parlor, and come up with, what they believe, is the correct probability mass function for how many flavours customers order, in total. 
$$
p_Y(y) = \begin{cases} 
0.25 & y = 1 \\
0.25 & y = 2 \\
0.1 & y = 3 \\
0.05 & y = 4 \\
0.05 & y = 5 \\
0.05 & y = 6 \\
0.15 & y = 7 \\
0.05 & y = 8 \\
0.04 & y = 9 \\
0.005 & y = 10 \\ 
0.005 & y = 50 \\
0 & \text{otherwise}
\end{cases}
$$

Using this probability mass function, find $\zeta(0.25)$, $\zeta(0.5)$, $\zeta(0.75)$, and $\zeta(0.99)$. What do each of these values mean?

::::{.callout .solution collapse='true'}
## Solution

a. $\zeta(0.25)$ is found by looking for $P(Y \leq \zeta(0.25)) = 0.25$. We note that $P(Y \leq 1) = P(Y = 1) = 0.25$, and so $\zeta(0.25) = 1$. This means that there is a $25\%$ chance that a randomly selected individual will order $1$ or fewer flavours. 
b. $\zeta(0.5)$ is found by looking for $P(Y \leq \zeta(0.5)) = 0.25$. Note that $P(Y \leq 2) = P(Y = 1) + P(Y = 2) = 0.5$, and so $\zeta(0.5) = 2$. This means that $50\%$ of customers order $2$ or fewer flavours, and $50\%$ of customers order more than $2$ flavours.
c. $\zeta(0.75)$ is found by looking for $P(Y \leq \zeta(0.75)) = 0.75$. Note that we can continue the cumulative sums from the probability mass function. This gives, in order, $0.25, 0.5, 0.6, 0.65, 0.7, 0.75, 0.9, 0.95, 0.99, 0.995, 1$. As a result, $P(Y \leq 6) = 0.75$ and so $\zeta(0.75) = 6$. This means that $75\%$ of customers order $7$ or fewer flavours (the remaining $25\%$ order more than $7$).
d. $\zeta(0.99)$ can be found through the same cumulative sums as in (c). This is given by $y = 9$, such that $P(Y \leq 9) = 0.99$, so $\zeta(0.99) = 9$. This means that $99\%$ of individuals order $9$ or fewer flavours. 

::::
:::

We can leverage percentiles to remedy some of the issues with the range as a measure of variability. Framed in terms of percentiles, the minimum value is $\zeta(0)$, and the maximum value is $\zeta(1)$. Instead of considering the extreme endpoints, we can consider the difference between more moderate percentiles. Doing so allows us to overcome the major concerns outlined with the range. If we take $\zeta(p_1)$ and $\zeta(p_2)$, for $p_1 < p_2$, then the difference between $\zeta(p_2) - \zeta(p_1)$ can be seen as a measure of variability, analogous to the range.

The most common choices would be to take $\zeta(0.25)$ and $\zeta(0.75)$, the $25$th and $75$th percentiles, respectively. These are also referred to as the first and third quartiles, respectively. They are named as, taking $\zeta(0.25)$, $\zeta(0.5)$ and $\zeta(0.75)$, the distribution is cut into quarters.

With the first and third quartiles computed, we can compute the **interquartile range**, which is given by $\zeta(0.75)-\zeta(0.25)$. 

:::{#def-iqr}
## Interquartile Range (IQR)
The interquartile range, or IQR, is defined as $\zeta(0.75) - \zeta(0.25)$, the difference between the third and first quartiles. It is a measure of spread, and is typically denoted as $\text{IQR} = Q3 - Q1$, where $Q$ stands for quartiles.
:::

Like the range, the IQR gives a measure of how much spread there tends to be in a distribution. Unlike the range, however, we can be more certain that both the first and third quartiles are reasonable values around which repeated observations of the random variable would be observed. Specifically, there is a probability of $0.5$ that a value between the first and third quartile will be observed. The larger the $\text{IQR}$, the more spread out these moderate observations will be, and as a result, the more variable the distribution is.

:::{#exm-percentile}
## Addressing the Ice Cream Perplexity
Having understood many of the percentiles of the distribution of ice cream flavours ordered at *Symmetric Scoops*, Charles and Sadie decide to have another shot at capturing the spread. Recall that the probability mass function for $Y$, the number of ice cream flavours ordered by a customer, is given by 
$$
p_Y(y) = \begin{cases} 
0.25 & y = 1 \\
0.25 & y = 2 \\
0.1 & y = 3 \\
0.05 & y = 4 \\
0.05 & y = 5 \\
0.05 & y = 6 \\
0.15 & y = 7 \\
0.05 & y = 8 \\
0.04 & y = 9 \\
0.005 & y = 10 \\ 
0.005 & y = 50 \\
0 & \text{otherwise}
\end{cases}
$$

What is the interquartile range for this population? How does this compare to the range?

::::{.callout .solution collapse='true'}
## Solution
We know that $\zeta(0.25) = 1$ and $\zeta(0.75) = 6$. As a result, the $\text{IQR} = 6 - 1 = 5$. By contrast, the range is given by $50 - 1 = 49$. It is incredibly rare to observe a customer ordering $50$ flavours, as a result, using $5$ is a better measure of spread of the distribution. To see this, we could consider plotting a histogram (more on these later on) that show the number of customers who order each type, and then intuitively as for a good measure of how *spread out* the data are.

```{r}
#| echo: false
#| label: symmetric_scoops_graph
#| cache: true
set.seed(31415)
results <- sample(size = 10000, x = c(1,2,3,4,5,6,7,8,9,10,50), prob = c(0.25,0.25,0.1,0.05,0.05,0.05,0.15,0.05,0.04,0.005,0.005), replace = TRUE)
hist(results,
main = "Histogram of the Number of Flavours \n Ordered by Customers at Symmetric Scoops", 
ylab = "Proportion of Customers",
xlab = "Number of Flavours Ordered",
breaks = c(0,1,2,3,4,5,6,7,8,9,10,50,51))
```
::::
:::

## The Variance and Mean Absolute Deviation
Both the range and the interquartile range give a sense of the variation in the distribution irrespective of the measures of location for that distribution. Another plausible method for assessing the variability of a distribution is to assess how far we expect observations to be from its center. Intuitively, if observations of $X$ are near the center with high probability, then the distribution will be less variable than if the average distance to the center is larger. 

This intuitive measure of variability is useful for capturing the behaviour of a random variable, particularly when paired with a measure of location. However, we do have to be careful: not all measures of dispersion based on this notion will be useful. Consider the most basic possibility, $X - E[X]$. We might ask, for instance, what is the expected value of this quantity. If we take $E[X - E[X]]$ then note that this a linear combination in expectation since $E[X]$ is just some number. Thus, $E[X-E[X]] = E[X] - E[X] = 0$. In other words, the expected difference between a random variable and its mean is exactly $0$. We thus need to think harder about how best to turn this intuition into a useful measure of spread.

The issue with this procedure is that some realizations are going to be below the mean, making the difference negative, and some will be above the mean, making the difference positive. Our defining relationship for the mean relied on balancing these two sets of mass. However, when discussing the variability of the random variable, we do not much care whether the observations are lower than expected or higher than expected, we simply care how much variability there is around what is expected. To remedy this, we should consider only the distance between the observation and the expectation, not the sign. That is, if $X$ is $5$ below $E[X]$ we should treat that the same as if $X$ is $5$ above $E[X]$.

There are two common ways to turn value into its magnitude in mathematics generally: squaring the number and using absolute values. Both of these tactics are useful approaches to defining measures of spread, and they result in the **variance** when using the expected value of the squared deviations, and the **mean absolute deviation** when using the absolute value. While $E[|X-E[X]|]$ is perhaps the more intuitive quantity to consider, generally speaking it will not be the one that we use. 

:::{#def-variance}
## Variance
The variance of a random variable, typically denoted $\text{var}(X)$, is given by the expected value of the squared deviations of a random variable from its mean. That is, $$\text{var}(X) = E\left[(X - E[X])^2\right].$$
:::

:::{#def-MAD}
## Mean Absolute Deviation
The mean absolute deviation of a random variable, typically denoted $\text{MAD}(X)$, is given by the expected value of the absolute value of the deviations of a random variable from its mean. That is, $$\text{MAD}(X) = E\left[|X - E[X]|\right].$$
:::

In general when we need a positive quantity in mathematics it will typically be preferable to consider the square to the absolute value.^[The reasons for this are plentiful, but generally squares are easier to handle than absolute values, and as a result become more natural quantities to consider.] The variance is **the** central measure of deviation for random variables. When discussing the variability of a random variable, it will almost universally be in reference to the variance.

Note that if we take $g(X) = (X-E[X])^2$, then the variance of $X$ is the expected value of a transformation. We have seen that to compute these we apply the law of the unconscious statistician, and substitute $g(X)$ into the defining relationship for the expected value, which for the variance gives $$\text{var}(X) = \sum_{x\in\in\mathcal{X}} (x-E[X])^2p_X(x).$$ In order to compute the variance, we must also find the mean as the function $g(X)$ relies upon this value.

:::{#exm-variance}
## Variation in the Number of Ice Cream Flavours Ordered
Noting how different the range and IQR were of the distribution of number of different flavours ordered by customers at the ice cream parlour, Charles and Sadie decide to turn to the variance and mean absolute deviation to try understand the distribution's variability once-and-for-all. Recall that the probability mass function for $Y$, the number of ice cream flavours ordered by a customer, is given by 
$$
p_Y(y) = \begin{cases} 
0.25 & y = 1 \\
0.25 & y = 2 \\
0.1 & y = 3 \\
0.05 & y = 4 \\
0.05 & y = 5 \\
0.05 & y = 6 \\
0.15 & y = 7 \\
0.05 & y = 8 \\
0.04 & y = 9 \\
0.005 & y = 10 \\ 
0.005 & y = 50 \\
0 & \text{otherwise}
\end{cases}
$$

Find the variance and the mean absolute variation for this distribution.

::::{.callout .solution collapse='true'}
## Solution
For both $\text{var}(Y)$ and $\text{MAD}(Y)$ we require the expected value of $Y$. To this end note that \begin{multline*}E[Y] = (0.25)(1) + (0.25)(2) + (0.1)(3) + (0.05)(4) + (0.05)(5) + (0.05)(6) + (0.15)(7) \\ + (0.05)(8) + (0.04)(9) + (0.005)(10) + (0.005)(50) = 3.91.\end{multline*}

Then, it can be useful to define a table with the probabilities, absolute, and squared deviations, so that the summations can be made easier. 

| $Y$            | $p_Y(y)$       | $(Y - E[Y])^2$ | $|Y - E[Y]|$   |
|:--------------:|:--------------:|:--------------:|:--------------:|
|  $1$           | $0.25$         | $8.4681$       | $2.91$         |
|  $2$           | $0.25$         | $3.6481$       | $1.91$         |
|  $3$           | $0.1$          | $0.8281$       | $0.91$         |
|  $4$           | $0.05$         | $0.0081$       | $0.09$         |
|  $5$           | $0.05$         | $1.1881$       | $1.09$         |
|  $6$           | $0.05$         | $4.3681$       | $2.09$         |
|  $7$           | $0.15$         | $9.5481$       | $3.09$         |
|  $8$           | $0.05$         | $16.7281$      | $4.09$         |
|  $9$           | $0.04$         | $25.9081$      | $5.09$         |
|  $10$          | $0.005$        | $37.0881$      | $6.09$         |
|  $50$          | $0.005$        | $2124.2881$    | $46.09$        |

With this table we can compute both the variance and mean absolute deviation by taking the second column times by the third, and adding up those results (for the variance) and the second column times the fourth, and adding up those results (for the MAD). Doing so results in $\text{var}(Y) = 17.5019$ and $\text{MAD}(Y) = 2.592$.
::::
:::

### Standard Deviation
The higher that an individual random variable's variance is, the more spread we expect there to be in repeated realizations of that quantity. Specifically, the more spread out around the mean value the random variable will be. A random variable with a low variance will concentrate more around its mean value than one with a higher variance. One confusing part of the variance of a random variable is in trying to assess the units. Suppose that a random quantity is measured in a particular set of units -- dollars, seconds, grams, or similar. In this case, our interpretations of measures of location will all be in the same units, which aids in drawing connections to the underlying phenomenon that we are trying to study. However, because the variance is squared, we cannot make the same extensions to it. Variance is not measured in the regular units, but in the regular units, *squared*.

Suppose you have a random time being measured, perhaps the reaction time for some treatment to take effect in a treated patient. Finding the mean or median will give you a result that you can read off in seconds. The range and interquartile range both give you the spread in seconds. However, if you work out the variance of this quantity it will be measured in seconds squared -- a unit that is challenging to have much intuition about. To remedy this we will often use a transformed version of the variance, called the **standard deviation**, returning the units to be only the original scale. 

:::{#def-sd}
The standard deviation of a random variable is the square root of the variance, which is to say $$\text{SD}(X) = \sqrt{\text{var}(X)}.$$ 
:::

We do not often consider computing the standard deviation directly, and so will most commonly refer to the variance when discussing the behaviour of a random variable, but it is important to be able to move seamlessly between these two measures of spread.

:::{#exm-sd}
## Variation in the Number of Ice Cream Flavours Ordered, the Finale
Having realized that the IQR and range are not directly comparable to the variance, as they are measured in different units, Charles and Sadie decide to end their investigation in the variability of the number of ice cream flavours ordered by calculating the standard deviation of $Y$. Recall that the probability mass function for $Y$, the number of ice cream flavours ordered by a customer, is given by 
$$
p_Y(y) = \begin{cases} 
0.25 & y = 1 \\
0.25 & y = 2 \\
0.1 & y = 3 \\
0.05 & y = 4 \\
0.05 & y = 5 \\
0.05 & y = 6 \\
0.15 & y = 7 \\
0.05 & y = 8 \\
0.04 & y = 9 \\
0.005 & y = 10 \\ 
0.005 & y = 50 \\
0 & \text{otherwise}
\end{cases}.
$$

What is the standard deviation of this random variable?

::::{.callout .solution collapse='true'}
## Solution
The standard deviation is given by the square root of the variance. This is given by $\sqrt{17.5019} = 4.18353$. 
::::
:::

:::{#exm-dolphin-olympics}
## The Dolphin Olympics
Sadie and Charles, on a trip to a beach in a particularly quirky town, find themselves watching the **dolphin Olympics**. These are a set of games put on by the local dolphin populations where they see how high they can jump from the water, and how many flips they can do while they do it. Charles and Sadie get to work determining the probability distributions related to the heights and the number of flips, with Sadie taking the heights and Charles taking the number of flips. At the end of the day they are discussing their findings and Sadie indicates that the standard deviation of the height that was jumped was $1.6m$. Charles is intrigued, saying, "I could have sworn that there was more variation in the jump heights than in the number of flips, but the variance of the flips was $2.56$, greater than the variability in the heights!"

Is Charles correct? Why or why not?

::::{.callout .solution collapse='true'}
## Solution
Charles has compared the standard deviation of the first quantity to the variance of the second. We must compare the same measure, and as a result, conclude that the variance of the heights was $1.6^2 = 2.56$, exactly the same as the second.
::::
:::

### Computing the Variance 
When computing the variance of a random quantity, we often use a shortcut for the formula, $$\text{var}(X) = E[X^2] - E[X]^2.$$ Generally, this is moderately more straightforward to calculate since $X^2$ is an easier transformation than $(X-E[X])^2$. This identity will come back time and time again, with a lot of versatility in the ways that it can be used. Typically, when a variance is needed to be calculated the process is to simply compute $E[X]$ and $E[X^2]$, and then apply this relationship.

:::{.callout-warning icon="false" collapse="true"}
## Proof of the Variance Identity
To determine the variance identity, we need only remember the definition of the variance (being $E[(X-E[X])^2]$), and then use summation techniques to manipulate the expression. To this end consider, \begin{align*}
\text{var}(X) &= \sum_{x\in\mathcal{X}} (x-E[X])^2p_X(x) \\
&= \sum_{x\in\mathcal{X}} (x^2 - 2xE[X] + E[X]^2)p_X(x) \\
&=\sum_{x\in\mathcal{X}} x^2p_X(x) - 2E[X]\sum_{x\in\mathcal{X}}xp_X(x) + E[X^2]\sum_{x\in\mathcal{X}}p_X(x)\\
&= E[X^2] - 2E[X]E[X] + E[X]^2\\
&= E[X^2] - E[X]^2.
\end{align*}
:::

:::{#exm-var-calc}
## Choosing the Outcome of a Die
Charles and Sadie come across a new game of chance involving the rolling of a die. In it a player chooses a number between $1$ and $6$. They then roll a die (up to) $6$ times. If they get their chosen number on the first throw for the first time, they get $\$1$. If they get their chosen number on the second throw, they get $\$2$. The same goes for the first time seeing their chosen number from throws $3$ through $6$. If they never get the chosen number, they have to pay $\$1$. 

What is the expected value and variance of a player playing this game?

::::{.callout .solution collapse='true'}
## Solution
Let $X$ represent the winnings of a play of this game. We know that $X$ takes a value in $\{-1, 1, 2, 3, 4, 5, 6\}$. The $p_X(-1)$ is given by the probability that the die never shows the chosen number. We can view this as $6$ independent trials, where each trial is the result of a die roll. If $A_j$ is the event that the die shows the selected number, then we can calculate $P(A_1^C, A_2^C, A_3^C, A_4^C, A_5^C, A_6^C) = P(A_1^C)P(A_2^C)P(A_3^C)P(A_4^C)P(A_5^C)P(A_6^C)$. Note that each of these probabilities is equivalent, and equivalently equal to $\dfrac{5}{6}$, and so this probability is $\left(\dfrac{5}{6}\right)^6 \approx 0.335$. The remainder of the outcomes happen if the first time that the number shows up is on toss $j$. This means that we have $A_1, \dots, A_{j-1}^C$ and then $A_j$ occurring. As a result, for $j=1,\dots,6$ we get $P(A_1^C, \dots, A_{j-1}^C, A_j) = P(A_1^C)^{j-1}P(A_j)$. Note that $P(A_j) = \dfrac{1}{6}$, no matter $j$, and so the total probability here is $\left(\dfrac{5}{6}\right)^{j-1}\left(\dfrac{1}{6}\right)$. This fully defines the probability mass function.

To get the $E[X]$ we can apply the standard expectation formula. To get the variance we can note that $\text{var}(X) = E[X^2] - E[X]^2$, and so while we compute $E[X]$ we can also compute $E[X^2]$. To this end $$E[X] = (-1)\left(\frac{5}{5}\right)^6 + \frac{1}{6}\left[1 + 2\left(\frac{5}{6}\right) + 3\left(\frac{5}{6}\right)^{2} + 4\left(\frac{5}{6}\right)^{3} + 5\left(\frac{5}{6}\right)^{4}+ 6\left(\frac{5}{6}\right)^{5}\right].$$ Solving this gives approximately $\$0.98$, so every play of the game you would expect to earn $98$ cents, approximately. To find $E[X^2]$, we use essentially the same setup, this time squaring the values, $$E[X] = (-1)^2\left(\frac{5}{5}\right)^6 + \frac{1}{6}\left[1^2 + 2^2\left(\frac{5}{6}\right) + 3^2\left(\frac{5}{6}\right)^{2} + 4^2\left(\frac{5}{6}\right)^{3} + 5^2\left(\frac{5}{6}\right)^{4}+ 6^2\left(\frac{5}{6}\right)^{5}\right].$$ This is $8.728$, approximately. Then the variance will be given by $$\text{var}(X) = E[X^2] - E[X]^2 \approx 8.728 - (0.98)^2 = 7.7676.$$
::::
:::

### The Variance of Transformations
With expectations, we saw that $E[g(X)]$ needed to be directly computed from the definition. The same is true for variances of transformations. Specifically, $\text{var}(g(X))$ is given by $E[(g(X) - E[g(X)])^2]$ which can be simplified with the previous relationship as $E[g(X)^2] - E[g(X)]^2$. Just as with expectations, it is important to realize that $\text{var}(g(X)) \neq g(\text{var}(X))$, and so dealing with transformations requires further work.

With expectations, we highlighted linear transformations as a special case, with $g(X) = aX + b$. For the variance, the linear transformations are also worth distinguishing from others. In particular, $$\text{var}(aX + b) = a^2\text{var}(X).$$ In the same way that the linearity of expectation demonstrates that the expected value of any constant is that constant, we can use this identity to show that the variance of constant is zero. However, we can also reason to this based on our definitions so far. Suppose that we have a random variable which is constant.^[This seems to be an oxymoron, but it is perfectly well defined.] A constant $b$ can be seen as a random variable with probability distribution $p_X(x) = 1$ if $x=b$ and $p_X(x) = 0$ otherwise. The expected value is going to be $E[X] = 1(b) = b$, and $E[X^2] = 1(b)^2 = b^2$. Thus, $\text{var}(b) = E[X^2] - E[X]^2 = b^2 - b^2 = 0$. From an intuitive perspective, there is no variation around the mean of a constant. It is always the same value. As a result, when taking the variance, we know that it should be $0$. 

:::{.callout-warning icon="false" collapse="true"}
## Proof of the Variance of Linear Transformations
To calculate the variance of a linear transformation of a random variable we can apply the standard identity for the variance, giving \begin{align*}
E[(aX+b)^2] &= E[a^2X^2 + 2abX + b^2] \\
&= E[a^2X^2] + E[2abX] + E[b^2]\\
&= a^2E[X^2] + 2abE[X] + b^2.
\end{align*} Next, we note that $E[aX + b] = aE[X] + b$ and so \begin{align*}
E[aX + b]^2 &= (aE[X] + b)^2 \\
&= a^2E[X]^2 + 2abE[X] + b^2.\end{align*} Differencing these two quantities gives $$a^2E[X^2] + 2abE[X] + b^2 - a^2E[X]^2 - 2abE[X] - b^2 = a^2(E[X^2] - E[X]^2).$$ By noting that $E[X^2] - E[X]^2$, we can complete the statement that $$\text{var}(aX + b) = a^2\text{var}(X).$$
:::

Thus, when applying a linear transformation, only the multiplicative constant matters, and it transforms the variance by a squared factor. This should make some intuitive sense that the additive constant does not change anything. If we consider that variance is a measure of spread, adding a constant value to our random quantity will not make it more or less spread out, it will simply shift where the spread is located. This is not true of the mean, which measures where the center of the distribution is, which helps explain why the result identities are different.

:::{#exm-var-linear-transformation}
## Sadie's Trip to America
When Sadie had recently returned from a long trip to America, Charles and Sadie worked out how to convert the expected values of temperatures from Celsius to Fahrenheit. They now wish to do the same, with the variances. The distribution of daily temperatures, in Celsius, is $$p_X(x) = \begin{cases}
    0.1 & x \in \{10, 11, 12, 13, 14\} \\
    0.05 & x \in \{15, 16, 17, 18, 19, 20, 21, 22, 23, 24\} \\
    0 & \text{otherwise}
\end{cases}$$

a. What is the variance of temperatures, in Celsius?
b. Supposing that the temperature in Fahrenheit is given by $Y = 1.8X + 32$, what is the variance of temperatures in Fahrenheit?

::::{.callout .solution collapse='true'}
## Solution
a. Here we find \begin{align*}
    E[X] = \sum_{x=10}^{24} xp_X(x) &= 0.1\sum_{x=10}^{14}x + 0.05\sum_{x=15}^{24}x \\
    &= 15.75. \\
    E[X^2] = \sum_{x=10}^{24} x^2p_X(x) &= 0.1\sum_{x=10}^{14}x^2 + 0.05\sum_{x=15}^{24}x^2 \\
    &= 267.25 \\
    \text{var}(X) &= 267.25 - 15.75^2 \\
    &= 19.1875.
\end{align*}

b. Since $\text{var}(X) = 19.1875$, then we know that $\text{var}(Y) = \text{var}(1.8X + 32) = 1.8^2\text{var}(X) = 62.1675$.
::::
:::

Unlike the expectation, the variance of additive terms will not generally be the addition of the variances themselves. That is, we cannot say that $\text{var}(g(X) + h(X)) = \text{var}(g(X)) + \text{var}(h(X))$. Writing out the definition shows issue with this, $$E[(g(X) + h(X))^2] = E[g(X)^2] + 2E[g(X)h(X)] + E[h(X)^2].$$ The first and third terms here are nicely separated and behave well. However, the central term is not generally easy to simplify. You can view $g(X)h(X)$ as a function itself, and so $$E[g(X)h(X)] \neq E[g(X)]E[h(X)].$$ Instead, this will typically need to be worked out for any specific set of functions.

## Exercises {.unnumbered}

:::{#exr-6.1}
Find the variance and standard deviation of the sum obtained in tossing a pair of standard dice.
:::

:::{#exr-6.2}
In a lottery there are $200$ prizes of $\$5$, $20$ prizes of $\$25$, and $5$ prizes of $\$100$. Assuming that $10,000$ lottery tickets are to be issued and sold, what is the fair prices to pay for a ticket?
:::

:::{#exr-6.3}
Suppose that $X$ is a random variable with mean $9.5$ and variance $0.16$. For each of the following, identify whether you can determine the mean and variance of the listed quantity, and if so, find it.

a. $3X$.
a. $2X + 4$.
a. $X^2$.
a. $\frac{X^3}{X^2}$.
:::

:::{#exr-6.4}
Consider the following pmf. 
$$
p(x) = \begin{cases}
        \frac{1}{5} & X \in \{0, 1, 2\} \\
        \frac{1}{10} & X \in \{3, 4\} \\
        \frac{1}{15} & X \in \{5, 6, 7\} \\
        0 & \text{otherwise}.
    \end{cases}
$$ 
    
Calculate: 
    
a. $E[X]$.
a. $\text{var}(X)$.
a. $\zeta(0.5)$.
a. $\text{Range}(X)$.
a. $\text{MAD}(X)$.
a. $E[X^3]$.
a. $\text{var}(X^3)$.
a. $E[2X + 3X^2]$.
a. $\text{var}(e^X)$.
:::

:::{#exr-6.5}
Peter and Paula play a game of chance that consists of several rounds. Each individual round is won, with equal probabilities of $1/2$, by either Peter or Paula; the winner then receives one point. Successive rounds are independent. Each has staked \$50 for a total of \$100, and they agree that the game ends as soon as one of them has won a total of 5 points; this player then receives the \$100. After they have completed four rounds, of which Peter has won three and Paula only one, a fire breaks out so that they cannot continue their game. 

a. How should the \$100 be divided between Peter and Paula? 
b. How should the \$100 be divided in the general case, when Peter needs to win $a$ more rounds and Paula needs to win $b$ more rounds?
:::

:::{#exr-6.6}
Suppose that $X$ is a random variable with mean $\mu$ and variance $\sigma^2$. Prove that $$Z=\frac{X-\mu}{\sigma},$$ is a random variable with mean $0$ and variance $1$.
:::

:::{#exr-6.7}
Describe several measures of location indicating the strengths and drawbacks of each.
:::

:::{#exr-6.8}
Describe several measures of variability indicating the strengths and drawbacks of each. 
:::

:::{#exr-6.9}
a. Suppose that two random quantities have the same mode, median, and expected value. Is their distribution guaranteed to be the same? Is it guaranteed to be similar? Why?
b. Suppose that two random quantities have the same range, IQR, variance, and mean absolute deviation. Is their distribution guaranteed to be the same? Is it guaranteed to be similar? Why?
c. Suppose that two random quantities have the same percentiles for all values for which the percentiles are defined. Is their distribution guaranteed to be teh same? Is it guaranteed to be similar? Why?
:::

:::{#exr-6.10}
Is it possible for a random variable, which only takes on positive values, to have a standard deviation which is larger than its mean? Explain.
:::

:::{#exr-6.11}
Is it possible to, knowing that $\text{var}(X) > \text{var}(Y)$, conclude anything regarding $\text{MAD}(X)$ and $\text{MAD}(Y)$? Explain.
:::

:::{#exr-6.12}
Consider a bag containing three red marbles, two green marbles, and four blue marbles. You randomly draw two marbles without replacement. Let $Y$ represent the number of green marbles drawn.

The probability mass function is given by,
$$
p_Y(y) = \begin{cases} 
\frac{6}{36} & y = 0 \\
\frac{12}{36} & y = 1 \\
\frac{3}{36} & y = 2 \\
0 & \text{otherwise}
\end{cases}
$$

a. What is the expected value of $X$?
b. What is the mode of $X$?
c. What is the variance of $X$?
d. What is the range of $X$?
e. What is the standard deviation of $X$?
:::

:::{#exr-6.13}
Suppose you toss a biased coin three times. Let $Z$ represent the number of heads obtained.

The probability mass function is given by,
$$
p_Z(z) = \begin{cases} 
\frac{1}{8} & z = 0 \\
\frac{3}{8} & z = 1 \\
\frac{3}{8} & z = 2 \\
\frac{1}{8} & z = 3 \\
0 & \text{otherwise}
\end{cases}
$$

a. What is the expected value of $X$?
b. What is the median of $X$?
c. What is the variance of $X$?
d. What is the standard deviation of $X$?
:::

:::{#exr-6.14}
Suppose you observe the number of cars passing a traffic light in a minute. Let $W$ represent the number of cars observed.

The probability mass function is given by,
$$
p_W(w) = \begin{cases} 
\frac{1}{10} & w = 0 \\
\frac{2}{10} & w = 1 \\
\frac{3}{10} & w = 2 \\
\frac{2}{10} & w = 3 \\
\frac{1}{10} & w = 4 \\
0 & \text{otherwise}
\end{cases}
$$

a. What is the expected value of $X$?
b. What is the mode of $X$?
c. What is the variance of $X$?
d. What is $\zeta(0.3)$? 
e. What is $\zeta(0.8)$?
e. What is the standard deviation of $X$?
:::

:::{#exr-6.15}
Consider a book containing 200 pages. You randomly select a page number. Let $V$ represent the sum of the digits in the page number selected.

a. What is the expected value of $X$?
c. What is the variance of $X$?
d. What is the range of $X$?
e. What is the standard deviation of $X$?
:::

