# The Analysis of Categorical Data

## Contingency Tables, Frequency Distributions, and Categorical Data
The concept of contingency tables (@def-contingency-table) was introduced in @sec-contingency-tables. Briefly, contingency tables succinctly summarize the breakdown of two (or more) traits in a population. The numbers of individual items in the population that satisfy the intersecting traits are recorded in a table, with each member of the population counted at some specific intersection. Contingency tables provide an effective tool for considering probability distributions over finite populations. This is particularly true when the traits being considered are categorical. 

Categorical data provide an interesting challenge for statistical inference. If collected data happen to be categorical, it may not be immediately clear how to analyze them. With quantitative measurements we can readily compute estimates that map onto specific parameters in the population distribution. The parallels with categorical data are less direct.^[There is no way, for instance, to consider the *average* favourite colour for a group of school-aged children, or to consider the median political party that a region supports. There may be techniques that can measure related quantities numerically, however, these are not typically numerical quantities.] In order to conduct inference using categorical data, we require a technique for systematically converting qualitative information into quantitative information. While it is conceivable to determine several strategies for this purpose, one highly effective tool is the use of frequency distributions (@def-frequency-distribution). 

Notably, a contingency table can be seen as a method of encoding the joint frequency distribution of two (or more) categorical factors in a sample. By simply recording the number of individuals who possess particular combinations of traits, we are able to have a succinct breakdown of the composition of the sample. This summarizes the categorical features of the sample, but it does so quantitatively. Using the frequency distribution it is possible to ask whether different traits occur with similar frequencies in the sample.^[For instance, do children report *red* and *blue* as being their favourite colour with equal proportions?] Alternatively, frequency distributions can begin to give insight into whether different traits are related to, or independent of, one another.^[For instance, is a child's favourite colour related to their favourite animal, or are these two characteristics independent?] 

Thus, (joint) frequency distributions, often expressed via contingency tables for samples, provide a useful mechanism for summarizing categorical data, and can in turn be used to conduct statistical inference. The primary technique for doing so is to consider what was expected to be seen based on an underlying null hypothesis, and then assess how different what was actually observed happened to be. In a sense we are comparing the observed contingency table to an expected contingency table, and the similarity (or difference) dictates the likelihood that our hypothesized model can adequately explain the data. 

## Expected Cell Counts for Contingency Tables
When dealing with random variables (@def-random-variable), the concept of an expected value was explored. The idea was that we can answer the question "What value do we expect to observe, if this random variable were to be realized?" on the basis of the underlying probability distribution. We can ask, and answer, a similar type of question in terms of contingency tables. Suppose, for instance, that we roll a die that has six different colours printed on each of its sides. If we roll the die $60$ times it is reasonable to ask ourselves "how many times do we expect to see each colour?". If the die is fair then we should expect that no colour turns up any more often than any other, and as a result, our expectation would be to observe $10$ rolls for each colour. Now, suppose further that we did run this experiment. By recording the colours of the $60$ rolls we could compare our observations to the expected observations, and the similarity of these two frequency distributions will give some evidence whether the die is actually fair or not. There is nothing particularly special about assuming the die is fair to determine the expected counts. 

More generally, if we pose any particular model underpinning reality, we can determine what we would expect to observe in a frequency distribution or contingency table. This may be achieved by imposing a particular distribution on the underlying population or through assumptions about the traits of the population that impact their distribution. Critically, we require the ability to translate these types of statements into expected values for the contingency table and frequency distribution. While any set of assumptions that generate expected values can be used, there are three common overarching types of assumptions that are worth considering directly in more depth. Namely, these are (i) the fit of a specific distribution, (ii) homogeneity of multiple populations, or (iii) independence of two characteristics. 

### Expected Values for Goodness-of-Fit
It is common to ask whether a particular distribution is a good fit for a particular set of data. This may be through named distributions^[Such as the Poisson, generating counts.] or it may be through explicitly specified distributions. In either case, these distributions give rise to expected frequency distributions. The idea, in brief, is to leverage the specific distribution to theoretically determine the expected number of observations in each cell, based on the overarching total. This was the process used by assuming that the die was fair. This assumption amounted to an assumption that the population follows a discrete uniform distribution (@sec-discrete-uniform), and could be used to determine the expected counts.

Instead of assuming that the distribution was a discrete uniform, perhaps we had an explicitly specified distribution. Suppose that we wondered whether the die was biased such that it was $5$ times as likely to come up as the $6$th colour, compared to all other options. Then, taking $p_i$ to be the probability that the $i$th colour is rolled, we can determine that $p_1 = p_2 = p_3 = p_4 = p_5 = 0.1$, while $p_6 = 0.5$. Then, if we still rolled the coin $60$ times we should expect to see $E_1 = E_2 = E_3 = E_4 = E_5 = 0.1(60) = 6$, while $E_6 = 0.5(60) = 30$. Any other explicit assumptions about the relative proportions of the different categories would result in a corresponding set of expected frequencies.^[While this is described as being a distribution that is explicitly characterized, rather than deriving from a named distribution, the underlying distribution is in fact named. Notably, the distribution follows a *multinomial* distribution, where the multinomial extends the concept of the binomial to multiple different options. This is characterized by a total number of observations ($60$ in the example), as well as a set of probabilities of observing each class ($p_1,\dots,p_6$).]

In general, by assuming an underlying distribution, we can determine the theoretical probability of a particular observation being observed in a particular category. Then, with this probability, say $p$, the expected count in a cell is given by $np$, where $n$ is the total number of observations in the frequency distribution or contingency table.

:::{.callout-tip icon="false"}
## The Expected Value for Goodness-of-Fit Testing

Suppose that a distribution is hypothesized regarding the frequency distribution for a particular trait. Namely, suppose that the distribution gives rise to a set of probabilities, $\{p_1,p_2,\dots,p_k\}$, such that each option for the trait $j=1,\dots,k$ will have probability $p_j$ of being observed. Moreover, suppose that a total of $n$ observations will be taken. Then, the expected value of the $j$th cell is $E_j = np_j$.

If the cell probabilities are not given directly, such as through a probability mass function or similar, they can often be derived via the use of a cumulative distribution function. Notably, if the categories are bins of numeric quantities, then, for a given continuous distribution with cumulative distribution function $F$, $$E_j = n(F(u_j) - F(\ell_j)),$$ where $u_j$ is the upper bound on the $j$th bin, and $\ell_j$ is the corresponding lower bound.
:::

### Expected Values for Homogeneity
If we wish to describe two quantities as being alike, we frequently describe them as being *homogenous*. If some trait is homogenous between two populations, then that trait is equally distributed between those different populations. We may wish to understand, for instance, whether genders are represented homogeneously between various careers. Note that this is not to say that all careers are equally likely to be pursued, but rather, that the rate that individuals pursue various careers is the same across gender. 

We are able to summarize the frequency distribution of a single trait^[Such as one's career.] across multiple populations^[Say, defined by individuals' gender.] using a contingency table. We take the characteristic that defines the populations as one of the factors (say the rows), and then the trait of interest as the other factor (say the columns). This summarizes data in a manner that is identical to taking a sample from a single population, measuring two different traits, but it is conceived of differently. Notably, we view this as multiple samples of a single trait, from multiple distinct populations. Denote the number of observations from population $i$, who have trait $j$ as $n_{i,j}$. Each population will have a set number of realizations drawn from it, denoted $n_{i, \bullet}$.^[The notation of a subscript $\bullet$ is used to suggest that you sum over that index. So, $n_{i,\bullet} = \sum_{j} n_{i,j}$.] We can use a similar notation to denote the number of observations with trait $j$, taking $n_{\bullet, j} = \sum_{i=1}^k n_{i, j}$.

|              |  Population 1   |  Population 2   | $\cdots$ | Population $k$  |           **Total** |
| :----------- | :-------------: | :-------------: | :------: | :-------------: | ------------------: |
| Trait $1$    |    $n_{1,1}$    |    $n_{2,1}$    | $\cdots$ |    $n_{k,1}$    |    $n_{\bullet, 1}$ |
| Trait $2$    |    $n_{1,2}$    |    $n_{2,2}$    | $\cdots$ |    $n_{k,2}$    |    $n_{\bullet, 2}$ |
| $\vdots$     |    $\vdots$     |    $\vdots$     | $\ddots$ |    $\vdots$     |            $\vdots$ |
| Trait $\ell$ |  $n_{1,\ell}$   |  $n_{2,\ell}$   | $\cdots$ |  $n_{k,\ell}$   | $n_{\bullet, \ell}$ |
|              | $n_{1,\bullet}$ | $n_{2,\bullet}$ | $\cdots$ | $n_{k,\bullet}$ |                 $n$ |

: The notation used for a contingency table based on $k$ distinct populations, with each of which categorizes individuals based on a single trait, with $\ell$ possible values. The notation $n_{i,j}$ gives the total number of individuals in population $i$ with trait $j$. Subscripts of $\bullet$ are used to sum over an index, so $n_{i,\bullet}$ is the total sample size from population $i$, and $n_{\bullet,j}$ gives the total number of individuals with trait $j$. {#tbl-example-contingency-table-population}

Thus, the total number of realizations will be $n = n_{1, \bullet} + n_{2, \bullet} + \cdots + n_{k, \bullet} = n_{\bullet, 1} + n_{\bullet, 2} + \cdots + n_{\bullet, \ell}$. In order for the populations to be homogenous, we would expect that the proportion of observations for any trait, say $\dfrac{n_{i,j}}{n_{i,\bullet}}$ should be equivalent across the various populations. Note that, across the entire sample, the proportion of individuals with trait $j$ is $\dfrac{n_{\bullet, j}}{n}$. Thus, if we take $$p_j = \dfrac{n_{\bullet, j}}{n},$$ under a homogenous model for the populations we should observe that $n_{i, j} = n_{i,\bullet}p_j = \dfrac{n_{i,\bullet}n_{\bullet,j}}{n}$.^[Notice that, if this were actually true, then for two populations, say $i$ and $k$, we would have $$\dfrac{n_{i,j}}{n_{i,\bullet}} = \frac{n_{\bullet,j}}{n} = \frac{n_{k,j}}{n_{k,\bullet}},$$ exactly as desired.]

Formally, we estimate the overall proportion of individuals in the sample with each trait, combining them across the different populations. We then suggest that, if the populations are truly homogenous with respect to the given trait, we would expect to see the proportion of individuals with the given trait being equivalent across each population. This gives rise to the homogenous expected frequency. 

:::{.callout-tip icon="false"}
## The Expected Value for Homogeneity Testing

Suppose that we are considering $k$ distinct populations, and hypothesize that these $k$ populations will be homogenous with respect to some trait that has $\ell$ distinct options. In this case, for the $i$th population and $j$th value of the trait, we should expect to see $$E{i,j} = \frac{n_{i,\bullet}n_{\bullet,j}}{n}.$$ That is, the expectation is the total size of the $i$th population, multiplied by the total count of individuals with the $j$th trait, divided by the total number of individuals. 
:::

### Expected Values for Independence
Suppose that, rather than drawing from $k$ distinct populations, we take a sample from a single population but measure two different traits.^[For instance, we may take a random sample of individuals and ask them for their gender and their job.] In this framing it is natural to inquire whether the traits measured in the population are independent of one another. Recall that independence (@def-independence) arises whenever one event (or random variable) provides no information regarding another event (or random variable). Mathematically, events $A$ and $B$ are independent if $P(A,B) = P(A)P(B)$, and random variables $X$ and $Y$ are independent if $p_{X,Y}(x,y) = p_X(x)p_Y(y)$.^[If the random variables are continuous, the same breakdown applies with density functions rather than probability mass functions, giving $f_{X,Y}(x,y) = f_X(x)f_Y(y)$.]

Thus, suppose that a two-way contingency table exists, using the same notation as @tbl-example-contingency-table-population, however, this time representing draws from a single population with two different traits measured. Thus, $n_{i,j}$ is the count of the number of individuals who have option $i$ for the first trait $1$ and option $j$ for the second trait. Then, consider the event $A$ to be that a randomly selected individual has $i$ as their first trait, and the event $B$ to be that a randomly selected individual has $j$ as their second trait. Thus, $$P(A) = \frac{n_{i,\bullet}}{n}, \quad P(B) = \frac{n_{\bullet,j}}{n},\quad\text{and}\quad P(A,B) = \frac{n_{i,j}}{n}.$$ If $A$ and $B$ are independent then, this would require that $$\frac{n_{i,j}}{n} = \frac{n_{i,\bullet}}{n}\times\frac{n_{\bullet,j}}{n} \implies n_{i,j} = \frac{n_{i,\bullet}n_{\bullet,j}}{n}.$$

The same logic applies to any combination of traits. As a result, if we hypothesize that two traits in a population are independent, this gives us a means of determining how many observations we should expect to see, in each cell. 

:::{.callout-tip icon="false"}
## The Expected Value for Independence Testing

Suppose that we are considering a single population, with two traits, the first of which has $k$ options and the second with $\ell$ options. Further, suppose we hypothesize that, in this population, these traits are independent of one another. In this case, for the $i$th option of trait one and $j$th value of trait two, we should expect to see $$E{i,j} = \frac{n_{i,\bullet}n_{\bullet,j}}{n}.$$ That is, the expectation is the total number of individuals with $i$ as the first trait, multiplied by the total count of individuals with the $j$th second trait, divided by the total number of individuals. 
:::

Note that the expected value for testing independence and the expected value for testing homogeneity are mathematically equivalent to one another. In a sense, the assumption of homogeneity is equivalent to the assumption of independence, and vice versa. The distinction, however, arises based on what the data represent and how the data arise. When we are considering independence, we are envisioning the sample as arising from one overarching population, and we take a random sample from this. When we are considering homogeneity, we are considering $k$ distinct populations, with random samples from each of these. Practically, this means that in homogeneity testing, we have $n_{i,\bullet}$ fixed for all $i$, while in independence testing, $n_{i,\bullet}$ is a random quantity that is only observed once the sample is realized. We should not expect to see the exact same contingency tables if we run two separate experiments, one randomly sampling from a combined population, and the other sampling from each population separately.^[The reason for this stems from the fact that each population may be different sizes. If you have one population that is much larger than another, in the overall simple random sample, you should expect far more realizations from this population. In the separate population framing, this would only hold if you choose to select fewer individuals from the smaller populations. The difference is analogous to the difference between simple random sampling and stratified random sampling.]

## Chi-Squared Tests

Categorical data are well-summarized using a frequency distribution.^[Or, equivalently, a contingency table] Concrete assumptions regarding the underlying distribution of any categorical traits give rise to expected frequency distributions. The comparison of these observed and expected tables give a natural method for testing the underlying assumptions. Specifically, the observed and expected frequency distributions naturally give rise to Pearson's chi-squared test statistic. This test statistic, under some minimal assumptions, will approximately follow a chi-squared distribution, supposing that the hypothesis used to generate the expected distribution is correct. Thus, this setup can be used to test assumptions regarding the underlying distribution of the categorical data.

### The Chi-squared Test Statistic
To express the test statistic, suppose that we make observations $O_{i,j}$ across the contingency table, for $i=1,\dots,k$ and $j=1,\dots,\ell$. Moreover, suppose that, under the assumptions that are being tested, the corresponding expected cell counts are found to be $E_{i,j}$. Then the chi-squared test statistic is given by $$\chi^2 = \sum_{i=1}^{k}\sum_{j=1}^{\ell}\frac{(O_{i,j} - E_{i,j})^2}{E_{i,j}}.$$ Note that, if instead of a contingency table, a frequency distribution is used, this can be reframed as having observations $O_i$, for $i=1,\dots,k$, with corresponding expectations $E_i$. Then, in this setting, $$\chi^2 = \sum_{i=1}^k \frac{(O_i - E_i)^2}{E_i}.$$ 

In general, the test statistic is computed by finding $$\frac{(\text{Observed} - \text{Expected})^2}{\text{Expected}},$$ for each cell in the contingency table or frequency distribution, and then summing all of these values. This test statistic does not depend on the underlying hypothesis that is being tested, only on being able to determine the expected quantities for each cell in the table or distribution. 

### The Null Distribution
Under the null hypothesis, that the expected values are actually correct, the chi-squared test statistic will have an (approximate) chi-squared distribution. There are two important caveats on this point: 

1. This approximation will only hold under assumptions on the sampling procedure, sample size, and expected cell counts; and
2. The exact number of degrees of freedom for the chi-squared distribution will depend on the assumptions generating the expectations. 

There are three primary assumptions required to conduct a chi-squared test. First, we must assume that the sample arises from simple random sampling. That is, there is an overarching population (or multiple populations in the case of testing for homogeneity), from which the observed data are drawn, with equal probability of any member of the population being sampled. If sampling is not done via a simple random sample, alternatives to the chi-squared procedure are required. Next, we require that the expected cell counts are all sufficiently large. As a general rule of thumb, all the expected cell counts should be above $5.$^[As with all *rules of thumb*, this is a rough rule that indicates where these tests may be appropriate. Some sources may indicate that you should have above $10$ for some expected values. Others indicate that, if you have many cells under consideration, it suffices to have $80\%$ of the expected cells above $5$, so long as none are zero. As with all size requirements, the truth depends on the underlying reality, however, the larger the expected counts, the better the approximation will work.] Note that this assumption puts an implicit requirement on the sample size. Namely, if your sample size is too small, the expected counts will not be large enough to adequately perform the hypothesis test. The final assumption that is required is that the observations in the sample are independent of one another. We need to make independent draws from the underlying population, rather than allowing for dependence between different observations.^[There are two important points on this. First, under a true simple random sample, independence will typically be a reasonable assumption. Second, the required independence is distinct from the type of independence we may test for using the chi-squared test. Independence of observations means that two subsequent realizations should not ever depend on one another. The independence that we may test for, conversely, refers to independence of traits within a population (that is, traits on the same individual are independent of one another).]

If the assumptions are all satisfied, then $\chi^2 \stackrel{H_0}{\sim} \chi^2_{d}$, where $d$ represents the number of degrees of freedom for the distribution. Broadly speaking, the degrees of freedom will be given by $k\ell - c$, where $k$ and $\ell$ are the total number of options for traits one and two, and $c$ is the number of *constraints* placed on the table. Note that, $k\ell$ gives the total number of cells in the contingency table. Moreover, if the test is being run on a frequency distribution rather than a contingency table, we can take $\ell = 1$. 

The number of constraints, $c$, arise while calculating the expected frequencies based on the underlying hypotheses. The question we are asking ourselves is "how many cell values are pre-determined, based on the assumptions?". Take, for instance, the simple goodness-of-fit assumptions. In this case we have that $E_i = np_i$, for $i=1,\dots,k$. Note, however, that in order to calculate $E_i$ we are assuming that the total number of observations is going to be $n$. By assumption, we know that $$E_1 + E_2 + \cdots + E_k = n.$$ As a result, once we have determined $\{E_1, E_2, \dots, E_{k-1}\}$, the final value, $E_k$ must also be determined as $$E_k = n - E_1 - E_2 - \cdots - E_{k-1}.$$ Thus, while we calculate $k$ expected values, we have a constraint on exactly $1$ of these values, so $c=1$. This gives the familiar $d = k - 1$.

In the contingency table for independence we calculate a total of $k\ell$ expected values. However, we make the assumption that both $n_{i,\bullet}$ and $n_{\bullet,j}$ are fixed for each row and column. This gives $k + \ell$ constraints, however, it is the case that $$\sum_{i=1}^{k} n_{i,\bullet} = \sum_{j=1}^{\ell} n_{\bullet,j} = n.$$ Thus, once we have constrained the $k$ totals for the rows, and $\ell - 1$ totals for the columns, the final value for the remaining column has already been constrained as well. Thus, in this case, $c = k + \ell - 1$. This gives, $$d = k\ell - (k + \ell - 1) = (k - 1)(\ell - 1).$$

An alternative framing for counting the degrees of freedom is to ask ourselves: "how many expected values can we independently compute?". To answer this we note that we calculate one in each cell, minus any of those that are constrained, hence giving $d = k\ell - c$. However, in some situations it may be easier to directly count the free to vary expected values. Either way, the assumptions guarantee that the distribution will be approximately chi-squared, and the hypotheses give rise to the number of degrees of freedom.

### Performing a Chi-squared Test
In order to perform a chi-squared test we follow a five-step procedure:

1. Compute the expected cell counts based on the underlying hypothesis.
2. Compute the $\chi^2$ test statistic, giving a realization of $x$.
3. Determine the null distribution (degrees of freedom).
4. Calculate the $p$-value as $P(\chi^2 > x)$, or determine the relevant (upper-tail) critical value.
5. Draw and communicate conclusions.

The first three steps are addressed in the previous sections. Once the realization of the test statistic and the null distribution are known, the test proceeds^[As do all other tests in the NHST framework.] either via $p$-values or critical values. When attempting to determine the $p$-value, it is helpful to consider which tail contradicts the null hypothesis. Under the null hypothesis we should expect that $O_{i,j} \approx E_{i,j}$, for all $i$ and $j$. If this is the case then we have that $\chi^2$ will be small in magnitude (close to zero). If this is not the case, however, we will see that $\chi^2$ will be large in magnitude (far from zero), and *always* positive.^[This arises since the differences between the observed and expected values are squared, so any difference, positive or negative, becomes a positive contribution to the test statistic.] As a result, it is only the **upper tail** that we are concerned with in terms of generating $p$-values or for finding critical values. 


```{r}
#| echo: false
#| label: fig-rejection-region-plot-chisq
#| fig-height: 3
#| fig-cap: The rejection region for a chi-squared hypothesis test. For chi-squared tests we do not differentiate between one- and two-tailed tests. Instead, if the alternative holds, we should expect to see large values of the test statistic. As a result, the $p$-value always corresponds to the right-tail probability. The $p$-value will, just as in other hypothesis tests, be compared to the level of significance.

library(ggplot2)
library(ggthemes)

# Define the value of t
t <- 20

# Create a sequence of x values for plotting the density curve
x <- seq(0, 30, length.out = 10000)

# Calculate the normal density function
y <- dchisq(x, df = 9)

# Create the ggplot
ggplot() +
  geom_area(data = data.frame(x = x[x >= t], y = y[x >= t], fill = as.factor(1)),
            aes(x = x, y = y), fill = "#F8766D") +
  geom_line(aes(x = x, y = y), linewidth = 1.1, color = "#00BFC4") +
  annotate(geom = "text", x = t-0.75, y = 0.05, label = "Observed Test Statistic", color = "#F8766D", angle = 90) + 
  annotate(geom = "text", x = 23, y = 0.03, label = "p-value", color = "#F8766D") + 
  annotate(geom = "text", x = qchisq(p=0.05, df=9, lower.tail=FALSE)-0.75, y = 0.05, label = "Critical Value", angle = 90) + 
  geom_vline(xintercept = qchisq(p=0.05, df=9, lower.tail=FALSE), linetype = 2) +
  geom_vline(xintercept = t, color = "#F8766D") + 
  theme_clean() + 
  labs(x = expression(chi^{2}), y = "") +
  scale_y_continuous(expand = c(0, 0), limits = c(0, 0.11)) + 
  theme(plot.background = element_blank(), legend.position="none") + 
  annotate("segment", x = qchisq(p=0.05, df=9, lower.tail=FALSE), y = 0.105, xend = 30, yend = 0.105, linewidth = 1.5, arrow = arrow(type = "closed", length = unit(0.04, "npc"))) + 
  annotate("text", x = 20, y = 0.095, label = "Rejection Region") + 
  annotate("segment",  x = 23, y = 0.027, xend = t+0.5, yend = dchisq(t+0.5, df = 9), arrow = arrow(type = "closed", length = unit(0.04, "npc")), color = "#F8766D")
```

Formally, taking $F$ to represent the cumulative distribution function of a $\chi^2_d$ random variable, then given $\chi^2 = x$, the $p$-value for the hypothesis test is $p = 1 - F(x)$. This can be compared to the level of significance, $\alpha$. Alternatively, the critical value can be determined as $\chi^2_{d, 1-\alpha}$. Then, if $x > \chi^2_{d,1-\alpha}$, the null hypothesis is rejected at the $\alpha$ level of significance. Conclusions are then drawn in the same manner as other hypothesis tests: if the null hypothesis is rejected at the given level of significance, there is sufficient evidence in the data to take the alternative as being a better description of reality than the null. The level of significance is still interpretable as the probability of making a Type I error, and it should still be balanced with the power of the hypothesis test, depending on the contextual requirements.

### The Three Common Chi-squared Tests
While it is conceptually possible to test any hypotheses that generate expected frequency distributions, it is most common to test one of the three aforementioned hypotheses: (i) the goodness-of-fit of a distribution, (ii) the homogeneity of two or more populations, or (iii) the independence of two characteristics. Respectively, these tests are often referred to as the chi-squared goodness-of-fit test, the chi-squared test for homogeneity, and the chi-squared test for independence. Each test follows the general framework for a chi-squared test: the expected values are determined, $\chi^2$ is calculated, the degrees of freedom are determined, and a $p$-value is derived. Because of the frequency with which these tests are used, it is useful to summarize each procedure as a concrete example of the relevant hypothesis test. 

:::{.callout-tip icon="false"}
## The Chi-squared Goodness-of-Fit Test

Suppose that observations are made for $k$ categories, based on a simple random sample, giving $O_1, \dots, O_k$. In order to test whether these are drawn from a particular distribution or not, we test $$H_0: \text{The Distribution is a Good Fit} \quad H_A: \text{The Distribution Does Not Fit}.$$ The expected cell counts, $E_1, \dots, E_k$ are computed under the null hypothesis (namely, that the distribution fits). In particular, the expected quantities are computed as $$E_i = p_in,$$ where $p_i$ is the probability of observing class $i$ based on the assumed distribution.

Then, the $\chi^2$ statistic is computed, $$\chi^2 = \sum_{i=1}^k \frac{(O_i - E_i)^2}{E_i} \stackrel{H_0}{\sim} \chi^2_{k-1}.$$

Finally, supposing that $\chi^2 = x$, the $p$-value is calculated as $p = P(\chi^2 \geq x)$, where $\chi^2$ is a $\chi^2_{k-1}$ random variable.
:::

Because the expected values for testing for independence and homogeneity are equivalent, the test procedures for independence and homogeneity are also equivalent. Recall that the difference between the two test procedures stems from the difference in the data structure^[Testing for homogeneity involves multiple separate populations, whereas testing for independence involves a single population with multiple traits.] and in the method of data collection.^[Testing for homogeneity relies on multiple simple random samples from various populations, whereas, testing for independence relies on a single simple random sample.] Given that the differences are contextual, the test can be viewed as a single test procedure. 

:::{.callout-tip icon="false"}
## The Chi-squared Test for Homogeneity / Chi-Squared Test for Independence

Suppose that observations are made either from:

1. A simple random sample from $k$ distinct populations, producing observations of a trait with $\ell$ categories; or 
2. A simple random sample from a single population, with two characteristics measured, the first with $k$ distinct categories and the second with $\ell$ categories.

In both cases, record the observed data as $O_{i,j}$ for $i=1,\dots,k$ (indexing the population or first trait), and $j=1,\dots,\ell$ (indexing the second trait). In order to test either:

1. $H_0: \text{The Populations are Homogenous with Respect to the Trait}$ versus $H_A: \text{The Populations are Heterogeneous with Respect to the Trait}$; or 
2. $H_0: \text{The Traits are Independent in the Population}$ versus $H_A: \text{The Traits are Dependent in the Population}$,

the expected values for each cell can be computed as $$E_{i,j} = \frac{n_{i,\bullet}n_{\bullet,j}}{n}.$$ 

Then, the $\chi^2$ statistic is computed, $$\chi^2 = \sum_{i=1}^k\sum_{j=1}^{\ell} \frac{(O_{i,j} - E_{i,j})^2}{E_{i,j}} \stackrel{H_0}{\sim} \chi^2_{(k-1)(\ell-1)}.$$

Finally, supposing that $\chi^2 = x$, the $p$-value is calculated as $p = P(\chi^2 \geq x)$, where $\chi^2$ is a $\chi^2_{(k-1)(\ell-1)}$ random variable.
:::

## Exercises {.unnumbered}

:::{#exr-18.1}
A die is rolled $120$ times, and the observed frequencies are recorded in the following table.

|  1  |  2  |  3  |  4  |  5  |  6  |
| :-: | :-: | :-: | :-: | :-: | :-: |
| 18  | 25  | 22  | 15  | 20  | 20  |

a. What are the expected cell counts that arise to test whether the die is fair?
b. Test the hypothesis that the die is fair, at the $\alpha = 0.05$ level of significance.
c. Test the hypothesis that the die shows up $2$ with a probability of $0.2$, and is equally likely to show up as any other number, at the $\alpha = 0.05$ level of significance.
:::

:::{#exr-18.2}
In a batch of flowers that a florist receives the flowers are all either red or white. They expect that the quantity of red flowers is represented by a binomial distribution, with proportion $0.75$. In a sample of $200$ flowers, they receive $40$ white flowers.

Test whether the resulting order matches the expected distribution at the $\alpha = 0.1$ level of significance. 
:::

:::{#exr-18.3}

A marketing analyst believes that consumer preference is evenly distributed across four major soda brands. To test this belief, they sample $310$ consumers and find that $70$ prefer brand A, $85$ prefer brand B, $70$ prefer brand C, and $85$ prefer brand $D$. 

a. Test the analyst's belief, at the $\alpha = 0.01$ level of significance. 
b. Suppose that brand $A$ invests in a heavy marketing push, specifically aimed at attracting the attention of customers who historically have preferred brand $D$. They will consider the campaign a success if they manage to attract $5\%$ of customers from $D$ to prefer their soda instead, leaving the rest of the industry unchanged. They have started from the assumption that all four brands are equally preferred. They, collect a sample of $500$ individuals, and find $148$ prefer brand A, $137$ prefer brand B, $125$ prefer brand C, and $90$ prefer brand D. Test whether these data fit their desired distribution at the $\alpha = 0.01$ level of significance. 
:::

:::{#exr-18.4}

A geneticist expects offspring to have a phenotype ratio of $9:3:3:1$. That is to say, there are 4 phenotypes in the population, and the geneticist expects that there will be $9$ times as many offspring with the first phenotype (compared to the fourth), and $3$ times as many offspring with the second and third phenotypes (compared with the fourth). 

In a sample of $160$ offspring, the observed frequencies are $95$, $35$, $20$, and $10$, for the four phenotypes, respectively. 

Do the data conform to the geneticist's expectations, at an $\alpha = 0.05$ level of significance? 
:::

:::{#exr-18.5}
A company claims that their candies are produced in equal proportions of five colors. 

a. Suppose that a bag of candies contains $20$ candies in total. Can the companies claim be tested in this setting?
b. Suppose that $5$ bags of candies are put together, resulting in $25$ red, $20$ blue, $18$ green, $17$ yellow, and $20$ brown candies. Is there evidence to suggest the company's claim is false, at an $\alpha = 0.05$ level of significance? 
:::

:::{#exr-18.6}
Researchers wish to compare the effectiveness of two different teaching methods. They consider two separate classes, applying each method to one of the classes, and recording the results (pass/fail) for the students. 

| Method   | Pass | Fail | Total |
| -------- | ---- | ---- | ----- |
| Method A | 30   | 10   | 40    |
| Method B | 25   | 15   | 40    |
| Total    | 55   | 25   | 80    |

a. Supposing that the methods are equally effective, what are the expected cell counts?
b. Test if the effectiveness of the two teaching methods is the same, at an $\alpha = 0.01$.
c. Explain if there are any concerns regarding the use of a chi-squared test, in this setting.

:::

:::{#exr-18.7}
A survey seeks to determine whether an individual's sex impacts their preferred type of movie (comedy, drama, action). The researchers take a random sample of $100$ women, and a random sample of $150$ men, soliciting their movie preferences. 

| Movie Preference | Women | Men | Total |
| ---------------- | ----- | --- | ----- |
| Comedy           | 30    | 60  | 90    |
| Drama            | 40    | 50  | 90    |
| Action           | 30    | 40  | 70    |
| Total            | 100   | 150 | 250   |

a. Is there a difference in movie preference (at $\alpha = 0.01$) between men and women?
b. Describe how this test would have changed if the researchers instead took a random sample of individuals, and asked them for their sex and movie preference.
:::

:::{#exr-18.8}
Three different factories produce the same product. Samples from each factory are inspected for defects. The following table shows the results of these samples.

| Factory   | Defective | Non-Defective | Total |
| --------- | --------- | ------------- | ----- |
| Factory A | 5         | 95            | 100   |
| Factory B | 15        | 185           | 200   |
| Factory C | 30        | 170           | 200   |
| Total     | 50        | 450           | 500   |

a. Is the defect rate the same across all three factories, at the $\alpha = 0.02$ level of significance?
b. Suppose that an additional $100$ observations are obtained from factor $A$, with $15$ defective and $85$ non-defective products. Do these additional observations change the conclusion in part (a)?
:::

:::{#exr-18.9}
A researcher wants to know if the distribution of American political affiliation (Democrat, Republican, Independent) is the same across three different age groups. Data are collected from each group, and reported in the table below. 

| Age Group | Democrat | Republican | Independent | Total |
| --------- | -------- | ---------- | ----------- | ----- |
| 18-24     | 150      | 50         | 25          | 225   |
| 25-34     | 120      | 75         | 30          | 225   |
| 35-54     | 80       | 110        | 40          | 230   |
| 55+       | 50       | 150        | 50          | 250   |
| Total     | 400      | 385        | 145         | 930   |

Perform the appropriate test, at the $\alpha = 0.05$ level of significance. 
:::

:::{#exr-18.10}
Four different brands of tires are tested for tread life. The following table reports the observed wear for these tires. 

| Tire Brand | Low | Medium | High | Total |
| ---------- | --- | ------ | ---- | ----- |
| A          | 0   | 4      | 0    | 4     |
| B          | 0   | 0      | 4    | 4     |
| C          | 4   | 0      | 0    | 4     |
| D          | 0   | 0      | 4    | 4     |
| Total      | 4   | 4      | 8    | 16    |

a. Can these data be used to test whether the brand has a significant impact on the observed wear? Explain.
b. A second study is run, producing the following data. Use these data to test whether tire brands have an impact on the wear patterns, at the $\alpha = 0.10$ level of significance.

| Tire Brand | Low | Medium | High | Total |
| ---------- | --- | ------ | ---- | ----- |
| A          | 20  | 30     | 0    | 50    |
| B          | 5   | 15     | 30   | 50    |
| C          | 40  | 10     | 0    | 50    |
| D          | 5   | 5      | 40   | 50    |
| Total      | 70  | 60     | 70   | 200   |

:::

:::{#exr-18.11}
A researcher seeks to investigate whether there is a relationship between smoking status (smoker/non-smoker) and lung cancer (yes/no). They collect data on a large sample of individuals, producing the following two-by-two contingency table. 

|            | Lung Cancer: Yes | Lung Cancer: No | Total |
| ---------- | ---------------- | --------------- | ----- |
| Smoker     | 80               | 120             | 200   |
| Non-Smoker | 20               | 180             | 200   |
| Total      | 100              | 300             | 400   |

a. If there is no relationship between smoking and lung cancer, how is this represented statistically?
b. What are the expected totals in the contingency table under the assumption that there is no association between smoking and lung cancer?
c. Test whether there is a relationship between smoking and lung cancer, based on the observed data, at the $\alpha = 0.05$ level of significance. 

:::

:::{#exr-18.12}
A policy analyst wants to determine whether there is an association between an individual's level of education (high school, bachelor's, graduate) and their income level (low, medium, high), in a particular region. As a result, they take a sample of $500$ individuals from the region, and observe the following results. 

| Education Level   | Low Income | Medium Income | High Income | Total |
| ----------------- | ---------- | ------------- | ----------- | ----- |
| High School       | 80         | 70            | 50          | 200   |
| Bachelor's Degree | 50         | 100           | 70          | 220   |
| Graduate Degree   | 20         | 50            | 10          | 80    |
| Total             | 150        | 220           | 130         | 500   |

In this region, is there evidence that an individual's level of income is associated with their education (at $\alpha = 0.01$)?

:::

:::{#exr-18.13}
A group of $200$ kindergarten students are given a survey where they are asked their favourite colour, and their favourite season. The results are summarized as follows. 

| Favourite Colour | Spring | Summer | Autumn | Winter | Total |
| ---------------- | ------ | ------ | ------ | ------ | ----- |
| Red              | 15     | 25     | 10     | 5      | 55    |
| Blue             | 20     | 20     | 15     | 10     | 65    |
| Green            | 10     | 15     | 20     | 10     | 55    |
| Yellow           | 5      | 10     | 5      | 5      | 25    |
| Total            | 50     | 70     | 50     | 30     | 200   |

a. Are there any concerns with testing this hypothesis, based on the observed data?
b. Regardless of your response in (a), test the hypothesis at the $\alpha = 0.05$ level of significance. 

:::

:::{#exr-18.14}
Data on the hair colour (blonde, brown, red, black) and eye colour (blue, green, brown) of a group of $550$ individuals is contained in the following table. 

| Hair Colour | Blue Eyes | Green Eyes | Brown Eyes | Total |
| ----------- | --------- | ---------- | ---------- | ----- |
| Blonde      | 60        | 30         | 10         | 100   |
| Brown       | 80        | 70         | 150        | 300   |
| Red         | 10        | 20         | 20         | 50    |
| Black       | 10        | 10         | 30         | 50    |
| Total       | 160       | 130        | 210        | 500   |

a. Using these data, test whether eye colours are equally distributed in this population ($\alpha = 0.05$).
b. Using these data, test whether hair colours are equally distributed in this population ($\alpha = 0.05$).
c. Using these data, test whether there is an association between eye colour and hair colour in this population ($\alpha = 0.05$).

:::

:::{#exr-18.15}
Research has long suggested that there is no such thing as "learning styles". That is, students do not perform any better when being taught to their reported learning style, compared with being taught in a randomly assigned manner. This finding has been robustly demonstrated overtime. A group of researchers is interested in whether an individual's reported preferred learning style has an impact on how well they perform on particular tasks (that correspond to the same learning styles). 

They collect the following data on $300$ individuals who report being either visual, auditory, or kinesthetic learners, and then rate which task (of a visual, auditory, or kinesthetic task) they performed best on. 

| Preferred Learning Style | Best on Visual Task | Best on Auditory Task | Best on Kinesthetic Task | Total |
| ------------------------ | ------------------- | --------------------- | ------------------------ | ----- |
| Visual                   | 60                  | 30                    | 30                       | 120   |
| Auditory                 | 30                  | 70                    | 20                       | 120   |
| Kinesthetic              | 20                  | 20                    | 20                       | 60    |
| Total                    | 110                 | 120                   | 70                       | 300   |

a. Explain what it would mean to test whether there is an association between preferred learning style and task performance in these data. Would the presence of an association provide evidence that learning styles really exist? Explain.
b. Test the hypothesis (at $\alpha = 0.05$) that there is an association between preferred learning style and task performance using these data. 
:::

:::{#exr-18.16}
A store owner believes that the number of customers visiting the store on each day of the week follows a specific pattern. Data are collected for a week. 

a. Formalize the store owner's hypothesis in the framework of hypothesis testing. What test needs to be performed, and why?
b. What data would need to be collected to perform the test? How would the test proceed?
:::

:::{#exr-18.17}
A pharmaceutical company is testing three different formulations of a drug. They want to know if the effectiveness (rated categorically) of the drugs is the same, and they further want to know if there are any differences in side effects for the various formulations. 

Describe how the company can test these hypotheses. Describe what the data collection would need to look like, and what hypothesis test would be required. 
:::

:::{#exr-18.18}
A researcher collects data on people's favourite genre of music and their personality type (introvert/extrovert).

a. Describe three separate hypotheses that can be tested on the basis of these data.
b. What hypothesis test would be used to test each of the described hypotheses? How do these tests differ from one another?
:::

:::{#exr-18.19}
a. Explain the difference between a test for homogeneity and a test for independence. 
b. Give an example of a research question that would be addressed by each test.
:::

:::{#exr-18.20}
A company wants to know if there is a relationship between employee satisfaction and productivity. They collect data on both variables. Employee satisfaction is rated categorically (as either satisfied or not satisfied), however, productivity is a quantitative measure ranging from $0$ (not at all productive) to $100$ (maximum productivity). 

a. What hypothesis test can the company perform to understand this question?
b. What must be done in order to perform the hypothesis test identified in part (a)?
c. Is there an alternative method that could be used to test this question? What must be done in order to perform this hypothesis test?
d. What concerns or considerations may exist to select between the test identified in (a) and that identified in (c)? Can you think of any additional hypothesis tests that may alleviate some of these concerns? 
:::

::: {.content-visible when-format='html'}
{{< include /calculator.qmd >}}
:::