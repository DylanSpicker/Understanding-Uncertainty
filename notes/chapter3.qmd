# The Core Concepts of Probability
## Assigning Probabilities (and The Equally Likely Outcome Model)
There are a plethora of ways to assign probabilities to different events. At the most basic level any rule that maps from the space of possible events to real numbers between $0$ and $1$ can be used as rules for probability assignment. That is, probability assignment is simply a set of rules which says "for this event assign this probability."

:::{#exm-coin-toss}
## Coin Toss Probabilities
Suppose that the fair coin used by Charles and Sadie is tossed one time. Write down the probability assignments relating to this experiment.

::::{.callout .solution collapse='true'}
## Solution
In this case we have $\mathcal{S} = \{\text{H},\text{T}\}$. Thus, the possible events for which we need to assign probabilities are $\emptyset$, $\{\text{H}\}$, $\{\text{T}\}$, and $\{\text{H},\text{T}\} = \mathcal{S}$. For any probability model we have $P(\emptyset) = 0$ and $P(\mathcal{S}) = 1$. When we say that a coin is "fair" we are saying that $P(\text{T}) = P(\text{H})$, and since these are the only two possible outcomes in the sample space, we must have that they each have probability $0.5$.
::::
:::

Not every assignment of probability values is going to be valid. Suppose, for instance, that we have a six-sided die, each side labelled with a number from one to six. If I told you that there was a probability of $0.5$ that it comes up $1$, $0.5$ that it comes up $2$, $0.5$ that it comes up $3$, $0.5$ that it comes up $4$, $0.5$ that it comes up $5$, and $0.5$ that it comes up $6$, you would probably call me a liar.^[Or else conclude that I was mistaken and maybe should not be teaching probability.] If, as we have previously seen, probabilities represent the long run proportion of time that a particular event is observed, we cannot have $6$ different outcomes each occurring in half of all cases.

Beyond the requirements that we impose on what constitutes a "valid" probability rule, we have another concern: scalability. It is perfectly acceptable to indicate that in an experiment with $3$ outcomes, the first has a probability of $0.25$, the second of $0.3$, and the third of $0.45$. What if the experiment has $100$ possible outcomes? Or $1000$? It quickly becomes apparent that enumerating the probabilities of each event in the sample space is an efficient way of assigning probabilities in practice. A core focus of our study of probability will be finding techniques that allow us to efficiently encode probability information into manageable objects. Once we have done this we will be in a position where we can manipulate these (comparatively) simple mathematical quantities in order to make statements and conclusions about any of the events of interest, even if they have never been explicitly outlined as having an assigned probability.

While we will consider myriad methods for accomplishing these goals throughout our study of probability, we begin with a very useful model which simplifies probability assignment, without any added complexity, and creates a solid foundation for us to explore the properties of probability models. We start by considering **equally likely outcomes.** As the name suggests, the probability model considering equally likely outcomes assigns an equal probability to every possible outcome of the experiment. This is a probability model that we are already distinctly familiar with: flipping a coin, rolling a die, or drawing a card are all examples of experiments which rely on the equally likely outcomes framework.

:::{.remark}
## Statisticians and Urn Models
In statistics and probability courses and books you will often have instructors or authors using fairly simple models to illustrate probability concepts. There will often be questions relating to coin tosses, and dice, and decks of cards, and everyone's favourite: urns. It will very frequently be the case that a statistics question will state that there is an urn with some combination of coloured balls within it, from which you will be selecting some number either with or without replacement. The frequency of these types of examples and questions often feels disconnected from the refrain that "uncertainty is all around us" and that "statistics is relevant to every aspect of our world!"^[One of the most famous quotes from a statistician was a thought shared by John Tukey, stating "The best thing about being a statistician is that you get to play in everyone's backyard." This is a common refrain, and one rooted in truth. Statistics is everywhere, across every field of human inquiry, and can help us make sense of everything from the trivial to the deeply important.] Why is it that we seldom see questions or examples that are directly tied to these wide spread applications of the lessons and techniques being taught?

In part these simple experiments are cleaner to handle than "real world" situations. We can easily assume that a die is fair and that takes care of any unsuspecting wrinkles that will necessarily come along with the "real world". This is not dissimilar to working under the assumption of frictionless surfaces in introductory physics, or assuming that human beings are rational in economics. Another key point is that most of us have deep familiarity with dice, and coins, and cards.^[This does not help to explain why we use urns so much, of course. When was the last time any of us drew a ball from an urn?] The same is not going to be true of stories that are derived from different use cases in the real world. A final important point, and this will be something we see in depth in the coming chapters, is that from a statistical point of view: there is no difference. Once we have the tools to work with these quantities, we have the tools to work with any of the quantities. This actually distinguishes the use of these types of examples in statistics and probability from those for other subjects: at no point is anything that we are learning incorrect, or overly simple - we are just focusing on the raw probabilistic nature of the phenomenon. As a result, we will continue to see these simple models in these notes. I would encourage you, whenever possible, to hold a topic in mind that matters more to you and start trying to draw the parallels between rolling dice, and whatever it is that you may care about. 

Why urns, specifically? Well, whether it be coin flipping or dice rolling or card selection, we can model this equivalently using an urn (with $2$, $6$, and $52$ items, respectively). The urn becomes more flexible to *exactly* dictate what the probability of any selection will be, which is a useful way of moving from equally likely models (each ball is equally likely to be selected) to arbitrary models (we can have however many identical balls in the urn as we would like).
:::

If we have an experiment with a sample space $\mathcal{S}$ which has $|\mathcal{S}| = k$ total elements^[Note that, when we have a set, using the absolute value symbols $|\cdot|$ stands for the **cardinality** of the set. Cardinality is just a fancy way of saying the size or the number of elements that the set has in it.], then each element of the sample space occurs with probability $\frac{1}{k}$. In the case of the coin toss example, $\mathcal{S} = \{\text{H}, \text{T}\}$, and so $k=2$ and each outcome occurs with probability $\frac{1}{2}$. In the case of drawing a card at random, there are $52$ different outcomes, and so $k=52$, and the probability of drawing any particular card is $\frac{1}{52}$.

It is critically important to recognize that the equal probability model assigns equal likelihood to the possible outcomes of an experiment, not the possible events of interest. It will not be the case that all events have the same probability. To make this concrete, consider the events $A$ "the ace of spades is drawn" and $B$ "any spade is drawn". It is clear that $B$ happens more frequently than $A$, even though we have said that this is an experiment with equally likely outcomes. Remember: an outcome is an observation from a single experimental run, an event is any collection of these possible outcomes.

A core goal is then bridging the gap between the probability of an outcome^[A quantity which in the equally likely outcome framework, we know exactly.] and the probability of an event. In order to do so, we will next consider the rules of probability, introducing properties that are required for valid probability assignments, and the techniques for manipulating probabilities to calculate the probabilities of quantities of interest.

## The Axioms of Probability
We have previously seen that not every probability assignment can be valid. For instance, assigning $0.5$ probability to each outcome on a die leads to a nonsensical scenario. With just a little imagination, we can conjure equally nonsensical scenarios in other ways. For instance, it would make very little sense to discuss the probability of an event being a negative value. What would it mean for an event to occur in a negative proportion of experimental runs? Alternatively, we can consider two events that are nested in one another: say event $A$ is that we draw the ace of spades, and event $B$ is that we draw any spade. Every single time that $A$ happens, we know that $B$ also happens. But there are ways that $B$ can occur where $A$ does not.^[For instance, the Queen of spades being drawn.] If I told you the probability of $A$ was $0.5$ and the probability of $B$ was $0.2$, this would violate our base instincts. How can it be more likely to draw the ace of spades than it would be to draw any spade at all?^[This is actually a scenario where our instincts may lead us awry in some situations. Consider the following from @kahneman: [Linda is 31 years old, single, outspoken, and very bright. She majored in philosophy. As a student, she was deeply concerned with issues of discrimination and social justice, and also participated in anti-nuclear demonstrations. Which is more probable? (a) Linda is a bank teller, or (b) Linda is a bank teller and is active in the feminist movement.]{.blockquote .d-block} A majority of respondents rate (b) as being more probable, even though (a) is contained in (b).]

Often in mathematics when we have an intuitive set of rules^[These rules, which we call "properties" are formally known as "axioms".] that particular quantities must obey, we work to add formality through defining properties of these concepts. To this end, we can define the key properties that probabilities must obey in order to be well-defined, valid probabilities. With three fairly basic properties, we can completely specify what must be true in order for a set of probabilities to be "valid", and to in turn match with our intuitions. 

:::{.callout-tip icon=false}
## The Axioms of Probability 

1. **Unitary:** Every valid set of probabilities must assign a probability of $1$ to the full sample space. That is, $P(\mathcal{S}) = 1$. This is an intuitive requirement as every time the experiment is run we observe an outcome in the sample space. As a result, in every experimental run the event $\mathcal{S}$ occurs.
2. **Non-negative:** We require that every probability is non-negative. We can have probabilities of $0$, but we can never have a probability less than zero. Again, this is sensible^[What would it mean to have a negative probability? It is perhaps a more interesting question than it seems at first glance. It is a topic that has come up in some pretty strange places and, while it is not presently sensible to call them "probabilities" in a traditional sense, there are interesting results which follow.] but is important to include in our formalization. Specifically, for every event $E$, we must have $P(E) \geq 0$.
3. **Additivity:** the final property requires slightly more parsing on first pass. Suppose that we define a sequence of events, $E_1, E_2, E_3, \dots$ such that no two events have any overlap. That is, $E_j \cap E_\ell = \emptyset$ for all $\ell\neq j$. Then, the final property we require for probabilities is that $$P\left(\bigcup_i E_i\right) = \sum_i P(E_i).$$ That is, the probability of the union of disjoint events is the summation of the probability of these events.
:::

It is worth dwelling slightly on axiom 3. Consider the case of drawing a card at random from a deck of $52$ cards. Using the equally likely outcome model for probability we know that the probability that any card is drawn is given by $\frac{1}{52}$. If I were to ask "what is the probability you draw that ace of spades?" under this model you can respond, immediately, with $\frac{1}{52}$. Now, if I were to ask "what is the probability that you draw the ace of spades or the two of spades?" then intuitively you likely figure that this will be $\frac{2}{52}$. Note that the event $E_1$, "draw the ace of spades" and the event $E_2$ "draw the two of spades", are disjoint events. Moreover, recall that the union is the "or" and so $E_1\cup E_2$ is the same as $E_1$ or $E_2$. Taken together then, $$P(E_1\cup E_2) = P(E_1) + P(E_2).$$ The axiom of additivity simply extends this intuition to an arbitrary number of events.

:::{#exm-additivity}
## Basic Additivity
Still unsure of how best to go about using cards to replace their coin game, Charles and Sadie are considering various different events and trying to understand their probabilistic behaviour. They take $S$, $C$, $H$, and $D$ to be the events that a spade, club, heart, or diamond are drawn from a standard deck of cards, respectively. Further, they take $C_j$ to be the event that a card with denomination $j$ is drawn ($j$ ranging from ace with $1$ through King with $13$). If they consider the union of any two (or more) of these events when can they leverage properties of additivity? When can't they?


::::{.callout .solution collapse='true'}
## Solution
In order to use the properties of additivity it is required that the two events are disjoint. Note that taking any two (or more) of $S$, $C$, $H$, and $D$ will lead to disjoint events. There is no way to draw a card which has two suits on it at once. Similarly, taking any two (or more) of $C_j$ will lead to disjoint events. However, mixing any of the suited events ($S$, $C$, $H$, and $D$) with any $C_j$ will not be disjoint. 

Consider $S\cap C_1$. The ace of spades is in $S$ since it is a spade and it is in $C_1$ since it is an ace. As a result, $S\cap C_1 = \{\text{Ace of Spades}\}$. Because of this we are not able to say that $P(A \cup C_1) = P(A) + P(C_1)$. However, we can say that $$P(S\cup C\cup H\cup D) = P(S) + P(C) + P(H) + P(D),$$ and could do the same with any subset of these sets. Similarly, we can take $$P\left(\bigcup_{j=1}^{13} C_j\right) = \sum_{j=1}^{13} P(C_j),$$ or any of the subsets there.
::::
:::

These three axioms fully define valid probabilities. Any mechanism that assigns probability values to events which conforms to these rules will assign valid probabilities. While it may seem counterintuitive that such basic rules fully define our notion of a probability, these rules readily give rise to many other properties that are indispensible when working with probabilities.

## Secondary Properties of Probabilities
Using the previously indicated axioms of probability we are able to derive many useful **secondary properties**. These properties will frequently be used to actually compute different probabilities, and are helpful to become familiar with. All of the following properties follow directly from the axioms, though, some are more clear than others. For the following we take $E$ and $E_1,E_2,E_3,\dots$ to be arbitrary events on some well defined sample space.

1. $P(E^C) = 1-P(E)$, and equivalently, $P(E) = 1 - P(E^C)$.
2. $P(\emptyset) = 0$.
3. $P(E_1 \cup E_2) = P(E_1) + P(E_2) - P(E_1\cap E_2)$.
4. $$P(E_1 \cup E_2 \cup E_3) = P(E_1) + P(E_2) + P(E_3) - P(E_1\cap E_2) - P(E_1 \cap E_3) - P(E_2 \cap E_3) + P(E_1 \cap E_2 \cap E_3).$$
5. If $E_1 \subset E_2$ then $P(E_1) \leq P(E_2)$. 

:::{.callout-warning icon="false" collapse="true"}
## Proofs of the Secondary Properties of Probability
It may be instructive to see how these properties are derived. Doing so generates added familiarity with manipulating probability expressions and helps to encourage deeper understanding. 

1. Note that, for any event $E$, by definition we have $E \cup E^C = \mathcal{S}$ and $E \cap E^C = \emptyset$. As a result, we can apply **additivity** to the sets $E$ and $E^C$ giving $P(E \cup E^C) = P(E) + P(E^C)$. However, since $E\cup E^C = \mathcal{S}$, then we know that $P(E \cup E^C) = P(\mathcal{S}) = 1$ by the **unitary** property. Taken together this tells us that $1 = P(E) + P(E^C)$, and rearranging gives $P(E^C) = 1 - P(E)$, or $P(E) = 1 - P(E^C)$, as required.

2. We know that $\mathcal{S}^C = \emptyset$. Using secondary property (1), $P(E) = 1 - P(E^C)$. Taking $E = \emptyset$ gives $P(\emptyset) = 1 - P(\mathcal{S}) = 1 - 1 = 0$, by the **unitary** property.

3. Here note that $E_1 \cup E_2$ can be written as $E_1 \cup E_2'$ where $E_2' = E_2\cap E_1^C$. That is, $E_2'$ contains the outcomes from $E_2$ which were not shared by $E_1$. Then $E_1 \cap E_2' = \emptyset$ so we can write $P(E_1 \cup E_2) = P(E_1 \cup E_2') = P(E_1) + P(E_2')$, by **additivity**. Now, if we define $E_2^* = E_2\cap E_1$ then $E_2 = E_2' \cup E_2^*$, and $E_2'\cap E_2^* = \emptyset$. Thus, $P(E_2) = P(E_2'\cup E_2^*) = P(E_2') + P(E_2^*)$. Rearranging this gives $P(E_2') = P(E_2) - P(E_2^*)$, and we know that $P(E_2^*) = P(E_1 \cap E_2)$. Thus, plugging into what we found before we get $$P(E_1 \cup E_2) = P(E_1) + P(E_2') = P(E_1) + P(E_2) - P(E_1 \cap E_2).$$

4. This follows exactly from the argument for (3). To see, first consider $E_2 \cup E_3$ to be an event itself, say $E_4$. Then we can apply the above result to $E_1 \cup E_4$. And then we need only repeat the process for the remaining terms. 

5. We can rewrite $E_2$ as $E_1 \cup (E_2 \cap E_1^C)$. These two sets are disjoint, since the one is $E_1$ and the other must contain only elements in $E_1^C$. Then, $P(E_2) = P(E_1) + P(E_2 \cap E_1^C)$ by **additivity**. Then, through the **non-negative** property we know that $P(E_2 \cap E_1^C) \geq 0$, and so rearranging we have $P(E_1) = P(E_2) - P(E_2\cap E_1^C) \leq P(E_2)$. 
:::

These properties are immensely useful when computing probabilities. In fact, these secondary properties will be used with more frequency than the basic axioms when manipulating probabilities in practice. It is worth building comfort with these properties, early and often, as they will assist in manipulating all probability expressions in the future. 

While these properties hold in general for all probability models, it is instructive to focus on the equal probability model to begin building familiarity with probability. These properties allow us to take events -- whether compound or simple -- and combine, rewrite, and manipulate expressions to assist in the handling of the computations. Eventually, however, we require the ability to assign numerical values to these probabilities.

Consider a simple event, $A$. Recall that a simple event is defined as a possible outcome of an experiment, and so in this case, $A$ corresponds directly to an event that may be observed. If our sample space is $k$ elements large, then $P(A) = \frac{1}{k}$ in this framework. For instance, if $A$ is the event that a two is rolled on a six-sided fair die, then $P(A) = \frac{1}{6}$. 

Now, suppose that a compound event is defined, $B$. By definition, a compound event can be expressed as a set of possible outcomes from the experiment. Suppose that we enumerate these possible events as $b_1, b_2, \dots, b_\ell$. Then we know that $B$ occurs if any of $b_1,b_2,\dots,b_\ell$ occur. Each $b_j$ are elements of the sample space and correspond to possible outcomes of the experiment. As a result, we know that $P(b_j) = \frac{1}{k}$, based on the equal probability assumption. Now, if we take any two distinct events, say $b_i$ and $b_j$, we know that they must be disjoint: $b_i \cap b_j = \emptyset$. This is because in an experiment run only one outcome can occur. Moreover, we can say that $B = b_1 \cup b_2 \cup\cdots\cup b_\ell$.

Using the axioms of probability outlined above we therefore know that $P(B) = \sum_{j=1}^\ell P(b_j) = \sum_{j=1}^\ell \frac{1}{k} = \frac{\ell}{k}$. This holds in general for any compound event in this setting. If we take $B$ to be the event that an even number is rolled on a six-sided die, then we would have $b_1$ is the event that a two is rolled, $b_2$ is the event that a four is rolled, and $b_3$ is the event that a six is rolled. There are three such events, and so the probability that an even number is rolled must be $\frac{3}{6} = 0.5$, which matches our intuition.

If we consider what this process is doing at its core, we can reframe the calculation as counting up the number of ways that event can happen and dividing by the total number of events. In our previous discussion, there were $\ell$ ways of $B$ occurring, a total of $k$ outcomes, and so the probability becomes $\frac{\ell}{k}$. In the equal probability model, this will always be the case. The probability of any event $A$ occurring is given by $$P(A) = \frac{N_A}{k},$$ where $N_A$ is the number of unique ways that $A$ can occur. In other words, $N_A$ is the size of the set $A$, $|A|$.

As a result of this, computing probabilities largely relies on the counting of possible outcomes corresponding to different events. If we can determine $N_A = |A|$, and the count of the total number of occurrences, $k$, then we can determine the probability of $A$. This study of counting is known as **combinatorics**, and it is where we will turn our attention next.

:::{#exm-complement-trick}
## Unmatched Six-Sided Dice
Charles and Sadie, not all together content with the progress through decks of cards, are considering games with dice. Suppose that they have two, fair, six-sided dice. They are interested in the probability that the two dice show different numbers when they are rolled. What is this probability?

::::{.callout .solution collapse='true'}
## Solution
Here, the key is to realize that the probability is easier to solve when considering the complement rather than the event itself. Notably, rolling two, fair, six-sided dice gives a total of $36$ possible outcomes. Of these, exactly $6$ have equal numbers showing on both dice. Thus, the probability that the two dice show the **same** number is going to be $\frac{6}{36} = \frac{1}{6}$. Then, using the fact that $P(E) = 1 - P(E^C)$, and that taking $E$ to refer to the event where the two dice show different numbers, then $E^C$ refers to the event that the two dice show the *same* number. As a result, the probability that we want is $P(E) = 1 - \frac{1}{6} = \frac{5}{6}$.
::::
:::

:::{#exm-complement-trick-two}
## Unmatched Arbitrary Dice 
Charles and Sadie, working from their intrigue about dice, have decided that instead of using two six-sided dice, they wish to take two dice of possibly different sizes. Suppose that the first die has $d_1$ sides and the second has $d_2$ sides, and that both dice are otherwise fair. They are interested in the probability that the two dice show different numbers when they are rolled. What is this probability?

::::{.callout .solution collapse='true'}
## Solution
This problem is conceptually no different from @exm-complement-trick. There will be a total of $d_1\times d_2$ possible combinations of the two dice to be rolled.^[Note: if this is not yet clear to you, that's okay! In the very next section we begin to discuss how to count the possible combinations in these types of scenarios.] Of these, the dice will match in $\min\{d_1, d_2\}$ events.^[Suppose that $d_1 = 2$ and $d_2 = 4$. Then here, the dice can match when they show either $1$ or $2$, but if the second die shows $3$ or $4$ there is no possibility of having a match at all.] Then, with the same complement trick discussed above, we get $$P(E) = 1 - P(E^C) = 1 - \frac{\min\{d_1,d_2\}}{d_1\times d_2}.$$
::::
:::

## Combinatorics
### The Product Rule
Fundamentally, counting is a matter of assessing the size of a collection of items. Sometimes, this is very straightforward. If you want to count the number of students in a classroom, you start at $1$ and enumerate upwards through the integers. To count the number of days until the next Holiday, you do the same thing. If you really need to sleep, perhaps you will count imaginary sleep until you drift off. There is not much to this type of counting, and it is certainly deeply familiar to you all. However, it is also quite limited in its utility.

Imagine that you are interested in determining how many possible ways there are of arranging a deck of $52$ cards. You could of course arrange them in a particular order, then count each of those. That would take a tremendous amount of time, so perhaps instead of using an actual deck you just write down the combinations. Still, each combination is going to be $52$ cards long, and keeping track of that all will be a tremendous challenge. This seems like an approachable question, and yet, it illustrates how complicated (and large) these types of "counting" problems can become very quickly.

Fortunately for us there are some strategies for simplifying these problems down, some of which you are likely already familiar with. Think about trying to form an outfit where you have $4$ different sweaters, $3$ pairs of pants, and $2$ options for your shoes. Suppose that any combination of these will work well. How many total outfits are there? Well, if you have already picked your sweater and pants, then there are going to be $2$ different outfits using these: one with each of the pairs of shoes. This is true for each possible sweater-pant combination, and so we can count $2$ for each one these. In other words, to get the total number of outfits we multiply the number of sweater-pant combinations by the number of shoe options. The same rationale can be applied to count the total number of sweater-pant combinations. For each sweater, there are $3$ pairs of possible pants, and so to get the total number we can take $3$ for each possible sweater, or in other words, $3\times 4$. Taken together then we have $4\times 3\times 2 = 24$ total possible outfits. 

Another way of framing this is that we have to make three sequential decisions: which of the $4$ sweaters, which of the $3$ pants, and which of the $2$ shoes are to be worn? When we do this we multiply through the number of alternatives at each decision point to get the total number of combinations. This is known as the **product rule for counting**.

:::{#def-multiplication-rule}
## Product Rule for Counting
The product rule for counting states that, when there are a sequence of $k$ decisions to be made, and for each decision $j=1,\dots,k$, there are $n_j$ options, then the total number of combinations will be $$N = n_1\times n_2\times\cdots\times n_k.$$
:::

:::{#exm-counting-coffee-orders}
## Counting Coffee Orders
When Charles and Sadie are out for coffee, Sadie enjoys ordering the same thing each time: a black coffee and vegan chocolate chip cookie. Charles, on the other hand, has decided to work through the entire menu of the local coffee shop, each day ordering a drink, with one add-in, and a snack. If there are $10$ different drinks, $8$ possible add-ins, and $12$ different snacks, how many trips to the coffee shop will it take until Charles has tried it all?

::::{.callout .solution collapse='true'}
## Solution
This necessitates an application of the product rule for counting. Specifically, we can view this as three sequential decisions, where the first decision is which drink (with $n_1 = 10$), the second decision is which add-in (with $n_2 = 8$), and the third decision is which snack (with $n_3 = 12$). Taking the product gives the total number of combinations as $10\times 8\times 12 = 960$. As a result, it will take $960$ visits (assuming that nothing on the menu changes!) to try all combinations. 
::::
:::

:::{#exm-rolling-sequence-of-dice}
## Sequence of Dice Rolls
Charles and Sadie have been enjoying playing with dice, but they lost one of the two they had. As a result, they are trying to come up with games revolving around rolling a single die. They decide to try a game called "six is lava", where they roll a single six-sided die $10$ times in a row. If they get $1$ or more sixes, they lose the game. They are not sure if $10$ is the correct number of rolls to use. What is the probability that they lose on any given set of $10$ rolls of the die in this game?

::::{.callout .solution collapse='true'}
## Solution
Once again this is a scenario where using the complement simplifies the problem. If we asked "what is the probability that no 6's are rolled, on $10$ rolls of the die" then we can count the number of possibilities through an application of the product rule. In particular, there are going to be $5$ options which are not $6$ at each possible step. We can view this as have $n_1 = n_2 = \cdots = n_{10} = 5$. Thus, the total number of ways of rolling **no** sixes is $5\times5\times5\times\cdots\times5 = 5^{10}$. 

Essentially the same process can be used to count the total number of possible rolls, replacing $5$ with $6$ to get the denominator. This means that there are $6^{10}$ total sequences of $10$ rolls, and $5^{10}$ which contain no sixes. As a result, taking $E$ to represent the probability that we observe no sixes on $10$ rolls of the die, we would get $$P(E) = \frac{5^{10}}{6^{10}}.$$ The question asks for $E^C$ and so we take $$P(E^C) = 1 - P(E) = 1 - \left(\frac{5}{6}\right)^{10}.$$ This is approximately `r round(1 - (5/6)^(10), 4)`.

Note that if instead of $10$ flips they had $n$ flips, the probability would be $1 - \left(5/6\right)^n$. We can plot this over various values of $n$, to see how the number of flips impacts this probability. The probability of $0.5$ is marked and we can see that taking $4$ tosses gives an ever so slightly greater than $0.5$ probability of losing (`r round(1 - (5/6)^4, 4)`).
```{r}
#| echo: false 

n <- 0:20
probs <- 1 - (5/6)^n

plot(probs ~ n, main = "Probability of losing in 'six is lava.'", xlab = "Number of Tosses", ylab = "Probability of Losing", type = 'p')
abline(h = 0.5, lty = 3)
```
::::
:::

### Tree Diagrams
Sometimes it is helpful to express counting rules graphically. To do so we rely on tree diagrams. 

:::{#def-tree-diagram}
## Tree Diagram
A tree diagram is a graphical representation for the product rule of counting. Specifically, a tree diagram puts each of the decisions in sequence, and draws a branch for each separate option, starting from the branches drawn at the previous decision step.
:::

To draw a tree diagram, you start with the first choice, drawing one branch for each of the $n_1$ alternatives, labelling each. Then, at the second choice, you do the same process at the end of each of the branches you drew for choice $1$, this time drawing $n_2$ branches there (so you will have just drawn $n_1\times n_2$ branches). Then for each of those you draw the $n_3$ further branches, and so on and so forth until the end. 

If you want to know the total number of choices, you simply count the end points at the very end of the diagram. Each branch corresponds to a single option. To determine which combination of choices it corresponds to, you simply read off the branch labels at each branch you take. If you want to know how many possible combinations come with certain options selected, you can look at only those branches which are downstream from the choices that you care about.

![A generic tree diagram. Here the first choice has three different options and the second choice has two. We can see the six total combinations, labelled on the right of the diagram, and can trace the choices required to get there.](/graphics/ch3-tree-diagram-generic){#fig-tree-diagram-general}

While tree diagrams can be quite useful for visualizing a problem, they often grow to be overly complex. As a result, we need to fall back on the numerical representation afforded to us through the product rule for counting. Counting problems, in general, can very quickly become tremendously large and complex. For this reason, we have several tools to assist us in reducing this complexity based on common types of problems that we would like to count.

### The Factorial
The first useful tool for simplifying these problems is the **factorial**. The factorial of an integer, denoted by $x!$ is given by the product of all integers from $x$ to $1$.^[Factorials are exciting because they always look like they are shouting!] That is, $$x! = x(x-1)(x-2)\cdots(2)(1).$$ If we consider the product rule for counting then note that if $n_1=1$, $n_2=2$, ... , $n_k = k$, then the total number of options is $k\times(k-1)\times\cdots\times 1 = k!$. The most common reason that this comes up is when we want to order a collection of items. Suppose that you have $10$ books that you want to place on a shelf. You can view this as making $10$ sequential decisions: what book goes first, second, third, and so on. There are $10$ options for the first book, then $9$ for the second (any except for the first one), and then $8$ for the third (any except for the first $2$). This continues down to the last book, and so we conclude that there are $10\times9\times8\times\cdots\times1 = 10!$ ways of arranging these books.

:::{#exm-factorial}
## Seating in a (Full) Coffee Shop
One day Charles and Sadie walk into the coffee shop and find that it is completely full. There are ten seats and ten people sitting in them. They are disappointed that they do not have room to sit themselves, however, they are never ones to pass up an interesting probability question. 

a. How many different ways could these ten people have sat in these ten seats? 
b. If there are ten drinks that have been made, and one is to be passed out to each seat, how many different ways can these ten people sit in these ten seats, with each of these ten drinks? 
c. Alongside the ten drinks, there are ten snacks to be served up as well. How many different ways can these ten people sit in these ten seats, with each of these ten drinks, and each of these ten snacks?

::::{.callout .solution collapse='true'}
## Solution
a. Here, we can think about lining up the ten seats in a row, each number $1$ through $10$. Then, we want to place one patron into each of the seats. This is no different from ordering $10$ books, and so the total is $10!$ which is $3,628,800$. 

b. Note that we still have to make the seat choices from part (a), so there are $10!$ ways of getting the $10$ people sat in the $10$ chairs. Once there, we can think of handing out the drinks to each of the numbered combinations of person-chair. This is no different from passing out the people as well, giving $10!$ ways of doing this. We use the product rule to combine these two choices, with $10!\times 10!$ total combinations of people-chair-drink. This is $13,168,189,440,000$.

c. Extending the same logic before, there are $10!\times 10!$ ways of getting each person sat in a chair with a drink. Then, there will be an additional $10!$ ways of passing out the snacks to these people. Taken together this gives $10!\times 10!\times 10!$ which is $47,784,725,839,872,000,000$.^[It is somewhat interesting to note how large these values get relatively quickly. This is a comparatively small question: only $10$ people with $10$ drinks and $10$ snacks. If you consider any counting problem with a larger number of items, these problems quickly grow to be intensely complex. For instance, a count of my main home book collection reveals $376$ of them. To order these would give $376!$ possible orderings. In decimal representation this is $$4.992244775852435618292576458782762114148884082811840265632\dots\times10^{806}.$$ This is an $807$ digit long number. This is an incomprehensibly large number. This number is $8×10^{726}$ **times larger than** the number of atoms in the universe. That is, if every atom in the universe were given some arrangements of books to hold onto, they would need to hold $8×10^{726}$ of them in order for all of the arrangements to be held. I point this out because combinatorics *explodes* in this way. Even simple problems grow out of hand very, very quickly. This is where comfort with the algebraic tools is required, rather than a reliance on intuition. There is simply no way to have intuition regarding the scope of these numbers, at least, not without a lot of practice.]
::::
:::

:::{.remark}
## 0!
Depending on how factorials are thought of, some trouble can come up around a quantity like $0!$. On one hand, if we view factorials as multiplying each number between $n$ and $1$ together then $0! = 0\times 1$ and we get $0! = 0$. On the other hand, if we view factorials as counting the number of ways which we can order a set of $n$ items, then $0!$ is the number of ways we can order $0$ items, which is $1$.^[Imagine I am placing books on my shelf. With $3$ books there are $3! = 6$ ways my shelf can look at the end. With $2$ books there are $2! = 2$ ways my shelf can look at the end. With $1$ book there are $1! = 1$ ways my shelf can look at the end. With $0$ books there are $0! = 1$ ways my shelf can look at the end.] So, which is it?

We take $0!$ to be equal to $1$. The ordering argument is perhaps the most convincing. However, if you are algebraically minded you may wonder how we get around the tricky issue of using our algebraic definition. The key insight is to not define $n!$ as the product of the numbers from $n$ to $1$, but rather, to define $$(n-1)! = \frac{n!}{n},$$ and specify that $1! = 1$. Then in this case we get all of the usual requirements for how we have discussed factorials, but we also get that $0! = \frac{(0+1)!}{(0+1)} = 1$. As a result, we will take $0! = 1$.^[Note, this does not help us with the factorials of negative numbers, nor of fractional numbers. Factorials *can* be extended to these in sensible ways, but these are not for combinatorial purposes and are no longer "factorials" exactly.]
:::

### Permutations and Combinations
Sometimes, we want to order items from a collection, but we want to only take a subset of these times. That is, suppose that you have $20$ books, only $9$ of them will fit on the self, and you want to know "how many ways can you put $10$ books on the shelf, in order, from your collection of $20$?" Using the product rule of counting for this directly, we recognize that there are $20$ options for the first, then $19$, then $18$, and so on until there are $12$ choices for the $10$th book to place. We can write this out in a seemingly strange way. \begin{align*}
  &\ \frac{(20)(19)(18)(17)(16)(15)(14)(13)(12)(11)(10)(9)(8)(7)(6)(5)(4)(3)(2)(1)}{(11)(10)(9)(8)(7)(6)(5)(4)(3)(2)(1)} \\
  &= \frac{(20)(19)(18)(17)(16)(15)(14)(13)(12)\cancel{(11)}\cancel{(10)}\cancel{(9)}\cancel{(8)}\cancel{(7)}\cancel{(6)}\cancel{(5)}\cancel{(4)}\cancel{(3)}\cancel{(2)}\cancel{(1)}}{\cancel{(11)}\cancel{(10)}\cancel{(9)}\cancel{(8)}\cancel{(7)}\cancel{(6)}\cancel{(5)}\cancel{(4)}\cancel{(3)}\cancel{(2)}\cancel{(1)}}
\end{align*}

This expression is $20!$ divided by $11!$, and gives the same as our argument from the product rule for counting directly. This is a more general result than our example with books would suggest. If we have $n$ items, and we want to choose $k$ of them taking into account the order those choices, it will always be $n!$ divided by $(n-k)!$. We call this a permutation. 

:::{#def-permutation}
## Permutations
If we wish to select $k$ items from a collection of $n$ items, where the ordering of these selections matters, then the total number is referred to as a permutation. Mathematically, $$P_{n,k} = \frac{n!}{(n-k)!}.$$
:::

Permutations arise when we select ordered subsets from a collection. We often, in combinatorial problems, talk about ordering, though sometimes what we mean by this is slightly more abstract. Suppose that you want to form a committee with $5$ different people, each of which occupies a different role: the president, vice president, treasurer, note taker, and critic. If there are $30$ people to select for this then there are $P_{30,5}$ total possible committees that can be formed. While there is not a sequential order here, we talk about this as being "ordered" since we can differentiate between the five roles. Instead of labelling them with their names, we could label them $1$ through $5$ and make the ordering more explicit. 

:::{#exm-permutations}
## Seating in a (Not Full) Coffee Shop
Still haunted by that time when the coffee shop was full, Sadie and Charles enter the coffee shop at a later date and find that, including themselves, there are only $7$ patrons in the store, and still the $10$ seats to choose from. 

a. How many different ways can the $7$ people sit in the $10$ different chairs?
b. If there are $10$ drinks on the menu, how many different ways can each person choose a chair and a drink? 
c. If the coffee shop can make only one of each drink, how does the previous total change?

::::{.callout .solution collapse='true'}
## Solution
a. In this case we are looking to order subsets from a total collection. If we line up the $7$ patrons we then need to select $7$ chairs to go with them, and keep these ordered. This is simply $$P_{10,7} = \frac{10!}{3!} = 604800.$$

b. In this part of the question we know that the $7$ people have $P_{10,7}$ ways of sitting into seats, we can view this as decision one. Then, for each of these $7$ people there is a decision of what drink they will order. For these $7$ decisions, $n_2 = \cdots = n_8 = 10$. As a result, we get the total number is $$P_{10,7}\times10\times10\times\cdots\times10 = P_{10,7}\times 10^{7} = 6,048,000,000,000.$$

c. This setting, like part (b) starts with a first decision involving $P_{10,7}$ choices. Then, instead of there being $7$ more decisions with $10$ choices each, we can either view this as $7$ more choices with a descending number of options $10$ for the first, then $9$, and so on, or we can view this as a single choice where we need to select $7$ **ordered** options from the $10$ drinks available. This gives $$P_{10,7} \times P_{10,7} = 365,783,040,000,$$ total choices.
::::
:::

Factorials compute the number of orderings for a set of objects, and permutations compute the number of ordered subsets from a collection of objects. What about when we do not wish to differentiate the order of subsets? Suppose that you still need to form a $5$ person committee, but you do not have explicit roles for the different members of the committee. Here we cannot use a permutation directly, as we know that this takes into account the order. 

To determine the number of unordered subsets, we will consider a different approach for taking ordered subsets. Suppose that we formulate the ordered committee as a two step procedure. First, we select $5$ people without concern for their order. Then we choose which order they will have. If $M$ represents the number of unordered sets of $5$ from this population, the product rule for counting tells us that the total number of ordered committees will be $M\times 5!$, since there are $5!$ arrangements of the $5$ people. Thus, we can write this down as $$P_{30,5} = \frac{30!}{25!} = M\times 5! \implies M = \frac{30!}{25!5!}.$$ 

This will be true far more broadly than our committee example. If we want to select $k$ items from a collection of $n$, we will have $n!$ divided by the product of $k!$ and $(n-k)!$. We refer to these as combinations.

:::{#def-combinations}
## Combinations
If we wish to select $k$ items from a collection of $n$ items, where the ordering of these selections does not matter, then the total number is referred to as a combination. Mathematically, $$\binom{n}{k} = \frac{n!}{k!(n-k)!}.$$
:::

We read $\binom{n}{k}$ as "$n$ choose $k$", which translates to "select $k$ items from a population of $n$ total options, without concern for their order."

To summarize: factorials allow us to order a complete collection, permutations allow us to select a subset with consideration of the ordering, and combinations allow us to select a subset from the collection without regard to the order. These three techniques can be used in combination with the product rule for counting to allow us to have very complex total summations. 

:::{#exm-combinations}
## Changing the Seating in the Coffee Shop
Some nights, the coffee shop hosts local music acts. Because of the added equipment, the coffee shop owners only keep out the number of seats that are going to be required based on the number of tickets that were sold. 

a. If there are $8$ tickets sold, how many different combinations of the $10$ chairs can get left out?
b. Suppose that only $6$ people end up showing up. How many different ways can the $6$ people sit in the $8$ chairs that are being selected from the $10$ total possibilities?

::::{.callout .solution collapse='true'}
## Solution
a. Here, there is no ordering for the $8$ chairs that are to be selected. As a result, we are simply looking for how many chairs can be selected from a group of $10$ of them. This is $$\binom{10}{8} = \frac{10!}{8!\cdot 2!} = 45.$$

b. With the first choice having $45$ possible options, the second choice that needs to be made is how $6$ people sit into $8$ chairs. Here the ordering *does* matter, since the chairs are distinguishable. As a result, this is a permutations question, with there being a total of $P_{8,6} = \frac{8!}{2!}$ ways of having the people select their seats. In total then there are $$\binom{10}{8}\times P_{8,6} = \frac{10!}{8!\cdot 2!}\cdot\frac{8!}{2!} = 907,200.$$
::::
:::

### Less Common Counting Techniques
While most of the problems we address will revolve around permutations and combinations (with heavy use of the product rule), there are additional techniques which are important to know (and recognize when to use). In particular, combinations and permutations each assume that we are sampling from our set **without replacement**. That is, each time you select an item, it is removed from the population. These are the most common situations in these combinatorial problems, however, there are *some* situations which arise where we need to count the number of ordered or unordered subsets *with* replacement. 

#### Ordered Subsets with Replacement
Consider, for instance, forming a password using only lowercase numbers and letters. If you decide on a fixed length for the password, then there are going to be $36$ choices at each decision point, and you want to take an ordered subset of these. This is forming an **ordered subset with replacement**, and to count how many different ways there are of doing this, we can simply use the product rule. That is, you have $36$ choices at each decision point, and so there are $36\times 36\times\cdots\times 36 = 36^k$ total decisions, where $k$ is the number of items to select. 

In general, if you have $n$ total items and you want to make an ordered set of $k$ of these items **with replacement** you will have $n^k$ total ways of doing this.

#### Unordered Subsets with Replacement
Forming unordered sets with replacement is slightly less intuitive. Consider rolling $k$ dice which are not distinguishable from one another. We know that there are $6$ total sides that can show up on each of these dice, but how many different combinations of numbers can show up overall? If the dice can be distinguished we would say that there are $6^k$ possible ways of doing this. However, some of these combinations are going to be equivalent in the unordered world. Take the simple case of $k=2$. Here we have the following possibilities: \begin{align*}
(1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6)&\\ 
(2, 2), (2, 3), (2, 4), (2, 5), (2, 6)&\\ 
(3, 3), (3, 4), (3, 5), (3, 6)&\\ 
(4, 4), (4, 5), (4, 6)&\\ 
(5, 5), (5, 6)&\\ 
(6,6)&
\end{align*}

This gives a total of $21$ possible combinations, rather than $36$. 

In general, if we want to find the way of selecting $k$ elements *with replacement* from a total of $n$, then the number of ways of doing this will be $$\binom{n+k-1}{k}.$$ In our example this gives $\binom{6+2-1}{2} = 21$.

#### Permutations with Identical Objects
Finally, it is worth understanding how to handle identical objects in combinatorial problems. Suppose that, of the $10$ books that we wish to place on a shelf, we have $2$ copies of one of them, $3$ copies of another one, and the other $5$ have one copy each. Supposing that there is no way to tell these identical objects apart, how many ways can we arrange the bookshelf? 

First, if we pretend that all of the items are actually able to be differentiated then there are $10!$ ways of placing these books. Now, in any of these permutations, had we swapped the order of the first book (with $2$ copies) the ordering would have been indistinguishable. As a result, for every ordering of the $8$ other books, we counted that permutation twice (when it should have only been counted once!). So to address the two repeated copies we need to take $10!/2$. Now, a similar argument is going to hold for the book with $3$ repeated copies. However, instead of there being $2$ permutations which are identical, there are going to be $3! = 6$ permutations which are identical. This is because we can reorder the $3$ copies of the book in anyway we choose, and still wind up with the same overall permutation.^[In fact, the reason that there are $2$ ways of doing this with the book with $2$ copies is since $2! = 2$.] As a result, the total number is going to be $$\frac{10!}{2!\cdot3!} = 302400.$$ 

We can see this same result through an alternative construction. First, we select which of the $10$ slots should have the first book. We do not care about the order, and so there are $\binom{10}{2}$ ways of doing this. Next, we can select which of the $8$ remaining slots should have the second book. Like the first one there will be $\binom{8}{3}$ ways of placing these. Now, there are $5$ slots remaining, and $5$ books to place, so as a result, we can order those in $5!$ different ways, and then slot them into the remaining places in order. This gives, in total $$\binom{10}{2}\binom{8}{3}(5!) = \frac{10!}{2!\cancel{8!}}\cdot\frac{\cancel{8!}}{3!\cancel{5!}}\cdot\cancel{5!} = \frac{10!}{2!3!}.$$

To generalize this, if we want to order $n$ elements, such that there are $k$ distinguishable elements with $n_1$ of the first type, $n_2$ of the second, and so forth until $n_k$ of the last type ($n = n_1 + n_2 + \cdots + n_k$), then the total number of orderings will be $$\frac{n!}{n_1!\cdot n_2!\cdots n_k!}.$$

## From Combinatorics to Probability
While combinatorics is a field of study on its own, with many intriguing tools and developments surrounding the enumeration of objects, for the purposes of simple probability models these tools will suffice. Ultimately, we care about counting since in the equal probability model, the probability of any event can be determined by counting the number of ways that the event can occur and dividing by the total number of outcomes that are possible. That is, we use these tools to derive $N_A$, the total number of ways that $A$ can occur, and $N$, the total number of experimental outcomes, and then we conclude that $$P(A) = \frac{N_A}{N}.$$

:::{#exm-poker-hands}
## Poker Hand Counts
During one of their conversations, Charles and Sadie were remarking how they never really played poker. As they understand it, in poker you are dealt a hand of $5$ cards and you want to use these $5$ cards to try to match certain sets of cards, some of which are more rare than others. Charles and Sadie start to get hung-up on discussions regarding "straights" and "flushes". 

A straight is any sequence of $5$ cards in ascending order (where aces can be low, or high). For instance, $7, 8, 9, 10, \text{J}$ of any suit. A flush, is any set of $5$ cards belonging to the same suit. Charles just *feels* that straights have to be more rare than flushes. 

a. How many different straights are there from a standard deck of cards?
b. How many different flushes are there from a standard deck of cards?
c. If dealt $5$ cards at random, what is the probability of a flush? What is the probability of a straight?
d. A straight flush occurs when you have $5$ cards in order, of the same suit. What is the probability of a straight flush?
e. If straight flushes were not counted as flushes, and not counted as straights, how do the probabilities of either hand change?

::::{.callout .solution collapse='true'}
## Solution
```{r}
#| echo: false
k<- 1:13
flush<-4*factorial(13)/(factorial(k)*factorial(13-k))
straight<-10*4^k
output_str<-paste0("k=", k, " gives ", round(straight/flush, 3), collapse=". ")
```
a. A straight necessitates drawing five cards in order, with each of any suit. Just as with the straight flush, there are $10$ possible starting values for the straight. Once we have selected the starting value, then for each of the five cards we can pick any of the four suits, resulting in $4$ choices each. That gives $$N_A = 10\times 4\times 4\times 4\times 4\times 4 = 10\times 4^5 = 10240.$$

b. A flush necessitates drawing **any** five cards from the same suit. If we had a suit fixed, there would be ${13\choose 5}$ ways of doing this, since we do not care about ordering. If we think about first choosing the suit, we have $4$ ways of doing that, resulting in $$N_A = 4 \times {13 \choose 5} = 5148.$$ 

c. To find the probabilities of each of these, we need to know the total number of $5$ card hands. We do not consider order, and so $N = \binom{52}{5} = 2598960$. Then, the probability is simply the number of combinations (calculated above) divided by the number of hands. This gives, for straights, $$P(A) = \frac{10240}{2598960} = \frac{128}{32487} \approx 0.00394,$$ and for flushes, $$P(A) = \frac{5148}{2598960} = \frac{33}{16660} \approx 0.00198.$$ As a result, we see that straights are roughly twice as common as flushes are.^[This is *still* a deeply unintuitive result to me. To me it feels like it should be harder to get a nice ordering of cards all in a row than it is to find ones of the same suit. And yet, it is about twice as likely to have the straights. Now, if you think about this deeply, it makes a lot of sense: there is a lot more leeway in selecting the straight than the flush. However, my brain refuses to accept this as intuitive. This is something that can often occur in probability questions where the true results can be far from what we would expect. An interesting question is how long does a straight have to be to be less likely than a flush of the same length. Note that the number of ways of choosing a flush will always be $4\times\binom{13}{k}$, and the number of ways of forming a straight will always be $10\times 4^k$. If we consider $k$ from $1$ to $13$, we can take the ratios of these to see just how much more (or less) likely a straight is. Doing this gives: [`r output_str`.]{.font-monospace .text-light .bg-dark .m-2 .p-2 .block} Up to (and including) $4$ cards the straight is indeed harder to achieve. However, this does not last long!]

d. Note that to form a straight flush, we first have to fix a suit. There are ${4\choose 1}=4$ total ways of doing this. Next, we need to pick which starting value we will use. Once a card has been selected as a starting value, the remaining cards are fixed. The start value ranges from A through to $10$. Correspondingly, we have $$N_A = {4\choose 1}{10 \choose 1} = 4\times10 = 40.$$ As a result, we get that $$P(A) = \frac{40}{2598960} = \frac{1}{64974}.$$

e. From (d) exactly $40$ of the straights and $40$ of the flushes are also straight flushes. We can thus remove $40$ from the totals of each of these, giving $\frac{10200}{2598960}$ for straights and $\frac{5108}{2598960}$ for flushes.
::::
:::

### References  {.unnumbered}