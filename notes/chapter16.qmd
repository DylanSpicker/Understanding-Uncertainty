# Hypothesis Testing and Confidence Intervals in Two Populations

With confidence intervals and hypothesis tests we began to make inferences regarding populations by quantifying our levels of uncertainty. Quantifying uncertainty sits at the core of inferential statistics, and through this we are able to effectively draw conclusions and learn about populations. With that said, the procedures discussed so far have focused on individual parameters from individual populations. While there are many scientific questions relating to a single parameter from a single population, it is perhaps more common to consider the comparison between two populations.^[For instance, we may wish to compare two different treatments or two different interventions; we may wish to consider whether one group is different from another, or whether one process performs the same as another; etc.] To answer questions relating to the comparison of multiple populations, we must extend the ideas of hypothesis testing and confidence intervals to the setting where we have samples from two (or more) populations. 

## Two Populations Rather than One

In the one population setting we supposed that we observed $X_1,\dots,X_n$ as an independent and identically distributed sample from some population. We then considered parameters of this population^[Such as $\mu$ or $\sigma^2$.] and addressed questions relating to these parameters. In the two population setting, we introduce a second independent and identically distributed sample, $Y_1,\dots,Y_m$, from a second population. The distribution for the $Y$s may or may not be the same distribution as the distribution for the $X$s. Moreover, the distribution of $X$ and of $Y$ may be independent of one another, or they may be related to one another. We will take $F_X$ and $F_Y$ to represent the distribution of $X$ and the distribution of $Y$, respectively, and we will use the same notational conventions for the parameters of these distributions. So, for instance, we may take $\mu_X$ and $\mu_Y$ to represent the population means, or $\sigma_X^2$ and $\sigma_Y^2$ to represent the population variances. The size of the samples we draw may be equal, or not. We assume that there are $n$ observations taken from $F_X$ and there are $m$ observations taken from $F_Y$. Each of these samples could be used for individual hypothesis testing or estimation, using the procedures that we have previously seen.

We will typically be concerned with the same parameters and estimators we have been considering previously. For instance, it is common to consider the population means. To this end we have $\mu_X$, which can be estimated by $\overline{X}$, and we have $\mu_Y$, which can be estimated by $\overline{Y}$. Even if $\mu_X = \mu_Y$ in practice, it is unlikely that we will observe $\overline{X} = \overline{Y}$, exactly. This may lead us to consider whether we can perform a hypothesis test that asks whether, based on the observed information, the two populations have exactly the same mean. 

To do this, we want to test $H_0: \mu_X = \mu_Y$ versus the alternative, $H_A: \mu_X \neq \mu_Y$. In order to determine how to test this hypothesis, it is helpful to re-express it as a single parameter. Notice that if $\mu_X = \mu_Y$, then $\mu_X - \mu_Y = 0$. We can rewrite the previous hypotheses as $H_0: \mu_X - \mu_Y = 0$ and $H_A: \mu_X - \mu_Y \neq 0$. If we define a new parameter, $\Delta = \mu_X - \mu_Y$, such that $\Delta$ represents the difference in mean values for the two distributions,^[Note, $\Delta = E[X] - E[Y]$ is a population parameter, one that depends on the joint distribution of $X$ and $Y$.] then we can state our hypothesis test as $H_0: \Delta = 0$ versus $H_A: \Delta \neq 0$. 

This test is no different from any of the tests we have seen before. If we can work out an estimator for $\Delta$, and work out the null distribution for this estimator, we can use the procedures outlined for null hypothesis significance testing to assess how much evidence exists against the hypothesis that the means are equal. Similarly, if we are able to work out an estimator for $\Delta$, and we can identify the sampling distribution of this estimator, then we can form confidence intervals for $\Delta$, allowing both point and interval estimation to proceed for the difference in population means. 

In order to estimate $\Delta$, it is helpful to consider that $\Delta = \mu_X - \mu_Y$. We know that $\overline{X}$ is an effective estimator for $\mu_X$ and that $\overline{Y}$ is an effective estimator for $\mu_Y$. Taken together, it is reasonable to suggest that an estimator for $\Delta$ is given by $$\widehat{\Delta} = \overline{X} - \overline{Y}.$$ Since both $\overline{X}$ and $\overline{Y}$ are unbiased for $\mu_X$ and $\mu_Y$, then $\widehat{\Delta}$ will also be unbiased for $\Delta$. If we are able to assess the null and sampling distributions of $\widehat{\Delta}$, we can leverage the same procedures for confidence intervals and hypothesis tests we have already discussed. Determining these distributions depends on the assumptions made regarding the distributions $F_X$ and $F_Y$, as well as the relationship between them.

Note that the same general approach could have been used to test hypotheses beyond the equality of the population means. For instance, suppose we wanted to know whether the mean of $X$ was $10$ units larger than the mean of $Y$. This corresponds to $\mu_X = \mu_Y + 10$, or $\mu_X - \mu_Y = 10$, and so we can test it using $H_0: \Delta = 10$ versus the alternative $H_0: \Delta \neq 10$. Alternatively, what if we had hypothesized that $\mu_X$ was really half as large as $\mu_Y$? In this case, we get that $2\mu_X = \mu_Y$, or $2\mu_X - \mu_Y = 0$. Now, if we introduce $\Delta_{2} = 2\mu_X - \mu_Y$ then we can test $H_0: \Delta_2 = 0$ versus $H_A: \Delta_2 \neq 0$. To estimate $\Delta_2$, we can note that $2\mu_X$ is estimated well by $2\overline{X}$, and so $$\widehat{\Delta}_2 = 2\overline{X} - \overline{Y},$$ is a reasonable estimator.^[It will be unbiased for the same reasons that $\widehat{\Delta}$ was.]

Beyond testing additional hypotheses related to the mean difference, we can also test alternative parameters in the distribution using a similar approach. Suppose we have two separate binomial distributions. Here we may have $p_X$ and $p_Y$ as the relevant proportions, and we may be interested in testing whether $p_X = p_Y$ or not. To do so, we can use the exact same procedure, taking $\Delta = p_X - p_Y$, and testing $H_0: \Delta = 0$ versus $H_A: \Delta \neq 0$.^[Or any of the other variations on this test.] Because $\widehat{p} = \overline{X}$, we can use the exact same estimator, $\widehat{\Delta}$, and continue in the same process outlined above. Alternatively, we may consider the variances of the population. By framing $\sigma_X^2 = \sigma_Y^2$, we can follow a similar procedure, either taking the difference $\Delta = \sigma_X^2 - \sigma_Y^2$, or, more commonly, the ratio, $\rho = \dfrac{\sigma_X^2}{\sigma_Y^2}$, and then testing hypothesis such as $H_0: \Delta = 0$ or $H_0: \rho = 1$, versus the alternatives $H_A: \Delta \neq 0$ or $H_A: \rho \neq 1$. 

The key idea when dealing with two samples from two populations is to frame the question in terms of a parameter of the joint distribution. Then, estimation (point and interval), and hypothesis testing can proceed on the basis of the single parameter that has been identified. At this point, it is a matter of working out the sampling distribution, and implementing the same procedures explored in the one sample case. The difficulty lies in determining the sampling and null distributions of these estimators. 

## Hypothesis Tests and Confidence Intervals for Mean Differences in Independent Populations
In order to determine the sampling and null distribution when dealing with two samples of data, a key assumption that needs to be clarified is whether or not the populations are independent of one another. If we know that $X \perp Y$, then it is typically more straightforward to derive the sampling and null distributions. Consider, specifically, $\Delta = \mu_X - \mu_Y$. As discussed, we can estimate $\Delta$ using $\widehat{\Delta} = \overline{X} - \overline{Y}$. We have seen that $$\overline{X} \sim N(\mu_X, \sigma_X^2) \quad\text{ and }\quad \overline{Y} \sim N(\mu_Y, \sigma_Y^2),$$ where these are the exact distributions if the populations are normal, and these hold approximately in large samples otherwise. Note that, regardless of the dependence or independence of $X$ and $Y$, it will always be the case that $$E[\widehat{\Delta}] = E[\overline{X}] - E[\overline{Y}] = \mu_X - \mu_Y = \Delta.$$ That is, $\widehat{\Delta}$ is unbiased for $\Delta$. If we are willing to assume that $X \perp Y$, then $$\text{var}(\widehat{\Delta}) = \text{var}(\overline{X}) + \text{var}(\overline{Y}) = \frac{\sigma_X^2}{n} + \frac{\sigma_Y^2}{m}.$$ 

If the two samples we take are independent of one another, then we can conclude that $$\widehat{\Delta} \sim N(\Delta, \frac{\sigma_X^2}{n} + \frac{\sigma_Y^2}{m}).$$ This result immediately allows for the calculation of confidence intervals, using the normal based confidence intervals previously investigated, whenever the variance terms, $\sigma_X^2$ and $\sigma_Y^2$ are known. If these variances are unknown, then the variance can be estimated using the individual sample variances, $S_X^2$ and $S_Y^2$. Replacing $\sigma_X^2$ and $\sigma_Y^2$ with these results in an underlying $t$ distribution, with $\nu$ degrees of freedom, where $$\nu = \frac{\left(\frac{S_X^2}{n} + \frac{S_Y^2}{m}\right)^2}{\frac{(S_X^2/n)^2}{n-1} + \frac{(S_Y^2/m)^2}{m-1}}.$$ Note that this is typically a fairly complicated expression, and so instead we will often take $\nu = \min\{n-1,m-1\}$. Then, confidence intervals for $\widehat{\Delta}$ can be formed using the $t$ distribution, following the same procedure as our other $t$ distribution confidence intervals. 

:::{.callout-tip icon="false"}
## Confidence Intervals for Differences of Means in Independent Populations

If we wish to estimate the difference of two population means, $\mu_X - \mu_Y$, from populations that are independent of one another, then we can form confidence $Z$- or $t$-based confidence intervals, depending on whether the variances are known or not.

1. If $\sigma_X^2$ and $\sigma_Y^2$ are known, then $$\overline{X} - \overline{Y} \pm Z_{\alpha/2}\sqrt{\frac{\sigma_X^2}{n} + \frac{\sigma_Y^2}{m}}.$$
2. If $\sigma_X^2$ and $\sigma_Y^2$ are unknown, then $$\overline{X} - \overline{Y} \pm t_{\nu, \alpha/2}\sqrt{\frac{S_X^2}{n} + \frac{S_Y^2}{m}} \quad\text{where}\quad \nu = \frac{\left(\frac{S_X^2}{n} + \frac{S_Y^2}{m}\right)^2}{\frac{(S_X^2/n)^2}{n-1} + \frac{(S_Y^2/m)^2}{m-1}}.$$ Here, $\nu$ is often taken to be $\min\{n-1, m-1\}$ instead.
:::

Under the null hypothesis that $\Delta = \Delta_0$, for some constant $\Delta_0$, we can transform the sampling distribution to the null distribution by noting that the mean will be $\Delta_0$, and the variance will remain unchanged. Then, depending on whether the variances are assumed to be known, or not, we can use a $Z$-test or a $t$-test for hypothesis testing. 

:::{.callout-tip icon="false"}
## Hypothesis Tests for Differences of Means in Independent Populations

If we have data selected from two, independent populations (denoted $X_1,\dots,X_n$ and $Y_1,\dots,Y_m$), and we wish to test hypotheses relating to the relationship in population means, we can use a $Z$- or $t$-test, based on our knowledge of the population variances.

1. If $\sigma_X^2$ and $\sigma_Y^2$ are known, then $$T = \frac{\overline{X} - \overline{Y} - \Delta_0}{\sqrt{\frac{\sigma_X^2}{n} + \frac{\sigma_Y^2}{m}}} \stackrel{H_0}{\sim} N(0, 1).$$ Assuming that we observe $T=t$,
    i. If $H_0: \Delta = \Delta_0$ versus $H_A: \Delta \neq \Delta_0$ then the $p$-value is $2\Phi(-|t|)$. The critical value is $Z_{1-\alpha/2}$.
    ii. If $H_0: \Delta \geq \Delta_0$ versus $H_A: \Delta < \Delta_0$ then the $p$-value is $\Phi(t)$. The corresponding critical value is $Z_{\alpha}$.
    iii. If $H_0: \Delta \leq \Delta_0$ versus $H_A: \Delta > \Delta_0$ then the $p$-value is $1 - \Phi(t)$. The corresponding critical value is $Z_{1-\alpha}$.
2. If the variances are unknown, then we take $$T = \frac{\overline{X} - \overline{Y} - \Delta_0}{\sqrt{\frac{S_X^2}{n} + \frac{S_Y^2}{m}}} \stackrel{H_0}{\sim} t_\nu,\quad\text{with}\quad\nu = \frac{\left(\frac{S_X^2}{n} + \frac{S_Y^2}{m}\right)^2}{\frac{(S_X^2/n)^2}{n-1} + \frac{(S_Y^2/m)^2}{m-1}}.$$ Taking $F(t)$ to be the cumulative distribution function for the $t_\nu$ distribution, and assuming we observe $T=t$,
    i. If $H_0: \Delta = \Delta_0$ versus $H_A: \Delta \neq \Delta_0$ then the $p$-value is $2F(-|t|)$. The critical value is $t_{\nu, 1-\alpha/2}$.
    ii. If $H_0: \Delta \geq \Delta_0$ versus $H_A: \Delta < \Delta_0$ then the $p$-value is $F(t)$. The corresponding critical value is $t_{\nu, \alpha}$.
    iii. If $H_0: \Delta \leq \Delta_0$ versus $H_A: \Delta > \Delta_0$ then the $p$-value is $1 - F(t)$. The corresponding critical value is $t_{\nu, 1-\alpha}$.
:::


### Pooled Variance Estimation
Sometimes, even when $\sigma_X^2$ and $\sigma_Y^2$ are unknown, it is reasonable to assume that the variances of the two populations will be equal. That is, we may have good reason to suspect that $\sigma_X^2 = \sigma_Y^2$, even if we do not know the value of $\sigma_X^2$.^[This can happen, for instance, when you are measuring the same trait in two different populations, but where the variability of the trait is quite well understood.] In these settings, instead of taking two separate variances, we ultimately have a single parameter, $\sigma^2$. When this happens, we can revisit the sampling distribution, noting that $$\overline{X} - \overline{Y} \sim N\left(\Delta, \sigma^2\left[\frac{1}{n}+\frac{1}{m}\right]\right).$$ If $\sigma^2$ is known, then the previously discussed sampling and null distributions are valid, taking $$\frac{\sigma_X^2}{n} + \frac{\sigma_Y^2}{m} = \sigma^2\left(\frac{1}{n} + \frac{1}{m}\right).$$

If the variance is unknown, it still needs to be estimated using the data. This can be done using the **pooled variance estimator**, $S_p^2$. Namely, $$S_p^2 = \frac{(n-1)S_X^2 + (m-1)S_Y^2}{n+m-2}.$$ Leveraging this pooled variance estimator gives $$\frac{\overline{X} - \overline{Y} - \Delta}{S_p\sqrt{\frac{1}{n} + \frac{1}{m}}} \sim t_{n+m-2},$$ and the equivalent null distribution when $\Delta$ is replaced by $\Delta_0$. Note that here there are $n+m-2$ degrees of freedom. Otherwise, the same procedures -- either using the $Z$-based or $t$-based intervals or tests -- can proceed with the pooled variance in place of the independent variances. 

The pooled variance estimator represents an anti-conservative approach to quantifying uncertainty. That is, if the same analysis is done with the pooled variance and with the unpooled variances, you should expect narrower confidence intervals and smaller $p$-values if the pooled variance is used. This can be an effective way of making complete use of prior knowledge, when the variances of the populations are actually equal. However, in the event that the variances are not actually equal, using the pooled variances will lead to erroneous conclusions. For this reason, you should rely on subject-matter guidance to determine whether equal variances should be assumed. If there is a strong scientific basis for the equality of the variances, and the data do not seem to dramatically contradict this assumption, then using pooled variances may be reasonable. Otherwise, you should estimate the variances separately. 

### Confidence Intervals and Hypothesis Tests for Multiple Proportions
The previous discussions centered around testing population means. Implicitly, this investigated populations that are continuous, where the mean is the primary measure of interest. The same techniques apply equally well to considering two population proportions. Namely, if $X \sim \text{Bin}(n, p_X)$, and $Y \sim \text{Bin}(m, p_Y)$, it may be of interest to us to determine the relationship between $p_X$ and $p_Y$. We may wish to proceed either with the estimation (point and interval) of the difference between $p_X$ and $p_Y$, or else to test hypotheses regarding the relationship of $p_X$ and $p_Y$. Recall that in the case of a single proportion we made use of the fact that, for large samples, the sample proportion will be approximately normally distributed. The same general technique applies in the case of two populations.

Since both $\widehat{p}_X$ and $\widehat{p}_Y$ are approximately normally distributed, their difference will also be approximately normally distributed. Supposing that we are willing to assume that $X\perp Y$, then $$\widehat{p}_X - \widehat{p}_Y \sim N\left(p_X - p_Y, \frac{p_X(1-p_X)}{n} + \frac{p_Y(1-p_Y)}{m}\right).$$ This sampling distribution allows for the construction of approximate confidence intervals for the difference in proportions. 

:::{.callout-tip icon="false"}
## Confidence Intervals for Differences of Proportions in Independent Populations

To estimate the difference of two population proportions, $p_X - p_Y$, from populations that are independent of one another, we can form confidence $Z$-based confidence intervals. Supposing that $n$ and $m$ are both sufficiently large, take $$\widehat{p}_X - \widehat{p}_Y \pm Z_{\alpha/2}\sqrt{\frac{\widehat{p}_X(1-\widehat{p}_X)}{n} + \frac{\widehat{p}_Y(1-\widehat{p}_Y)}{m}}.$$

If there is a desire to ensure that the computed interval is conservative, then $\widehat{p}_X$ and $\widehat{p}_Y$ can both be replaced by $0.5$. This will maximize the standard error, and will ensure coverage of the interval even if the estimated proportion is off in the current sample. 
:::

The same approach can be extended to test hypotheses relating to the population differences. If we consider the hypothesis $H_0: p_X - p_Y = 0$, then assuming the null hypothesis holds we know that $p_X = p_Y = p$, for some value $p$. If this is the case then the variance of $\widehat{p}_X - \widehat{p}_Y$ can be expressed as $$\text{var}(\widehat{p}_X - \widehat{p}_Y) = p(1-p)\left(\frac{1}{n} + \frac{1}{m}\right).$$ This variance is maximized by taking $p = 0.5$, and so we can ensure conservative hypothesis tests of the hypothesis $H_0: p_X - p_Y = 0$ (or the one-sided alternatives). On the other hand, suppose that the null hypothesis was given by $p_X - p_Y = c$, for $c \neq 0$. Then, $p_X = p + c$ and $p_Y = p$, for some proportion $p$. In this setting, $$\text{var}(\widehat{p}_X - \widehat{p}_Y) = \frac{(p+c)(1-(p+c))}{n} + \frac{p(1-p)}{m}.$$ This does not simplify in the way that the variance expression simplifies under the null hypothesis of equality. Further, this expression is not easily maximized over $p$. Without careful choice of $p$ this approximation tends to perform rather poorly. As a result, if we wish to test $H_0: p_X - p_Y = c$, for any $c \neq 0$, then we require alternative procedures, not relying on the normal approximation. 

:::{.callout-tip icon="false"}
## Hypothesis Tests for Equality of Proportions in Two Independent Populations

To test $H_0: p_X - p_Y = 0$ versus the alternative $H_A: p_X - p_Y \neq 0$, using data from two independent populations, with success proportions $p_X$ and $p_Y$ respectively, we can use a $Z$-test based on $$T = \frac{\widehat{p}_X - \widehat{p}_Y}{0.5\sqrt{\frac{1}{n} + \frac{1}{m}}} \stackrel{H_0}{\sim} N(0,1).$$ Then, the two-tailed $p$-value can be computed as $2\times\Phi(-|t|)$, when $T=t$.

* To test $H_0: p_X - p_Y \geq 0$ versus the alternative $H_A: p_X - p_Y < 0$, the $p$-value will be given by $\Phi(t)$.
* To test $H_0: p_X - p_Y \leq 0$ versus the alternative $H_A: p_X - p_Y > 0$, the $p$-value will be given by $1 - \Phi(t)$.
:::

## The Analysis of Paired Data
When the populations are independent from one another, regardless of whether a common variance is assumed or not, the two-population problem can be transformed into a question regarding a single parameter by adequately adjusting the sampling and the null distributions. Ultimately, this results in the same $Z$- and $t$-based procedures introduced previously. It will often be the case, however, that we cannot assume independence between the populations. If we cannot assume independence between the two populations, there will not be a single method of proceeding with investigating mean differences. Instead, our results will depend on the manner in which the populations are dependent. While many different forms of dependence may be of interest, a common assumption is that the populations are **paired**. By paired we mean that $X_i$ and $Y_i$ are observations that are naturally connected to one another, for instance, by being measurements of different quantities on the same unit, or measurements of the same unit overtime, or similar. When data are paired, it will always be the case that the samples are of the same size, taking $n=m$.

:::{#def-paired-data}
## Paired Data
Paired data refer to observations in a dataset that are linked to one another through some underlying, natural connection. Data may be paired because they are taken on the same unit, or because the observations are connected in another meaningful way. In paired data, there is an obvious and meaningful one-to-one correspondence between measurements of one variable and another. 
:::

Suppose, for instance, that we measure the same trait at two different points in time, for all the same units. Then $X_1,\dots,X_n$ correspond to the first measurements that are taken, and $Y_1,\dots,Y_n$ correspond to the second. Here, $X_1$ and $Y_1$ are naturally paired since they are the same unit observed at two points in time. We expect that $X_1$ and $Y_1$ will be related to one another, though, not exactly the same. The same can be said for $X_2$ and $Y_2$, and more generally for $X_i$ and $Y_i$. When data are paired they are not independent.

Suppose that, with paired data, we are concerned with hypotheses regarding mean differences, considering for instance $H_0: \mu_X - \mu_Y = 0$. In independent samples we considered the difference of sample means as an estimator, $\overline{X} - \overline{Y}$, where the samples of $X$ and $Y$ could have been of possibly different sizes. If we consider the same quantities when data are paired, we can rewrite the expression as follows: \begin{align*}
\overline{X} - \overline{Y} &= \frac{1}{n}\sum_{i=1}^n X_i - \frac{1}{n}\sum_{i=1}^n Y_i \\
&= \frac{1}{n}\sum_{i=1}^n (X_i - Y_i) \\
&= \frac{1}{n}\sum_{i=1}^n D_i.
\end{align*} Here, $D_i = X_i - Y_i$ is the difference between $X$ and $Y$ within each pair. In other words, when data are paired, we can first consider the differences between observations, giving a set of $n$ observations, $D_1, \dots, D_n$. Then, using this single sample, we can construct confidence intervals or perform hypothesis tests. 

Once the sample of differences is formed, the problem of interval estimation or hypothesis testing is no different from the one sample procedures outlined. The sampling distribution of $\overline{D}$ is exactly normal if both $X_i$ and $Y_i$ are exactly normal, and it will be approximately normal using the Central Limit Theorem if $n$ is sufficiently large. The expected value, $E[\overline{D}] = \mu_X - \mu_Y$, according to the exact same logic applied for the independent case. The only slight difficulty is with the variance, where notably $$\text{var}(\overline{D}) = \frac{\sigma_X^2 + \sigma_Y^2 - 2\sigma_{XY}}{n}.$$ Here $\sigma_{XY}$ is the **covariance** between $X$ and $Y$ (@def-covariance).^[As a reminder, the covariance between two random variables is a measure of their relationship. A positive covariance means that there is a positive relationship between the quantities, where $Y$ increases as $X$ increases. A negative has the opposite effect. If two random variables are independent, then they have covariance $0$.] Typically, for paired data, $\sigma_{XY} > 0$ and so the variance will be **smaller** than when it is assumed that $X$ and $Y$ are independent of one another. Because of this, making the assumption that the data are paired is an anti-conservative assumption. There needs to be good reason to suppose that $X$ and $Y$ really are paired, and that they should be analyzed in this manner, as otherwise the $p$-values may be artificially deflated. 

:::{.callout-tip icon="false"}
## Confidence Intervals for Differences of Means in Paired Populations

To estimate the difference of two population means, $\mu_X - \mu_Y$, from populations that are paired with one another, we can form confidence $Z$- or $t$-based confidence intervals, depending on whether the variances are known or not. First, we find the paired differences, $D_i = X_i - Y_i$, and then consider the sample $D_1, \dots, D_n$ as a single sample. The variance of $D_i$ is given by $$\text{var}(D_i) = \sigma_D^2 = \sigma_X^2 + \sigma_Y^2 - 2\sigma_{XY}.$$

1. If $\sigma_D^2$ is known, take $$\overline{D} \pm Z_{\alpha/2}\frac{\sigma_D}{\sqrt{n}}.$$
2. If $\sigma_D^2$ is unknown, take $$\overline{D} \pm t_{n-1, \alpha/2}\frac{S_D}{\sqrt{n}}.$$ 
:::

:::{.callout-tip icon="false"}
## Hypothesis Tests for Differences of Means in Paired Populations

If we have data selected from two, paired populations (denoted $X_1,\dots,X_n$ and $Y_1,\dots,Y_m$), to test hypotheses relating to the relationship in population means, we can use a $Z$- or $t$-test, based on our knowledge of the population variances. First, we find the paired differences, $D_i = X_i - Y_i$, and then consider the sample $D_1, \dots, D_n$ as a single sample. The variance of $D_i$ is given by $$\text{var}(D_i) = \sigma_D^2 = \sigma_X^2 + \sigma^2 - 2\sigma_{XY}.$$

1. If $\sigma_D^2$ is known, then we take $$Z = \frac{\overline{D} - D_0}{\sigma_D/\sqrt{n}} \stackrel{H_0}{\sim} N(0, 1).$$ If we observe $T=t$,
    i. If $H_0: D = D_0$ versus $H_A: D \neq D_0$ then the $p$-value is $2\Phi(-|t|)$. The critical value is $Z_{1-\alpha/2}$.
    ii. If $H_0: D \geq D_0$ versus $H_A: D < D_0$ then the $p$-value is $\Phi(t)$. The corresponding critical value is $Z_{\alpha}$.
    iii. If $H_0: D \leq D_0$ versus $H_A: D > D_0$ then the $p$-value is $1 - \Phi(t)$. The corresponding critical value is $Z_{1-\alpha}$.
2. If $\sigma_D^2$ is unknown, then we take $$T = \frac{\overline{D} - D_0}{S_D/\sqrt{n}} \stackrel{H_0}{\sim} t_{n-1}.$$ Taking $F(t)$ to be the cumulative distribution function for the $t_{n-1}$ distribution, and assuming we observe $T=t$,
    i. If $H_0: D = D_0$ versus $H_A: D \neq D_0$ then the $p$-value is $2F(-|t|)$. The critical value is $t_{\nu, 1-\alpha/2}$.
    ii. If $H_0: D \geq D_0$ versus $H_A: D < D_0$ then the $p$-value is $F(t)$. The corresponding critical value is $t_{\nu, \alpha}$.
    iii. If $H_0: D \leq D_0$ versus $H_A: D > D_0$ then the $p$-value is $1 - F(t)$. The corresponding critical value is $t_{\nu, 1-\alpha}$.
:::

## Exercises {.unnumbered}
:::{#exr-16.01}
The electric lightbulbs of manufacturer $A$ have mean lifetime of $1400$ hours with a standard deviation of $200$ hours, while those from manufacturer $B$ have mean lifetime $1200$ hours with a standard deviation of $100$ hours. If random samples of $125$ bulbs of each brand are tested, what is the probability that the brand $A$ bulbs will have mean lifetime which is:

a. $160$  hours more than brand $B$?
b. $250$ hours more than brand $B$?
:::

:::{#exr-16.02}
A study of the effectiveness of giving blood plasma containing complement component C4A to pediatric cardiopulmonary bypass patients measured the average stay of patients. Of the $58$ receiving the plasma the average length was $8.5$ days, with a standard deviation of $1.9$. Of the $58$ not receiving the plasma, the average length was $11.9$ days with a standard deviation of $3.6$ days. Is there a statistically significant difference here?
:::

:::{#exr-16.03}
In a sample of $482$ female spotted flounder, the average weight was $20.95$g with a standard deviation of $14.5$g. A sample of $614$ males had an average weight of $22.79$g with a standard deviation of $15.6$g. Can you conclude that the mean weight of the males is greater than that of the females?
:::

:::{#exr-16.04}
In a sample with $413$ male identified college students and $382$ female identified college students, the average number of energy drinks consumed by the males per month was $2.49$ with a standard deviation of $4.87$. Females on average consumed $1.22$ with a standard deviation of $3.24$. Can you conclude that there is a difference in energy drink consumption between males and females?
:::

:::{#exr-16.05}
In a test to compare the effectiveness of two drugs designed to lower cholesterol levels, $75$ randomly selected patients were given drug $A$ and $100$ were given drug $B$. Those given drug $A$ reduced their levels by an average of $40$ with a standard deviation of $12$, while those given $B$ reduced their levels by an average of $42$ with a standard deviation of $15$. Can you conclude that the reduction is greater with drug $B$ than with drug $A$?
:::

:::{#exr-16.06}
The National Opinion Research Center polled a sample of $92$ people aged $18-22$ in the year $2002$, asking them how many hours per week they spent on the internet. The sample mean was $7.38$ with a standard deviation of $12.83$. A second sample of $123$ was taken $2$ years later. For this sample the mean was $8.20$ and the standard deviation was $9.84$. Can you conclude that the mean number changed between $2002$ and $2004$?
:::

:::{#exr-16.07}
A crayon manufacturer is comparing the effects of two kinds of yellow dye on the brittleness of crayons. Four crayons are tested with each dye $A$ and dye $B$. The measured results are $\{1.0, 2.0, 1.2, 3.0\}$ for dye $A$, and $\{3.0, 3.2, 2.6, 3.4\}$ for dye $B$. 

a. Can you conclude the mean strength of dye $B$ exceeds that of dye $A$?
b. Can you conclude the mean strength of dye $B$ exceeds that of dye $A$ by $1$ or more?
:::

:::{#exr-16.08}
In a study of the relationship of the shape of a tablet to its dissolution time, $6$ disk- and $8$ oval-shaped tablets were dissolved. The dissolve times in seconds for the disk were $\{269.0, 249.3, 255.2, 252.7, 247.0, 261.6\}$, and for the ovals they were $\{268.8, 260.0, 273.5, 253.9, 278.5, 289.4, 261.6, 280.2\}$. Is there a difference between the mean dissolve time of each shape?
:::

:::{#exr-16.09}
Two weights, each labeled as $100$g, are weighed several times on the same scale. The results of excess weight, in $\mu$g, were $\{53, 88, 89, 62, 39, 66\}$ for the first weight, and $\{23, 39, 28, 2, 49\}$ for the second. Since the same scale was used for each weight and since the weights are similar, a common variance can be used. Can you conclude that the weights differ? 
:::

:::{#exr-16.10}
Measurements of ultimate compressive stress for green mixed oak are compared between two grades of lumber. For $11$ specimens of no. $1$ grade lumber, the average compressive stress was $22.1$ with a standard deviation of $4.09$. For $7$ specimens of no. $2$ grade lumber, the average was $20.4$ with a standard deviation of $3.08$. Can you conclude that the mean compressive stress is different between the two grades?
:::

:::{#exr-16.11}
In an experiment to test the effectiveness of a new sleeping aid, a sample of $12$ patients took the new drug and a sample of $14$ took an existing drug. Of the patients taking the new drug, the average time to fall asleep was $27.3$ minutes with a standard deviation of $5.2$, while the average time for the common drug was $32.7$ with a standard deviation of $4.1$ minutes. Can you conclude that the new drug is better than the previous in terms of mean time to sleep?
:::

:::{#exr-16.12}
The following data corresponds to measurements of latency time in milliseconds to muscle flexing when an electric impulse stimulated either the motor points or the nerves. Is there a difference in latency period between these two techniques?

| Subject | Nerve | Motor Point |
| :-----: | :---: | :---------: |
|    1    |  59   |     56      |
|    2    |  57   |     52      |
|    3    |  58   |     56      |
|    4    |  38   |     32      |
|    5    |  53   |     47      |
|    6    |  47   |     42      |
|    7    |  51   |     48      |
:::

:::{#exr-16.13}
Cardiac function is assessed via impedance cardiography while performing the Valsalva maneuver. A set of $11$ subjects perform the measure both standing and reclining, and the mean impedance ratio is reported in the following table. Is there a difference in the mean impedance ratio between the two positions?

| Subject | Standing | Reclining |
| :-----: | :------: | :-------: |
|    1    |   1.45   |   0.98    |
|    2    |   1.71   |   1.42    |
|    3    |   1.81   |   0.70    |
|    4    |   1.01   |   1.10    |
|    5    |   0.96   |   0.78    |
|    6    |   0.83   |   0.54    |
|    7    |   1.23   |   1.34    |
|    8    |   1.00   |   0.72    |
|    9    |   0.80   |   0.75    |
|   10    |   1.03   |   0.82    |
|   11    |   1.39   |   0.60    |

:::

:::{#exr-16.14}
The amount of surface deflection caused by air crafts landing on an airport runway is assessed between two landing gears, one simulating a Boeing 747 and the other simulating a Boeing 777, with measurements contained in the following table. Can you conclude that the mean deflection differs between the two gears?

| Test | 747  | 777  |
| :--: | :--: | :--: |
|  1   | 4.01 | 4.57 |
|  2   | 3.87 | 4.48 |
|  3   | 3.72 | 4.36 |
|  4   | 3.76 | 4.43 |
:::

:::{#exr-16.15}
Two extrusion machines that manufacture steel rods are being compared. In a sample of $1000$ rods taken from machine $1$, $960$ met specifications. In a sample of $600$ rods from machine $2$, $582$ met specifications. Machine $1$ is less costly to run and so it is preferable, unless machine $2$ is substantially better. Test the hypothesis that machine $2$ is better and draw a conclusion about which machine to use.
:::

:::{#exr-16.16}
Resistors labeled as $100\Omega$ are purchased from two vendors. The specification of this type of resistor is that its actual resistance must be within $5\%$ of the labeled resistance. In a sample of $180$ from vendor $A$, $150$ met specification; in a sample of $270$ from vendor $B$, $233$ met specification. Test whether there should be a change in supplier from vendor $A$ to vendor $B$, and draw a conclusion.
:::

:::{#exr-16.17}
To test the effectiveness of protective packaging, a firm shipped out $1200$ orders in regular light packaging and $1500$ orders in heavy-duty packaging. Of the orders shipped in light packaging, $20$ arrived damaged while of those shipped in heavy-duty packaging, $15$ arrived damaged. Can you conclude that heavy-duty packaging reduces the proportion of damaged shipments? 
:::

:::{#exr-16.18}
In order to determine whether to pitch a new advertising campaign more toward men or women, an advertiser provided each couple in a random sample of $500$ married couples with a new type of TV remote control that is supposed to be easier to find when needed. Of the $500$ men, $62\%$ said that the new remote was easier to find; of the $500$ women, $54\%$ said it was easier to find. Let $p_1$ be the population proportion of married men who think that the remote is easier to find and $p_2$ be the proportion of married women. Can we use $0.62 - 0.54$ to test $H_0: p_1 - p_2 = 0$? If so, perform the test. If not, explain.
:::

:::{#exr-16.19}
Two groups $A$ and $B$, each consisting of $100$ people who have a disease, are enrolled in a trial to test the efficacy of a newly developed serum. Group $A$ receives the serum, and group $B$ receives a fake treatment, known as a placebo. It is found that in group $A$ and $B$ there were $75$ and $65$ people who recovered from the disease, respectively. Test the hypothesis that the serum is effective for treatment of the disease. Draw a conclusion as to whether the serum should be used.
:::


::: {.content-visible when-format='html'}

## Self-Assessment {.unnumbered}

Note: the following questions are still experimental. Please contact me if you have any issues with these components. This can be if there are incorrect answers, or if there are any technical concerns. Each question currently has an ID with it, randomized for each version. If you have issues, reporting the specific ID will allow for easier checking!

For each question, you can check your answer using the checkmark button. You can cycle through variants of the question by pressing the arrow icon. 


```{r}
#| echo: false
#| message: false
#| warning: false

library(exams2forms)
set.seed(31415)

```


:::{#sa-16.01}
```{r}
#| echo: false
#| message: false
#| results: 'asis'
#| warning: false
#| cache: true
exams2forms(file="16.A.TwoSampleZTest.Rmd", 
            edir = "../PracticeQuestions/Chapter16", 
            n = 100, cloze_schoice_display = "buttons")
```
:::

:::{#sa-16.02}
```{r}
#| echo: false
#| message: false
#| results: 'asis'
#| warning: false
#| cache: true
exams2forms(file="16.B.TwoSampleTTest.Rmd", 
            edir = "../PracticeQuestions/Chapter16", 
            n = 100, cloze_schoice_display = "buttons")
```
:::

:::{#sa-16.03}
```{r}
#| echo: false
#| message: false
#| results: 'asis'
#| warning: false
#| cache: true
exams2forms(file="16.C.TwoSampleNormalVariances.Rmd", 
            edir = "../PracticeQuestions/Chapter16", 
            n = 100, cloze_schoice_display = "buttons")
```
:::

:::{#sa-16.04}
```{r}
#| echo: false
#| message: false
#| results: 'asis'
#| warning: false
#| cache: true
exams2forms(file="16.D.TwoSampleProportions.Rmd", 
            edir = "../PracticeQuestions/Chapter16", 
            n = 100, cloze_schoice_display = "buttons")
```
:::

:::{#sa-16.05}
```{r}
#| echo: false
#| message: false
#| results: 'asis'
#| warning: false
#| cache: true
exams2forms(file="16.E.PooledVariance.Rmd", 
            edir = "../PracticeQuestions/Chapter16", 
            n = 100, cloze_schoice_display = "buttons")
```
:::

:::{#sa-16.06}
```{r}
#| echo: false
#| message: false
#| results: 'asis'
#| warning: false
#| cache: true
exams2forms(file="16.F.PairedDifferences.Rmd", 
            edir = "../PracticeQuestions/Chapter16", 
            n = 100, cloze_schoice_display = "buttons")
```
:::

:::

::: {.content-visible when-format='html'}
{{< include /calculator.qmd >}}
:::